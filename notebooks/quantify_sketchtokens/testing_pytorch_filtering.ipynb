{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0ba68fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce RTX 2080 Ti\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2gray, rgba2rgb\n",
    "import skimage.io as skio\n",
    "from skimage.util.shape import view_as_windows\n",
    "from skimage.transform import resize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats  \n",
    "import scipy.stats\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import torch\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import argparse\n",
    "\n",
    "from utils import default_paths, nsd_utils, texture_utils\n",
    "from model_fitting import initialize_fitting\n",
    "from feature_extraction import fwrf_features\n",
    "\n",
    "try:\n",
    "    device = initialize_fitting.init_cuda()\n",
    "except:\n",
    "    device = 'cpu:0'\n",
    "    \n",
    "class bent_gabor_feature_bank():\n",
    "    \n",
    "    def __init__(self, freq_values=None, bend_values=None, orient_values=None, \\\n",
    "                 image_size=128, device='cpu:0'):\n",
    "        \n",
    "        self.image_size = image_size;\n",
    "        self.device = device\n",
    "        \n",
    "        self.__set_kernel_params__(freq_values, bend_values, orient_values)\n",
    "        self.__generate_kernels__()\n",
    "        \n",
    "    def __set_kernel_params__(self, freq_values, bend_values, orient_values):\n",
    "        \n",
    "        \"\"\"\n",
    "        Set some default params for the banana kernels.\n",
    "        sigmaXbend:  sigma for the bent gaussian in x-direction\n",
    "        sigmaYbend:  sigma for the bent gaussian in y-direction\n",
    "        xA_shift:    center shift in x direction\n",
    "        yA_shift:    center shift in y direction   \n",
    "        \n",
    "        freq_values: freq of filters, cyc/image\n",
    "        orient_values: orientation of filters, 0-2pi\n",
    "        bend_values: control bending of filters\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sigmaXbend = 2;\n",
    "#         self.sigmaXbend = 3;\n",
    "        self.sigmaYbend = 6;\n",
    "        self.kernel_size = self.image_size\n",
    "        self.xA_shift   = 0\n",
    "        self.yA_shift   = 0\n",
    "        \n",
    "        if freq_values is None:\n",
    "            self.freq_values = [64, 32, 16, 8]\n",
    "        else:\n",
    "            self.freq_values = freq_values\n",
    "            \n",
    "        nyquist = 0.5*self.kernel_size\n",
    "        if any(np.array(self.freq_values)>nyquist):\n",
    "            raise ValueError('for image of size %d x %d, must have freqs < %.2f'%\\\n",
    "                            (self.kernel_size, self.kernel_size, nyquist))\n",
    "        self.kA = np.array(self.freq_values)*2*np.pi / self.kernel_size\n",
    "        self.scale_values = np.log(2*np.pi/self.kA)/np.log(np.sqrt(2))\n",
    "        \n",
    "        if orient_values is None:\n",
    "            self.orient_values = np.linspace(0,2*np.pi, 9)[0:8]\n",
    "        else:\n",
    "            self.orient_values = orient_values\n",
    "        if bend_values is None:\n",
    "            self.bend_values = [0, 0.02,0.07,0.10,0.18,0.45]\n",
    "        else:\n",
    "            self.bend_values = bend_values\n",
    "            \n",
    "        print('freq values')\n",
    "        print(self.freq_values)\n",
    "        print('scale values')\n",
    "        print(self.scale_values)\n",
    "        print('bend values')\n",
    "        print(self.bend_values)\n",
    "        print('orient values')\n",
    "        print(self.orient_values)\n",
    "\n",
    "    def __make_bananakernel__(self, kA, bA, alphaA, is_curved):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate banana wavelet kernels.  The kernels\n",
    "        can be used to filter a image to quantify curvatures.\n",
    "\n",
    "        kA:          scale param, length of the wave vector K\n",
    "                     kA =  2*np.pi/((np.sqrt(2))**scale)\n",
    "                     filter frequency: (cycle/object) = kA*kernel_size / (2*pi)\n",
    "        bA:          bending value b (arbitrary, roughly between 0-0.5)\n",
    "        alphaA:      direction of the wave vector (i.e. orientation in rad)\n",
    "        is_curved:   Are we making a curved gabor? If false, making a sharp angle detector.\n",
    "                     Note if bA==0, then these are the same. \n",
    "\n",
    "        return SpaceKernel, FreqKernel\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        assert not (isinstance(bA, complex))\n",
    "       \n",
    "        kernel_size = self.kernel_size\n",
    "        if kernel_size%2 !=0:\n",
    "            kernel_size = kernel_size + 1\n",
    "        [xA, yA] = np.meshgrid(np.arange(-kernel_size/2, kernel_size/2,1),np.arange(-kernel_size/2, kernel_size/2,1)) \n",
    "        xA = xA - self.xA_shift\n",
    "        yA = yA - self.yA_shift\n",
    "\n",
    "        xRotL = np.cos(alphaA)*xA + np.sin(alphaA)*yA \n",
    "        yRotL = np.cos(alphaA)*yA - np.sin(alphaA)*xA\n",
    "\n",
    "        if is_curved:\n",
    "            # make a curved \"banana\" gabor.\n",
    "            scale = np.log(2*np.pi/kA)/np.log(np.sqrt(2))\n",
    "            xRotBendL = xRotL + bA/scale * (yRotL)**2\n",
    "        else:\n",
    "            # otherwise making a sharp angle detector, use abs instead of squaring.\n",
    "            # adjusting the constant here to make the bA values ~similar across curved/angle filters.\n",
    "            xRotBendL = xRotL + bA*4 * np.abs(yRotL)\n",
    "            \n",
    "        yRotBendL = yRotL\n",
    "\n",
    "        \"\"\"make the DC free\"\"\" \n",
    "        tmpgaussPartA = np.exp(-0.5*(kA)**2*((xRotBendL/self.sigmaXbend)**2 + (yRotBendL/(self.sigmaYbend))**2))\n",
    "        tmprealteilL  = 1*tmpgaussPartA*(np.cos(kA*xRotBendL) - 0)\n",
    "        tmpimagteilL  = 1*tmpgaussPartA*(np.sin(kA*xRotBendL) - 0)\n",
    "\n",
    "        numeratorRealL = np.sum(tmprealteilL)\n",
    "        numeratorImagL = np.sum(tmpimagteilL)\n",
    "        denominatorL   = np.sum(tmpgaussPartA)\n",
    "\n",
    "        DCValueAnalysis = np.exp(-0.5 * self.sigmaXbend * self.sigmaXbend)\n",
    "        if denominatorL==0:\n",
    "            DCPartRealA = DCValueAnalysis\n",
    "            DCPartImagA = 0\n",
    "        else:    \n",
    "            DCPartRealA = numeratorRealL/denominatorL\n",
    "            DCPartImagA = numeratorImagL/denominatorL\n",
    "            if DCPartRealA < DCValueAnalysis:\n",
    "                DCPartRealA = DCValueAnalysis\n",
    "                DCPartImagA = 0\n",
    "\n",
    "        \"\"\"generate a space kernel\"\"\" \n",
    "        preFactorA = kA**2\n",
    "        gaussPartA = np.exp(-0.5*(kA)**2*((xRotBendL/self.sigmaXbend)**2 + (yRotBendL/(self.sigmaYbend))**2))\n",
    "        realteilL  = preFactorA*gaussPartA*(np.cos(kA*xRotBendL) - DCPartRealA)\n",
    "        imagteilL  = preFactorA*gaussPartA*(np.sin(kA*xRotBendL) - DCPartImagA)\n",
    "\n",
    "        \"\"\"normalize the kernel\"\"\"  \n",
    "        normRealL   = np.sqrt(np.sum(realteilL**2))\n",
    "        normImagL   = np.sqrt(np.sum(imagteilL**2))\n",
    "        normFactorL = kA**2\n",
    "\n",
    "        total_std = normRealL + normImagL\n",
    "        if total_std == 0:\n",
    "            total_std = 10**20\n",
    "        norm_realteilL = realteilL*normFactorL/(0.5*total_std)\n",
    "        norm_imagteilL = imagteilL*normFactorL/(0.5*total_std)\n",
    "        \n",
    "        space_kernel = norm_realteilL + norm_imagteilL*1j\n",
    "        freq_kernel = np.fft.ifft2(space_kernel)\n",
    "        \n",
    "        return space_kernel, freq_kernel\n",
    " \n",
    "    def __generate_kernels__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Make the bank of filters.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_scales = len(self.scale_values)\n",
    "        n_orients = len(self.orient_values)\n",
    "        n_bends = len(self.bend_values)\n",
    "        \n",
    "        curv_freq_kernels,rect_freq_kernels,lin_freq_kernels, \\\n",
    "            curv_space,rect_space,lin_space = [],[],[],[],[],[]\n",
    "        \n",
    "        curv_kernel_pars = np.zeros((n_scales*(n_bends-1)*n_orients, 4))\n",
    "        rect_kernel_pars = np.zeros((n_scales*(n_bends-1)*n_orients, 4))\n",
    "        lin_kernel_pars = np.zeros((n_scales*n_orients, 4))\n",
    "        \n",
    "        ci=-1; ri=-1; li=-1\n",
    "            \n",
    "        for is_curved in [True, False]:\n",
    "\n",
    "            for kA, bA, alphaA in itertools.product(self.kA, self.bend_values, self.orient_values):\n",
    "\n",
    "                space_kernel, freq_kernel = self.__make_bananakernel__(kA, bA, alphaA, is_curved)\n",
    "\n",
    "                if bA == 0:\n",
    "                    if not is_curved:\n",
    "                        # the linear kernels each get defined twice (once with is_curv=True and False)\n",
    "                        # only counting one occurence of each.\n",
    "                        lin_freq_kernels.append(freq_kernel)\n",
    "                        lin_space.append(space_kernel.real) \n",
    "                        li+=1\n",
    "                        lin_kernel_pars[li,:] = [kA, bA, alphaA, is_curved]\n",
    "                    else:\n",
    "                        continue\n",
    "                elif is_curved:\n",
    "                    # this is a curved banana filter\n",
    "                    curv_freq_kernels.append(freq_kernel)\n",
    "                    curv_space.append(space_kernel.real)\n",
    "                    ci+=1\n",
    "                    curv_kernel_pars[ci,:] = [kA, bA, alphaA, is_curved]\n",
    "                else:\n",
    "                    # this is a second-order rectilinear filter\n",
    "                    rect_freq_kernels.append(freq_kernel)\n",
    "                    rect_space.append(space_kernel.real)\n",
    "                    ri+=1\n",
    "                    rect_kernel_pars[ri,:] = [kA, bA, alphaA, is_curved]\n",
    "                    \n",
    "\n",
    "        self.kernels = {'curv_freq':curv_freq_kernels, 'curv_space':curv_space,\n",
    "                        'rect_freq':rect_freq_kernels, 'rect_space':rect_space, \n",
    "                        'lin_freq':lin_freq_kernels, 'lin_space':lin_space}\n",
    "        self.rect_kernel_pars = rect_kernel_pars\n",
    "        self.curv_kernel_pars = curv_kernel_pars\n",
    "        self.lin_kernel_pars = lin_kernel_pars\n",
    "        self.n_rect_kernels = self.rect_kernel_pars.shape[0]\n",
    "        self.n_curv_kernels = self.curv_kernel_pars.shape[0]\n",
    "      \n",
    "    def plot_kernel_bends(self, ori_ind=0, scale_ind=0):\n",
    "\n",
    "        rect_kernel_pars = self.rect_kernel_pars\n",
    "        curv_kernel_pars = self.curv_kernel_pars\n",
    "        lin_kernel_pars = self.lin_kernel_pars\n",
    "        rect_spat_kernel_list = self.kernels['rect_space']\n",
    "        curv_spat_kernel_list = self.kernels['curv_space']\n",
    "        lin_spat_kernel_list = self.kernels['lin_space']\n",
    "\n",
    "        plt.figure(figsize=(20,12))\n",
    "        npx = 3;\n",
    "        npy = len(self.bend_values)\n",
    "\n",
    "        ori = self.orient_values[ori_ind]\n",
    "        sc = self.kA[scale_ind]\n",
    "\n",
    "        kk2plot = np.where((rect_kernel_pars[:,2]==ori) & (rect_kernel_pars[:,0]==sc))[0]\n",
    "        for ki, kk in enumerate(kk2plot):\n",
    "            plt.subplot(npx, npy, ki+1)\n",
    "            plt.pcolormesh(rect_spat_kernel_list[kk])\n",
    "            plt.axis('square')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.axis('off')\n",
    "            plt.title('bend=%.2f'%(rect_kernel_pars[kk,1]))\n",
    "\n",
    "        kk2plot = np.where((curv_kernel_pars[:,2]==ori) & (curv_kernel_pars[:,0]==sc))[0]\n",
    "        for ki, kk in enumerate(kk2plot):\n",
    "            plt.subplot(npx, npy,ki+npy+1)\n",
    "            plt.pcolormesh(curv_spat_kernel_list[kk])\n",
    "            plt.axis('square')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.axis('off')\n",
    "            plt.title('bend=%.2f'%(curv_kernel_pars[kk,1]))\n",
    "\n",
    "        kk2plot = np.where((lin_kernel_pars[:,2]==ori) & (lin_kernel_pars[:,0]==sc))[0]\n",
    "        for ki, kk in enumerate(kk2plot):\n",
    "            plt.subplot(npx, npy ,ki+npy*2+1)\n",
    "            plt.pcolormesh(lin_spat_kernel_list[kk])\n",
    "            plt.axis('square')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.axis('off')\n",
    "            plt.title('bend=%.2f'%(lin_kernel_pars[kk,1]))\n",
    "\n",
    "        plt.suptitle('ori=%.2f rad, freq=%.2f cyc/im'%(ori, self.freq_values[scale_ind]))\n",
    "        \n",
    "    def filter_image_batch(self, image_batch, which_kernels='curv'):\n",
    "        \n",
    "        \n",
    "        if which_kernels=='curv':\n",
    "            kernel_list = self.kernels['curv_freq']\n",
    "        elif which_kernels=='rect':\n",
    "            kernel_list = self.kernels['rect_freq']\n",
    "        elif which_kernels=='linear':\n",
    "            kernel_list = self.kernels['lin_freq']\n",
    "        else:\n",
    "            raise ValueError('which_kernels must be one of [curv, rect, linear]')\n",
    "\n",
    "        \"\"\"image x, image y, kernel dimension, all images (4D array)\"\"\"\n",
    "        all_kernels = np.dstack(kernel_list)\n",
    "        \n",
    "        \"\"\"calculate kernel norm for normalization\"\"\"\n",
    "        all_kernels_power =  np.einsum('ijk,ijk->k',np.abs(all_kernels),np.abs(all_kernels))\n",
    "        all_kernels_power =  np.sqrt(all_kernels_power)\n",
    "\n",
    "        \"\"\"stack fft image list to a 3d array\"\"\"\n",
    "        image_batch_fft = np.fft.fft2(image_batch, axes=(0,1))\n",
    "        \n",
    "        all_conved_images = np.abs(np.fft.ifft2(image_batch_fft[:,:,np.newaxis,:]*all_kernels[:,:,:,np.newaxis],axes=(0,1)))\n",
    "        all_conved_images = np.power(all_conved_images,1/2) ## power correction\n",
    "        all_conved_images = all_conved_images/all_kernels_power[np.newaxis, np.newaxis,:,np.newaxis]\n",
    "    \n",
    "        return np.fft.fftshift(all_conved_images, axes=(0,1))\n",
    "    \n",
    "    def filter_image_batch_pytorch(self, image_batch, which_kernels='all', to_numpy=True):\n",
    "\n",
    "        if which_kernels=='curv':\n",
    "            kernel_list = self.kernels['curv_freq']\n",
    "        elif which_kernels=='rect':\n",
    "            kernel_list = self.kernels['rect_freq']\n",
    "        elif which_kernels=='linear':\n",
    "            kernel_list = self.kernels['lin_freq']\n",
    "        elif which_kernels=='all':\n",
    "            kernel_list = self.kernels['curv_freq']+self.kernels['rect_freq']+self.kernels['lin_freq']\n",
    "        else:\n",
    "            raise ValueError('which_kernels must be one of [curv, rect, linear, all]')\n",
    "\n",
    "        # stack all the filters together, [self.kernel_size, self.kernel_size, n_filters]\n",
    "        # and send to specified device.\n",
    "        all_kernels = np.dstack(kernel_list)\n",
    "        all_kernels_tensor = torch.complex(torch.Tensor(np.real(all_kernels)), \\\n",
    "                                           torch.Tensor(np.imag(all_kernels)))\n",
    "        \n",
    "        # Compute power of each kernel, will use to normalize the convolution result.\n",
    "        all_kernels_power =  torch.sum(torch.sum(torch.pow(torch.abs(all_kernels_tensor), 2), axis=0), axis=0)\n",
    "        all_kernels_power =  torch.sqrt(all_kernels_power)\n",
    "\n",
    "        # send image batch to device [self.image_size, self.image_size, n_images]\n",
    "        image_batch_tensor = torch.Tensor(image_batch).to(self.device)\n",
    "        # get frequency domain representation of images\n",
    "        image_batch_fft = torch.fft.fftn(image_batch_tensor, dim=(0,1))\n",
    "\n",
    "        # apply the filters by multiplying all at once\n",
    "        mult = image_batch_fft.view([self.image_size, self.image_size,1,-1]) * \\\n",
    "                all_kernels_tensor.view([self.image_size, self.image_size,-1,1])\n",
    "        # back to spatial domain \n",
    "        all_conved_images = torch.abs(torch.fft.ifftn(mult,dim=(0,1)))\n",
    "        # power correction\n",
    "        all_conved_images = torch.pow(all_conved_images,1/2) \n",
    "        all_conved_images = all_conved_images/ \\\n",
    "                    all_kernels_power.view([1,1,all_kernels_power.shape[0],1])\n",
    "        \n",
    "        # shift back to original spatial configuration\n",
    "        all_conved_images = torch.fft.fftshift(all_conved_images, dim=(0,1))\n",
    "\n",
    "        if to_numpy:\n",
    "            all_conved_images = all_conved_images.detach().cpu().numpy()\n",
    "            \n",
    "        return all_conved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_curv_kernel = np.argmax(mean_curv_over_space, axis=1)\n",
    "best_rect_kernel = np.argmax(mean_rect_over_space, axis=1)\n",
    "best_lin_kernel = np.argmax(mean_lin_over_space, axis=1)\n",
    "\n",
    "curv_z = scipy.stats.zscore(mean_curv_over_space, axis=0)\n",
    "rect_z = scipy.stats.zscore(mean_rect_over_space, axis=0)\n",
    "lin_z = scipy.stats.zscore(mean_lin_over_space, axis=0)\n",
    "\n",
    "best_curv_kernel_z = np.argmax(curv_z, axis=1)\n",
    "best_rect_kernel_z = np.argmax(rect_z, axis=1)\n",
    "best_lin_kernel_z = np.argmax(lin_z, axis=1)\n",
    "\n",
    "mean_curv_z = np.mean(curv_z, axis=1)\n",
    "mean_rect_z = np.mean(rect_z, axis=1)\n",
    "mean_lin_z = np.mean(lin_z, axis=1)\n",
    "\n",
    "curv_rect_index = (mean_curv_z - mean_rect_z - mean_lin_z) / \\\n",
    "                  (mean_curv_z + mean_rect_z + mean_lin_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a7815b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bank.kernels['curv_freq']+bank.kernels['rect_freq']+bank.kernels['lin_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "de1e1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq values\n",
      "[64, 32, 16, 8]\n",
      "scale values\n",
      "[2. 4. 6. 8.]\n",
      "bend values\n",
      "[0, 0.02, 0.07, 0.1, 0.18, 0.45]\n",
      "orient values\n",
      "[0.         0.78539816 1.57079633 2.35619449 3.14159265 3.92699082\n",
      " 4.71238898 5.49778714]\n"
     ]
    }
   ],
   "source": [
    "bank = bent_gabor_feature_bank(device='cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "58d4fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = np.random.normal(0,1,[100,100,200])\n",
    "time = time.time\n",
    "ans = np.mean(np.mean(thing, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1def17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = np.random.normal(0,1,[128,128,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1c7400e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "output1 = np.concatenate([bank.filter_image_batch_pytorch(image_batch, k) for k in ['curv','rect', 'linear']], axis=2)\n",
    "elapsed = time.time() - st\n",
    "print('elapsed time = %.5f s'%elapsed)\n",
    "st = time.time()\n",
    "output2 = bank.filter_image_batch_pytorch(image_batch, 'all')\n",
    "elapsed = time.time() - st\n",
    "print('elapsed time = %.5f s'%elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e65dfa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time = 54.49722 s\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "output3 = np.concatenate([bank.filter_image_batch(image_batch, k) for k in ['curv','rect', 'linear']], axis=2)\n",
    "elapsed = time.time() - st\n",
    "print('elapsed time = %.5f s'%elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a0e5a050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 128, 352, 50), (128, 128, 352, 50))"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape, output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "57249989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.dtype, output2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0e2d3a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4696233, 1.4214585, 1.7884996, 2.476495 , 0.7059956, 1.3420668,\n",
       "       1.8813096, 2.1318772, 2.8678038, 2.6955152, 1.8448137, 1.6807752,\n",
       "       2.3423147, 1.7749122, 2.268266 , 1.711766 , 2.5058978, 2.2039583,\n",
       "       0.6841866, 1.3269516, 2.5777104, 1.5785817, 1.0995805, 2.1393645,\n",
       "       2.17551  , 1.4904385, 1.6286892, 2.0808933, 1.0770144, 1.4902999,\n",
       "       2.4968717, 3.195699 , 2.496212 , 1.8734933, 2.2282205, 2.0609915,\n",
       "       1.793022 , 1.2400877, 2.4840724, 2.538421 , 3.2831697, 1.8803866,\n",
       "       2.2383482, 2.0741968, 1.4740773, 1.7631953, 1.6768072, 3.549436 ,\n",
       "       1.4978232, 0.9511576], dtype=float32)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1[100,120,300,:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "908655eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4696233, 1.4214585, 1.7884996, 2.476495 , 0.7059956, 1.3420668,\n",
       "       1.8813096, 2.1318772, 2.8678038, 2.6955152, 1.8448137, 1.6807752,\n",
       "       2.3423147, 1.7749122, 2.268266 , 1.711766 , 2.5058978, 2.2039583,\n",
       "       0.6841866, 1.3269516, 2.5777104, 1.5785817, 1.0995805, 2.1393645,\n",
       "       2.17551  , 1.4904385, 1.6286892, 2.0808933, 1.0770144, 1.4902999,\n",
       "       2.4968717, 3.195699 , 2.496212 , 1.8734933, 2.2282205, 2.0609915,\n",
       "       1.793022 , 1.2400877, 2.4840724, 2.538421 , 3.2831697, 1.8803866,\n",
       "       2.2383482, 2.0741968, 1.4740773, 1.7631953, 1.6768072, 3.549436 ,\n",
       "       1.4978232, 0.9511576], dtype=float32)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[100,120,300,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4e123304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time = 0.77986 s\n",
      "elapsed time = 2.77879 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "output1 = bank.filter_image_batch_pytorch(image_batch, 'rect')\n",
    "elapsed = time.time() - st\n",
    "print('elapsed time = %.5f s'%elapsed)\n",
    "st = time.time()\n",
    "output2 = bank.filter_image_batch(image_batch, 'rect')\n",
    "elapsed = time.time() - st\n",
    "print('elapsed time = %.5f s'%elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "65722b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 160, 5)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5e9203a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c66b0057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1aec9f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2784, 0.2287, 0.2413, 0.1713, 0.1855])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1[100,120,10,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6c7dc03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27844459, 0.22866219, 0.24127039, 0.17133451, 0.18545024])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[100,120,10,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6f6a47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel_list = bank.kernels['curv_freq']\n",
    " \n",
    "\"\"\"image x, image y, kernel dimension, all images (4D array)\"\"\"\n",
    "all_kernels = np.dstack(kernel_list)\n",
    "\n",
    "\"\"\"calculate kernel norm for normalization\"\"\"\n",
    "all_kernels_power =  np.einsum('ijk,ijk->k',np.abs(all_kernels),np.abs(all_kernels))\n",
    "all_kernels_power =  np.sqrt(all_kernels_power)\n",
    "all_kernels_power_1 = all_kernels_power\n",
    "\n",
    "\n",
    "\"\"\"stack fft image list to a 3d array\"\"\"\n",
    "image_batch_fft = np.fft.fft2(image_batch, axes=(0,1))\n",
    "\n",
    "mult = image_batch_fft[:,:,np.newaxis,:]*all_kernels[:,:,:,np.newaxis]\n",
    "\n",
    "all_conved_images = np.abs(np.fft.ifft2(mult,axes=(0,1)))\n",
    "# all_conved_images_1 = all_conved_images\n",
    "all_conved_images = np.power(all_conved_images,1/2) ## power correction\n",
    "all_conved_images_1 = all_conved_images\n",
    "# all_conved_images = all_conved_images/all_kernels_power[np.newaxis, np.newaxis,:,np.newaxis]\n",
    "\n",
    "# all_conved_images_1 = all_conved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3bfe557b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02727183, 0.02727721, 0.02727183, 0.02727721, 0.02727183,\n",
       "       0.02727721, 0.02727266, 0.02727722, 0.02727266, 0.02727722])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kernels_power_1[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "92421577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0273, 0.0273, 0.0273, 0.0273, 0.0273, 0.0273, 0.0273, 0.0273, 0.0273,\n",
       "        0.0273])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kernels_power_2[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "407c2a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.93472890e-09-3.40413453e-10j,  1.77749642e-09-1.50342876e-09j,\n",
       "        3.93845074e-09-5.78986695e-09j,  4.38945932e-09+4.00415532e-09j,\n",
       "       -6.78794118e-09+3.18103287e-09j])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_1[10,90,50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ecbafd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9347e-09-3.4041e-10j,  1.7775e-09-1.5034e-09j,\n",
       "         3.9385e-09-5.7899e-09j,  4.3895e-09+4.0042e-09j,\n",
       "        -6.7879e-09+3.1810e-09j])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_2[10,90,50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "524a205c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00593715, 0.00647518, 0.00165913, 0.00410518, 0.00667207])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conved_images_1[50,100,80,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9b79e064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0059, 0.0065, 0.0017, 0.0041, 0.0067])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conved_images_2[50,100,80,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "85d3e60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conved_images_2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "068a6b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5250e-05, 4.1928e-05, 2.7527e-06, 1.6853e-05, 4.4517e-05])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conved_images_2[50,100,80,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3084a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "53890dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stack all the filters together, [self.kernel_size, self.kernel_size, n_filters]\n",
    "# and send to specified device.\n",
    "all_kernels = np.dstack(kernel_list)\n",
    "all_kernels_tensor = torch.complex(torch.Tensor(np.real(all_kernels)), \\\n",
    "                                   torch.Tensor(np.imag(all_kernels)))\n",
    "\n",
    "# Compute power of each kernel, will use to normalize the convolution result.\n",
    "all_kernels_power =  torch.sum(torch.sum(torch.pow(torch.abs(all_kernels_tensor), 2), axis=0), axis=0)\n",
    "all_kernels_power =  torch.sqrt(all_kernels_power)\n",
    "all_kernels_power_2 = all_kernels_power\n",
    "\n",
    "# send image batch to device [self.image_size, self.image_size, n_images]\n",
    "image_batch_tensor = torch.Tensor(image_batch).to(bank.device)\n",
    "# get frequency domain representation of images\n",
    "image_batch_fft = torch.fft.fftn(image_batch_tensor, dim=(0,1))\n",
    "\n",
    "# apply the filters by multiplying all at once\n",
    "mult = image_batch_fft.view([bank.image_size, bank.image_size,1,-1]) * \\\n",
    "        all_kernels_tensor.view([bank.image_size, bank.image_size,-1,1])\n",
    "# # back to spatial domain \n",
    "all_conved_images = torch.abs(torch.fft.ifftn(mult,dim=(0,1)))\n",
    "# all_conved_images_2 = all_conved_images\n",
    "# # power correction\n",
    "all_conved_images = torch.pow(all_conved_images,1/2) \n",
    "# all_conved_images_2 = all_conved_images\n",
    "# all_conved_images = all_conved_images/ \\\n",
    "#             all_kernels_power.view([1,1,all_kernels_power.shape[0],1])\n",
    "all_conved_images_2 = all_conved_images\n",
    "# all_conved_images_2 = all_conved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "89b8a641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.+6.j])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = 5+6j\n",
    "thing = np.array([thing])\n",
    "thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dc40e26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(npz.array([thing]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f41fb89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.+6.j])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = torch.complex(torch.Tensor(np.real(thing)), torch.Tensor(np.imag(thing)))\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a8e83b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.8102])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8c642c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.81024968])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "887ff845",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (numpy.ndarray, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-2fa5571c0f3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (numpy.ndarray, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "torch.Tensor(thing, dtype=torch.cfloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5235d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernels = np.random.normal(0,1,[40,40,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977423fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.5405043 , 40.97954212, 39.59402806, 40.80134468, 39.94954381])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kernels_power =  np.einsum('ijk,ijk->k',np.abs(all_kernels),np.abs(all_kernels))\n",
    "all_kernels_power =  np.sqrt(all_kernels_power)\n",
    "all_kernels_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "946f295b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40.5405043 , 40.97954212, 39.59402806, 40.80134468, 39.94954381])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kernels_power = np.sum(np.sum(all_kernels**2, axis=0), axis=0)\n",
    "all_kernels_power =  np.sqrt(all_kernels_power)\n",
    "all_kernels_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97fabb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([40.5405, 40.9795, 39.5940, 40.8013, 39.9495])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kernels_torch = torch.Tensor(all_kernels).to('cpu:0')\n",
    "all_kernels_power =  torch.sum(torch.sum(torch.pow(all_kernels_torch, 2), axis=0), axis=0)\n",
    "all_kernels_power =  torch.sqrt(all_kernels_power)\n",
    "all_kernels_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "635fe360",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = np.random.normal(0,1,[40,40,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da6d043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch_fft = np.fft.fft2(image_batch, axes=(0,1))\n",
    "       \n",
    "all_kernels_power = np.sum(np.sum(all_kernels**2, axis=0), axis=0)\n",
    "all_kernels_power =  np.sqrt(all_kernels_power)\n",
    "\n",
    "all_conved_images = np.abs(np.fft.ifft2(image_batch_fft[:,:,np.newaxis,:]*all_kernels[:,:,:,np.newaxis],axes=(0,1)))\n",
    "all_conved_images = np.power(all_conved_images,1/2) ## power correction\n",
    "all_conved_images = all_conved_images/all_kernels_power[np.newaxis, np.newaxis,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a26ef5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01810492, 0.01245891],\n",
       "       [0.00791344, 0.0263366 ],\n",
       "       [0.03090774, 0.02566644],\n",
       "       [0.02327567, 0.0121277 ],\n",
       "       [0.01799749, 0.02118772]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conved_images[20,20,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e12cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch_torch = torch.Tensor(image_batch).to('cpu:0')\n",
    "image_batch_fft = torch.fft.fftn(image_batch_torch, dim=(0,1))\n",
    "        \n",
    "all_kernels_torch = torch.Tensor(all_kernels).to('cpu:0')\n",
    "all_kernels_power =  torch.sum(torch.sum(torch.pow(all_kernels_torch, 2), axis=0), axis=0)\n",
    "all_kernels_power =  torch.sqrt(all_kernels_power)\n",
    "\n",
    "mult = image_batch_fft.view([image_batch_fft.shape[0],image_batch_fft.shape[1],1,-1]) * \\\n",
    "        all_kernels_torch.view([all_kernels_torch.shape[0], all_kernels_torch.shape[1],-1,1])\n",
    "all_conved_images = torch.abs(torch.fft.ifftn(mult,dim=(0,1)))\n",
    "all_conved_images = torch.pow(all_conved_images,1/2) ## power correction\n",
    "all_conved_images = all_conved_images/ \\\n",
    "            all_kernels_power.view([1,1,all_kernels_power.shape[0],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebfef6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0181, 0.0125],\n",
       "        [0.0079, 0.0263],\n",
       "        [0.0309, 0.0257],\n",
       "        [0.0233, 0.0121],\n",
       "        [0.0180, 0.0212]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conved_images[20,20,:,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
