{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fc19ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce GTX TITAN X\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import torch\n",
    "import time\n",
    "import h5py\n",
    "import torch.nn\n",
    "\n",
    "#import custom modules\n",
    "code_dir = '/user_data/mmhender/imStat/code/'\n",
    "sys.path.append(code_dir)\n",
    "from utils import torch_utils, nsd_utils\n",
    "from utils import default_paths \n",
    "from model_fitting import initialize_fitting\n",
    "from feature_extraction import texture_statistics_gabor\n",
    "\n",
    "device = initialize_fitting.init_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c24ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "subject=1\n",
    "n_sf = 4; \n",
    "n_ori = 4;\n",
    "debug=True\n",
    "use_node_storage = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ff3540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading images for subject 1\n",
      "\n",
      "image data size: (10000, 3, 227, 227) , dtype: uint8 , value range: 0 255\n"
     ]
    }
   ],
   "source": [
    "if use_node_storage:\n",
    "    pyramid_texture_feat_path = default_paths.gabor_texture_feat_path_localnode\n",
    "else:\n",
    "    pyramid_texture_feat_path = default_paths.gabor_texture_feat_path\n",
    "\n",
    "# Load and prepare the image set to work with (all images for the current subject, 10,000 ims)\n",
    "stim_root = default_paths.stim_root\n",
    "image_data = nsd_utils.get_image_data(subject)  \n",
    "image_data = nsd_utils.image_uncolorize_fn(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24eb276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n",
      "Feature types to exclude from the model:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Params for the spatial aspect of the model (possible pRFs)\n",
    "aperture = 1.0\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "# Set up the feature extractor - fixing these parameters\n",
    "feature_types_exclude = None\n",
    "n_prf_sd_out = 2\n",
    "padding_mode = 'circular'\n",
    "nonlin_fn=False\n",
    "autocorr_output_pix=5\n",
    "\n",
    "do_varpart=False # this doesn't do anything here\n",
    "group_all_hl_feats = False # this doesn't do anything here\n",
    "\n",
    "compute_features = True\n",
    "\n",
    "# Set up the Gabor filtering modules\n",
    "_gabor_ext_complex, _gabor_ext_simple, _fmaps_fn_complex, _fmaps_fn_simple = \\\n",
    "        initialize_fitting.get_gabor_feature_map_fn(n_ori, n_sf, padding_mode=padding_mode, device=device, \\\n",
    "                                                             nonlin_fn=nonlin_fn);    \n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "\n",
    "_feature_extractor = texture_feature_extractor(_fmaps_fn_complex, _fmaps_fn_simple, \\\n",
    "                                        sample_batch_size=batch_size, autocorr_output_pix=autocorr_output_pix, \\\n",
    "                                        n_prf_sd_out=n_prf_sd_out, aperture=aperture, \\\n",
    "                                        feature_types_exclude=feature_types_exclude, do_varpart=do_varpart, \\\n",
    "                                        group_all_hl_feats=group_all_hl_feats, compute_features = compute_features, \\\n",
    "                                               device=device)     \n",
    "n_pix = image_data.shape[2]\n",
    "_feature_extractor.init_for_fitting((n_pix, n_pix), models, device)\n",
    "n_features = _feature_extractor.n_features_total\n",
    "n_images = image_data.shape[0]\n",
    "n_prfs = models.shape[0]\n",
    "n_batches = int(np.ceil(n_images/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ca3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for images [2 - 3]\n",
      "Clear maps fn\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.49210526315789477, -0.55, 0.03999999910593033]\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00689\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.08354\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.02118\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.25288\n",
      "Final size of features concatenated is [2 x 1468]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'complex_feature_means', 'simple_feature_means', 'complex_feature_autocorrs', 'simple_feature_autocorrs', 'complex_within_scale_crosscorrs', 'simple_within_scale_crosscorrs', 'complex_across_scale_crosscorrs', 'simple_across_scale_crosscorrs']\n",
      "There are 612 zeros columns in feature matrix\n",
      "model 1, min/max of features in batch: [tensor(-0.5169, device='cuda:0', grad_fn=<MinBackward1>), tensor(2.3903, device='cuda:0', grad_fn=<MaxBackward1>)]\n"
     ]
    }
   ],
   "source": [
    "# n_zeros = np.zeros((len(_feature_extractor.feature_types_all), len(models)))\n",
    "\n",
    "features_each_prf = np.zeros((n_images, n_features, n_prfs), dtype=np.float32)\n",
    "\n",
    "bb=1\n",
    "# for bb in range(n_batches):\n",
    "\n",
    "#     if debug and bb>1:\n",
    "#         continue\n",
    "\n",
    "batch_inds = np.arange(batch_size * bb, np.min([batch_size * (bb+1), n_images]))\n",
    "\n",
    "print('Extracting features for images [%d - %d]'%(batch_inds[0], batch_inds[-1]))\n",
    "\n",
    "image_batch = torch_utils._to_torch(image_data[batch_inds,:,:,:], device)\n",
    "\n",
    "_feature_extractor.clear_big_features()\n",
    "\n",
    "mm=1\n",
    "\n",
    "# for mm in range(n_prfs):\n",
    "\n",
    "#     if debug and mm>1:\n",
    "#         continue\n",
    "\n",
    "x,y,sigma = models[mm,:]\n",
    "print('Getting features for pRF [x,y,sigma]:')\n",
    "print([x,y,sigma])\n",
    "\n",
    "features_batch, _ = _feature_extractor(image_batch, models[mm],mm)\n",
    "\n",
    "print('model %d, min/max of features in batch: [%s, %s]'%(mm, torch.min(features_batch), torch.max(features_batch))) \n",
    "\n",
    "features_each_prf[batch_inds,:,mm] = torch_utils.get_value(features_batch)\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "        #     feats = torch_utils.get_value(features_batch)\n",
    "#     for fi, ff in enumerate(_feature_extractor.feature_types_all):\n",
    "#         n_zeros[fi,mm] = np.sum(feats[0,_feature_extractor.feature_column_labels==fi]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c14b236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1468])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ffb24bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prf features to /user_data/mmhender/features/gabor_texture/S1_features_each_prf_4ori_4sf.h5py\n",
      "\n",
      "Took 133.87022 sec to write file\n"
     ]
    }
   ],
   "source": [
    "fn2save = os.path.join(gabor_texture_feat_path, 'S%d_features_each_prf_%dori_%dsf.h5py'%(subject, n_ori, n_sf))\n",
    "\n",
    "print('Writing prf features to %s\\n'%fn2save)\n",
    "\n",
    "t = time.time()\n",
    "with h5py.File(fn2save, 'w') as data_set:\n",
    "    dset = data_set.create_dataset(\"features\", np.shape(features_each_prf), dtype=np.float64)\n",
    "    data_set['/features'][:,:,:] = features_each_prf\n",
    "    data_set.close()  \n",
    "elapsed = time.time() - t\n",
    "\n",
    "print('Took %.5f sec to write file'%elapsed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dacaa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types to exclude from the model:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "compute_features = False\n",
    "do_varpart = True\n",
    "group_all_hl_feats = True\n",
    "feature_types_exclude =[]\n",
    "# Set up the Gabor filtering modules\n",
    "_gabor_ext_complex, _gabor_ext_simple, _fmaps_fn_complex, _fmaps_fn_simple = \\\n",
    "        initialize_fitting.get_gabor_feature_map_fn(n_ori, n_sf, padding_mode=padding_mode, device=device, \\\n",
    "                                                             nonlin_fn=nonlin_fn);    \n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "\n",
    "_feature_extractor = texture_feature_extractor(_fmaps_fn_complex, _fmaps_fn_simple, \\\n",
    "                                        sample_batch_size=batch_size, autocorr_output_pix=autocorr_output_pix, \\\n",
    "                                        n_prf_sd_out=n_prf_sd_out, aperture=aperture, \\\n",
    "                                        feature_types_exclude=feature_types_exclude, do_varpart=do_varpart, \\\n",
    "                                        group_all_hl_feats=group_all_hl_feats, compute_features = compute_features, \\\n",
    "                                               device=device)     \n",
    "n_pix = image_data.shape[2]\n",
    "_feature_extractor.init_for_fitting((n_pix, n_pix), models, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36b1cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, names = _feature_extractor.get_partial_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "580a991a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(_feature_extractor.feature_column_labels==1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c767a41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_feature_extractor.feature_type_dims_all[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44617d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[1,0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c163071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "239b16da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed features from /user_data/mmhender/features/gabor_texture/S1_features_each_prf_4ori_4sf.h5py\n",
      "Took 0.74224 seconds to load file\n",
      "Size of features array for this image set is:\n",
      "(4, 1468, 875)\n",
      "Final size of features concatenated is [4 x 268]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'complex_feature_means', 'simple_feature_means', 'complex_within_scale_crosscorrs', 'simple_within_scale_crosscorrs', 'complex_across_scale_crosscorrs', 'simple_across_scale_crosscorrs']\n"
     ]
    }
   ],
   "source": [
    "image_inds = np.array([50,1,3,0])\n",
    "\n",
    "mm=1\n",
    "features, _ = _feature_extractor(image_inds, models[mm], mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4b5c837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.2160e-01,  3.2229e-03, -1.4685e-01,  2.3903e+00,  6.6216e-04,\n",
       "         4.5570e-04,  3.0108e-03,  7.9473e-04,  4.7998e-04,  9.5921e-04,\n",
       "         2.0805e-03,  2.4186e-04,  3.7862e-04,  4.0115e-04,  1.6596e-03,\n",
       "         1.6495e-04,  1.7310e-04,  2.1910e-04,  1.2667e-03,  1.6235e-04,\n",
       "        -5.4945e-04,  2.9002e-04, -4.6649e-05,  4.3697e-04,  1.5617e-03,\n",
       "         2.5565e-03,  6.9240e-04, -3.1160e-04, -2.9121e-04,  1.0848e-05,\n",
       "         1.5219e-04,  5.6136e-04,  1.6385e-03,  8.0067e-05, -1.5039e-04,\n",
       "        -1.4116e-04, -7.7383e-05,  7.5931e-05,  5.4542e-05,  3.8311e-05,\n",
       "         6.9384e-04, -3.0298e-05,  1.1892e-05,  5.5548e-06, -4.9100e-06,\n",
       "         9.8671e-05,  7.0241e-06, -4.0852e-06,  2.5101e-04, -5.9195e-05,\n",
       "         7.7677e-06, -2.1266e-06,  3.3692e-08,  2.0791e-07,  1.1240e-07,\n",
       "        -8.4538e-09,  1.5253e-08,  2.7286e-08,  1.4615e-06,  4.8881e-06,\n",
       "         7.2372e-07,  4.5302e-06, -4.4878e-07,  1.2408e-06,  5.1010e-06,\n",
       "         1.6955e-05, -6.7393e-08, -9.5603e-07,  1.2934e-06, -3.0986e-06,\n",
       "         1.1712e-06,  3.1706e-07,  5.7352e-07, -2.7577e-06,  1.0502e-06,\n",
       "         2.0707e-07,  5.3732e-06,  1.1081e-06,  1.1022e-05,  1.1042e-07,\n",
       "         3.4172e-06,  3.5886e-06,  3.6919e-06,  5.2598e-07,  7.1501e-06,\n",
       "         9.7349e-07,  2.2479e-06,  2.9296e-06,  4.1181e-05,  5.1806e-06,\n",
       "        -1.3013e-06,  4.7041e-05,  5.5897e-06, -9.5567e-07, -7.7366e-06,\n",
       "        -1.9117e-05, -3.1278e-06,  3.5040e-05, -5.1005e-06, -1.3652e-05,\n",
       "         1.9879e-06, -1.0417e-05,  2.6459e-06,  3.8215e-05,  2.2878e-06,\n",
       "         1.1806e-07,  3.4173e-06, -1.2100e-06,  8.9248e-07,  2.2459e-05,\n",
       "         1.7855e-06,  1.0200e-05,  5.9066e-07,  2.2922e-07,  3.1355e-07,\n",
       "         3.4979e-06, -6.9435e-07, -3.9685e-06,  5.1481e-07,  3.4471e-08,\n",
       "        -8.8157e-08,  6.8502e-06, -9.4397e-07,  4.7023e-06,  1.1566e-06,\n",
       "         1.4416e-07,  4.2889e-07,  3.7898e-07,  1.0697e-06,  1.0643e-08,\n",
       "         1.2344e-06,  1.9068e-07,  2.1795e-06,  7.7046e-08,  2.1898e-06,\n",
       "         4.5243e-07, -4.3296e-08, -5.8128e-08,  2.7474e-07, -7.2159e-08,\n",
       "         1.0894e-06,  2.0869e-06,  5.0564e-06,  1.9339e-06, -2.7866e-06,\n",
       "        -1.6819e-06, -1.5988e-06,  4.1397e-06,  5.6503e-06,  1.1862e-05,\n",
       "         2.0595e-05, -1.8261e-06, -6.6847e-07, -6.4445e-07, -1.0929e-06,\n",
       "         2.4980e-07,  6.1525e-08, -5.8865e-07,  4.9845e-08,  3.2566e-08,\n",
       "         6.1967e-07,  1.1395e-06, -1.2173e-06,  2.3303e-07,  2.3463e-05,\n",
       "        -8.0247e-06,  1.0845e-04, -5.3246e-06,  6.6498e-07,  1.1593e-06,\n",
       "        -3.2860e-07, -1.0899e-07, -4.8703e-06,  5.8782e-06,  1.9343e-06,\n",
       "         1.0702e-05, -5.3980e-06,  2.3800e-06, -4.8426e-06,  1.3783e-05,\n",
       "         2.0172e-06, -2.2313e-05, -3.6875e-05,  7.0607e-06,  1.9002e-06,\n",
       "        -8.3339e-07,  1.8900e-06, -5.0154e-06,  1.2254e-05,  3.1715e-06,\n",
       "        -2.7109e-06,  6.0409e-06,  1.2293e-05,  4.5939e-06,  2.2148e-05,\n",
       "         9.9189e-06, -1.3834e-05, -1.5709e-06,  1.2755e-04,  2.8815e-06,\n",
       "        -4.1427e-06, -1.2293e-06, -8.2178e-06, -2.7725e-06,  2.9885e-06,\n",
       "        -5.1088e-06, -5.2233e-06, -2.4495e-07, -5.7048e-07, -1.0001e-05,\n",
       "         1.3479e-05,  9.8850e-07,  5.5160e-06, -2.8711e-05,  3.8355e-05,\n",
       "         1.2468e-07, -1.4113e-06,  1.4349e-07,  2.8165e-06,  4.5196e-07,\n",
       "        -1.8167e-06, -3.1031e-06, -2.1395e-06,  1.4518e-06, -5.9577e-06,\n",
       "         1.3908e-05,  1.9145e-05, -8.4438e-07, -5.1204e-06,  4.4563e-06,\n",
       "         8.0275e-05, -2.4079e-06, -1.3314e-06,  4.3842e-06,  2.7943e-06,\n",
       "        -2.5679e-07, -4.3772e-08, -1.9286e-07, -1.6966e-07, -1.5779e-08,\n",
       "         5.0984e-07, -2.9037e-06, -3.9576e-06, -2.5281e-08,  2.5765e-06,\n",
       "        -6.4014e-06, -6.7030e-05, -1.2470e-06,  6.5734e-07, -7.2940e-08,\n",
       "        -2.5003e-06, -5.4610e-07,  3.6151e-08,  1.8021e-08,  1.4602e-07,\n",
       "        -1.4053e-07, -2.9810e-07,  1.6241e-06,  1.8580e-06, -3.4959e-07,\n",
       "         4.8457e-06, -6.0964e-06, -5.2365e-05, -1.8973e-06,  8.1290e-07,\n",
       "        -2.7785e-07,  1.9459e-06,  5.0360e-07], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6cb87f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.2160e-01,  3.2229e-03, -1.4685e-01,  ..., -2.7785e-07,\n",
       "         1.9459e-06,  5.0360e-07], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_batch[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9df9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_each_prf.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6281186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING THERE ARE ZEROS IN FEATURES MATRIX\n",
      "\n",
      "zeros for columns:\n",
      "612\n"
     ]
    }
   ],
   "source": [
    "if torch.any(torch.all(features_batch==0, axis=0)):\n",
    "    print('\\nWARNING THERE ARE ZEROS IN FEATURES MATRIX\\n')\n",
    "    print('zeros for columns:')\n",
    "    print(np.sum(torch_utils.get_value(torch.all(features_batch==0, axis=0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d99f94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(gabor_texture_feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b25d36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 32, 400, 800, 24, 48, 48, 96]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.feature_type_dims_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d793537",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98e6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import numpy_utils, torch_utils, texture_utils, prf_utils\n",
    "from utils import default_paths2 as default_paths\n",
    "from model_fitting import fwrf_fit\n",
    "gabor_texture_feat_path = default_paths.gabor_texture_feat_path\n",
    "\n",
    "class texture_feature_extractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Module to compute higher-order texture statistics of input images (similar to Portilla & Simoncelli style texture model), \n",
    "    within specified area of space.\n",
    "    Builds off lower-level feature maps for various orientation/spatial frequency bands, extracted using the modules specified \n",
    "    in '_fmaps_fn_complex' and '_fmaps_fn_simple' (which should be Gabor filtering modules)\n",
    "    Can specify different subsets of features to include/exclude (i.e. pixel-level stats, simple/complex cells, cross-correlations, \n",
    "    auto-correlations)\n",
    "    \n",
    "    Inputs to the forward pass are images and pRF parameters of interest [x,y,sigma].\n",
    "    \"\"\"\n",
    "    def __init__(self,_fmaps_fn_complex, _fmaps_fn_simple, sample_batch_size=100, \\\n",
    "                 autocorr_output_pix=3, n_prf_sd_out=2, aperture=1.0, \\\n",
    "                 feature_types_exclude=None,  do_varpart=False, group_all_hl_feats=False, \\\n",
    "                 compute_features=True, device=None):\n",
    "        \n",
    "        super(texture_feature_extractor, self).__init__()\n",
    "        \n",
    "        self.fmaps_fn_complex = _fmaps_fn_complex\n",
    "        self.fmaps_fn_simple = _fmaps_fn_simple\n",
    "#         dtype = torch_utils.get_value(next(_fmaps_fn_complex.parameters())).dtype \n",
    "        self.n_sf = _fmaps_fn_simple.n_sf\n",
    "        self.n_ori = _fmaps_fn_simple.n_ori\n",
    "        self.n_phases = _fmaps_fn_simple.n_phases\n",
    "\n",
    "        self.sample_batch_size = sample_batch_size\n",
    "        self.autocorr_output_pix = autocorr_output_pix\n",
    "        self.n_prf_sd_out = n_prf_sd_out\n",
    "        self.aperture = aperture\n",
    "        self.device = device      \n",
    "        \n",
    "        self.do_varpart = do_varpart\n",
    "        self.group_all_hl_feats = group_all_hl_feats       \n",
    "        \n",
    "        self.update_feature_list(feature_types_exclude)\n",
    "        self.do_pca = False\n",
    "        \n",
    "        # if compute features is false, this means the features are already generated, so will be looking for a \n",
    "        # saved h5py file of pre-computed features. If true, will run the extraction step now.\n",
    "        self.compute_features = compute_features\n",
    "        \n",
    "        if not self.compute_features:\n",
    "            self.features_file = os.path.join(gabor_texture_feat_path, 'S%d_features_each_prf_%dori_%dsf.h5py'%(subject, self.n_ori, self.n_sf))\n",
    "            if not os.path.exists(self.features_file):\n",
    "                raise RuntimeError('Looking at %s for precomputed features, not found.'%self.features_file)                \n",
    "            self.features_each_prf = None\n",
    "        \n",
    "    def init_for_fitting(self, image_size, models=None, dtype=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Additional initialization operations.\n",
    "        \"\"\"\n",
    "        # These two methods make sure that the 'resolutions_each_sf' property of the two feature extractors\n",
    "        # are populated with the correct feature maps sizes for this image size.\n",
    "        self.fmaps_fn_complex.get_fmaps_sizes(image_size)\n",
    "        self.fmaps_fn_simple.get_fmaps_sizes(image_size)      \n",
    "        self.max_features = self.n_features_total            \n",
    "\n",
    "    def update_feature_list(self, feature_types_exclude):\n",
    "        \n",
    "        feature_types_all = ['pixel_stats', 'complex_feature_means', 'simple_feature_means',\\\n",
    "                         'complex_feature_autocorrs','simple_feature_autocorrs',\\\n",
    "                         'complex_within_scale_crosscorrs','simple_within_scale_crosscorrs',\\\n",
    "                         'complex_across_scale_crosscorrs','simple_across_scale_crosscorrs']\n",
    "        self.feature_types_all = feature_types_all\n",
    "        ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, self.n_ori)] for oo1 in range(self.n_ori) if oo1<self.n_ori-1])\n",
    "        n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "        feature_type_dims = [4,self.n_ori*self.n_sf, self.n_ori*self.n_sf*self.n_phases, \\\n",
    "                              self.n_ori*self.n_sf*self.autocorr_output_pix**2, self.n_ori*self.n_sf*self.n_phases*self.autocorr_output_pix**2, \\\n",
    "                              self.n_sf*n_ori_pairs, self.n_sf*n_ori_pairs*self.n_phases, (self.n_sf-1)*self.n_ori**2, (self.n_sf-1)*self.n_ori**2*self.n_phases]\n",
    "        self.feature_type_dims_all = feature_type_dims        \n",
    "         \n",
    "        # Decide which features to ignore, or use all features      \n",
    "        if feature_types_exclude is None:\n",
    "            feature_types_exclude = []\n",
    "        # a few shorthands for ignoring sets of features at a time\n",
    "        if 'crosscorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['complex_within_scale_crosscorrs','simple_within_scale_crosscorrs','complex_across_scale_crosscorrs','simple_across_scale_crosscorrs'])\n",
    "        if 'autocorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['complex_feature_autocorrs','simple_feature_autocorrs'])\n",
    "        if 'pixel' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['pixel_stats'])\n",
    "        self.feature_types_exclude = feature_types_exclude\n",
    "        print('Feature types to exclude from the model:')\n",
    "        print(self.feature_types_exclude)    \n",
    "        \n",
    "        self.feature_types_include  = [ff for ff in feature_types_all if not ff in self.feature_types_exclude]\n",
    "        if len(self.feature_types_include)==0:\n",
    "            raise ValueError('you have specified too many features to exclude, and now you have no features left! aborting.')\n",
    "            \n",
    "        feature_dims_include = [feature_type_dims[fi] for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in self.feature_types_exclude]\n",
    "        # how many features will be needed, in total?\n",
    "        self.n_features_total = np.sum(feature_dims_include)\n",
    "        \n",
    "        # numbers that define which feature types are in which column\n",
    "        self.feature_column_labels = np.squeeze(np.concatenate([fi*np.ones([1,feature_dims_include[fi]]) for fi in range(len(feature_dims_include))], axis=1).astype('int'))\n",
    "        assert(np.size(self.feature_column_labels)==self.n_features_total)\n",
    "        \n",
    "        if self.group_all_hl_feats:\n",
    "            # In this case pretend there are just two groups of features:\n",
    "            # Lower-level which includes pixel and gabor-like.\n",
    "            # Higher-level which includes all autocorrelations and cross-correlations. \n",
    "            # This makes it simpler to do variance partition analysis.\n",
    "            # if do_varpart=False, this does nothing.\n",
    "            assert(len(self.feature_types_exclude)==0) # the following lines won't make sense if any features were missing, so check this\n",
    "            self.feature_column_labels[self.feature_column_labels<=2] = 0\n",
    "            self.feature_column_labels[self.feature_column_labels>2] = 1\n",
    "            self.feature_group_names = ['lower-level', 'higher-level']\n",
    "        else:\n",
    "            self.feature_group_names = self.feature_types_include\n",
    "\n",
    "    def get_partial_versions(self):\n",
    "        \n",
    "        if not hasattr(self, 'max_features'):\n",
    "            raise RuntimeError('need to run init_for_fitting first')\n",
    "            \n",
    "        n_feature_types = len(self.feature_group_names)\n",
    "        partial_version_names = ['full_model'] \n",
    "        masks = np.ones([1,self.n_features_total])\n",
    "        \n",
    "        if self.do_varpart and n_feature_types>1:\n",
    "            \n",
    "            # \"Partial versions\" will be listed as: [full model, model w only first set of features, model w only second set, ...             \n",
    "            partial_version_names += ['just_%s'%ff for ff in self.feature_group_names]\n",
    "            masks2 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels==ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "            masks = np.concatenate((masks, masks2), axis=0)\n",
    "            \n",
    "            if n_feature_types > 2:\n",
    "                # if more than two types, also include models where we leave out first set of features, leave out second set of features...]\n",
    "                partial_version_names += ['leave_out_%s'%ff for ff in self.feature_group_names]           \n",
    "                masks3 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels!=ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "                masks = np.concatenate((masks, masks3), axis=0)           \n",
    "        \n",
    "        # masks always goes [n partial versions x n total features]\n",
    "        return masks, partial_version_names\n",
    "        \n",
    "    def load_precomputed_features(self, image_inds):\n",
    "    \n",
    "        print('Loading pre-computed features from %s'%self.features_file)\n",
    "        t = time.time()\n",
    "        with h5py.File(self.features_file, 'r') as data_set:\n",
    "#             values = np.copy(data_set['/features'])\n",
    "            values = np.copy(data_set['/features'][0:100])\n",
    "            data_set.close() \n",
    "        elapsed = time.time() - t\n",
    "        print('Took %.5f seconds to load file'%elapsed)\n",
    "        \n",
    "        self.features_each_prf = values[image_inds,:,:]\n",
    "        \n",
    "        print('Size of features array for this image set is:')\n",
    "        print(self.features_each_prf.shape)\n",
    "        \n",
    "    def clear_big_features(self):\n",
    "        \n",
    "        if self.compute_features:\n",
    "            print('Clear maps fn')\n",
    "        else:\n",
    "            print('Clearing precomputed features from memory.')\n",
    "            self.features_each_prf = None\n",
    "       \n",
    "    def forward(self, images, prf_params, prf_model_index, fitting_mode=True):\n",
    "        \n",
    "        if isinstance(prf_params, torch.Tensor):\n",
    "            prf_params = torch_utils.get_value(prf_params)\n",
    "        assert(np.size(prf_params)==3)\n",
    "        prf_params = np.squeeze(prf_params)\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = torch_utils.get_value(images)\n",
    "             \n",
    "        if not self.compute_features:\n",
    "            \n",
    "            # Load from file the features for this set of images\n",
    "            # In this case, the item passed in through \"images\" must actually be the indices of the images to use, not images themselves.\n",
    "            # Check to make sure this is the case.\n",
    "            assert(len(images.shape)==1)\n",
    "            image_inds = images\n",
    "            if self.features_each_prf is None:\n",
    "                self.load_precomputed_features(image_inds)\n",
    "            else:\n",
    "                assert(self.features_each_prf.shape[0]==len(image_inds))\n",
    "            \n",
    "             # Taking the features for the desired prf model\n",
    "            features = self.features_each_prf[:,:,prf_model_index]\n",
    "            features = torch_utils._to_torch(features, self.device)\n",
    "            \n",
    "            # Choosing which of these columns to include in model (might be all)\n",
    "            feature_column_labels_all = np.squeeze(np.concatenate([fi*np.ones([1,self.feature_type_dims_all[fi]]) for fi in range(len(self.feature_type_dims_all))], axis=1).astype('int'))\n",
    "            all_feat = OrderedDict()\n",
    "            for fi, ff in enumerate(self.feature_types_all):\n",
    "                if ff in self.feature_types_include:\n",
    "                    all_feat[ff] = features[:,feature_column_labels_all==fi]\n",
    "                else:\n",
    "                    all_feat[ff] = None\n",
    "                    \n",
    "        else:\n",
    "            \n",
    "            if not hasattr(self.fmaps_fn_simple, 'resolutions_each_sf'):\n",
    "                raise RuntimeError('Need to run init_for_fitting first')\n",
    "\n",
    "            if 'pixel_stats' in self.feature_types_include:\n",
    "                print('Computing pixel-level statistics...')    \n",
    "                t=time.time()\n",
    "                x,y,sigma = prf_params\n",
    "                n_pix=np.shape(images)[2]\n",
    "                g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=self.aperture, dtype=np.float32)\n",
    "                spatial_weights = g[2][0]\n",
    "                wmean, wvar, wskew, wkurt = texture_utils.get_weighted_pixel_features(images, spatial_weights, device=self.device)\n",
    "                pix_feat = torch.cat((wmean, wvar, wskew, wkurt), axis=1)\n",
    "                elapsed =  time.time() - t\n",
    "                print('time elapsed = %.5f'%elapsed)\n",
    "            else:\n",
    "                pix_feat = None\n",
    "\n",
    "            if 'complex_feature_means' in self.feature_types_include:\n",
    "                print('Computing complex cell features...')\n",
    "                t = time.time()\n",
    "                complex_feature_means = get_avg_features_in_prf(self.fmaps_fn_complex, images, prf_params,\\\n",
    "                                                                sample_batch_size=self.sample_batch_size, \\\n",
    "                                                                aperture=self.aperture, device=self.device, to_numpy=False)\n",
    "                elapsed =  time.time() - t\n",
    "                print('time elapsed = %.5f'%elapsed)\n",
    "            else:\n",
    "                complex_feature_means = None\n",
    "\n",
    "            if 'simple_feature_means' in self.feature_types_include:\n",
    "                print('Computing simple cell features...')\n",
    "                t = time.time()\n",
    "                simple_feature_means = get_avg_features_in_prf(self.fmaps_fn_simple, images,  prf_params,\\\n",
    "                                                               sample_batch_size=self.sample_batch_size, \\\n",
    "                                                               aperture=self.aperture,  device=self.device, to_numpy=False)\n",
    "                elapsed =  time.time() - t\n",
    "                print('time elapsed = %.5f'%elapsed)\n",
    "            else:\n",
    "                simple_feature_means = None\n",
    "\n",
    "            # To save time, decide now whether any autocorrelation or cross-correlation features are desired. If not, will skip a bunch of the slower computations.     \n",
    "            self.include_crosscorrs = np.any(['crosscorr' in ff for ff in self.feature_types_include])\n",
    "            self.include_autocorrs = np.any(['autocorr' in ff for ff in self.feature_types_include])\n",
    "\n",
    "            if self.include_autocorrs and self.include_crosscorrs:\n",
    "                print('Computing higher order correlations...')\n",
    "            elif self.include_crosscorrs:\n",
    "                print('Computing higher order correlations (SKIPPING AUTOCORRELATIONS)...')\n",
    "            elif self.include_autocorrs:\n",
    "                print('Computing higher order correlations (SKIPPING CROSSCORRELATIONS)...')\n",
    "            else:\n",
    "                print('SKIPPING HIGHER-ORDER CORRELATIONS...')    \n",
    "            t = time.time()\n",
    "            complex_feature_autocorrs, simple_feature_autocorrs, \\\n",
    "            complex_within_scale_crosscorrs, simple_within_scale_crosscorrs, \\\n",
    "            complex_across_scale_crosscorrs, simple_across_scale_crosscorrs = get_higher_order_features(self.fmaps_fn_complex, self.fmaps_fn_simple, \\\n",
    "                                                                                                        images, prf_params=prf_params, \n",
    "                                                                                                        sample_batch_size=self.sample_batch_size, \\\n",
    "                                                                                                        include_autocorrs=self.include_autocorrs, \\\n",
    "                                                                                                        include_crosscorrs=self.include_crosscorrs, \n",
    "                                                                                                        autocorr_output_pix=self.autocorr_output_pix, \\\n",
    "                                                                                                        n_prf_sd_out=self.n_prf_sd_out, \n",
    "                                                                                                        aperture=self.aperture,  device=self.device)\n",
    "            elapsed =  time.time() - t\n",
    "            print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "            all_feat = OrderedDict({'pixel_stats': pix_feat, 'complex_feature_means':complex_feature_means, 'simple_feature_means':simple_feature_means, \n",
    "                        'complex_feature_autocorrs': complex_feature_autocorrs, 'simple_feature_autocorrs': simple_feature_autocorrs, \n",
    "                        'complex_within_scale_crosscorrs': complex_within_scale_crosscorrs, 'simple_within_scale_crosscorrs':simple_within_scale_crosscorrs,\n",
    "                        'complex_across_scale_crosscorrs': complex_across_scale_crosscorrs, 'simple_across_scale_crosscorrs':simple_across_scale_crosscorrs})\n",
    "\n",
    "        # Now concatenating everything to a big matrix\n",
    "        feature_names_full = list(all_feat.keys())\n",
    "        feature_names = [fname for fname in feature_names_full if fname in self.feature_types_include]\n",
    "        assert(feature_names==self.feature_types_include) # double check here that the order is correct\n",
    "        \n",
    "        for ff, feature_name in enumerate(feature_names):   \n",
    "            assert(all_feat[feature_name] is not None)\n",
    "            if ff==0:\n",
    "                all_feat_concat = all_feat[feature_name]\n",
    "            else:               \n",
    "                all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "\n",
    "        assert(all_feat_concat.shape[1]==self.n_features_total)\n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        print('Feature types included are:')\n",
    "        print(feature_names)\n",
    "\n",
    "        if torch.any(torch.isnan(all_feat_concat)):\n",
    "            print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "        if torch.any(torch.all(all_feat_concat==0, axis=0)):\n",
    "            # Note the zero columns here are not a bug, they happen because number of autocorr features\n",
    "            # varies with prf size/position. Print the number of zero columns anyway...\n",
    "            print('There are %d zeros columns in feature matrix'%np.sum(torch_utils.get_value(torch.all(all_feat_concat==0, axis=0))))\n",
    "\n",
    "        feature_inds_defined = np.ones((self.n_features_total,), dtype=bool)\n",
    "            \n",
    "        return all_feat_concat, feature_inds_defined\n",
    "    \n",
    "    \n",
    "\n",
    "def get_avg_features_in_prf(_fmaps_fn, images, prf_params, sample_batch_size, aperture, device, to_numpy=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    For a given set of images and a specified pRF position and size, compute the mean (weighted by pRF)\n",
    "    in each feature map channel. Returns [nImages x nFeatures]\n",
    "    This could be done inside the get_higher_order_features fn, but it is nice to keep them separate in case\n",
    "    we just want to run this (faster) part.\n",
    "    \"\"\"\n",
    "    \n",
    "    dtype = images.dtype.type    \n",
    "    x,y,sigma = prf_params\n",
    "    n_trials = images.shape[0]\n",
    "    n_features = _fmaps_fn.n_features\n",
    "    fmaps_rez = _fmaps_fn.resolutions_each_sf\n",
    "\n",
    "    features = np.zeros(shape=(n_trials, n_features), dtype=dtype)\n",
    "    if to_numpy==False:\n",
    "         features = torch_utils._to_torch(features, device=device)\n",
    "\n",
    "    # Define the RF for this \"model\" version - at several resolutions.\n",
    "    _prfs = [torch_utils._to_torch(prf_utils.make_gaussian_mass(x, y, sigma, n_pix, size=aperture, \\\n",
    "                              dtype=dtype)[2], device=device) for n_pix in fmaps_rez]\n",
    "\n",
    "    # To make full design matrix for all trials, first looping over trials in batches to get the features\n",
    "    # Only reason to loop is memory constraints, because all trials is big matrices.\n",
    "    t = time.time()\n",
    "    n_batches = np.ceil(n_trials/sample_batch_size)\n",
    "    bb=-1\n",
    "    for rt,rl in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "\n",
    "        bb=bb+1\n",
    "\n",
    "        # Multiplying feature maps by RFs here. \n",
    "        # Feature maps in _fm go [nTrials x nFeatures(orientations) x nPixels x nPixels]\n",
    "        # Spatial RFs in _prfs go [nPixels x nPixels]\n",
    "        # Once we multiply, get [nTrials x nFeatures]\n",
    "        # note this is concatenating SFs together from low (smallest maps) to high (biggest maps). \n",
    "        # Cycles through all orient channels in order for first SF, then again for next SF, etc.\n",
    "        _features = torch.cat([torch.tensordot(_fm, _prf, dims=[[2,3], [0,1]]) \\\n",
    "                               for _fm,_prf in zip(_fmaps_fn(torch_utils._to_torch(images[rt], \\\n",
    "                                       device=device)), _prfs)], dim=1) # [#samples, #features]\n",
    "\n",
    "        # Add features for this batch to full design matrix over all trials\n",
    "        if to_numpy:\n",
    "            features[rt] = torch_utils.get_value(_features)\n",
    "        else:\n",
    "            features[rt] = _features\n",
    "\n",
    "        elapsed = time.time() - t\n",
    "\n",
    "    return features\n",
    "\n",
    " \n",
    "\n",
    "def get_higher_order_features(_fmaps_fn_complex, _fmaps_fn_simple, images, prf_params, sample_batch_size=20, include_autocorrs=True, include_crosscorrs=True, autocorr_output_pix=7, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute all higher-order features (cross-spatial and cross-feature correlations) for a batch of images.\n",
    "    Input the functions that define first level feature maps (simple and complex cells), and prf parameters.\n",
    "    Returns arrays of each higher order feature.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')    \n",
    "        \n",
    "    n_trials = np.shape(images)[0]\n",
    "    \n",
    "    assert(np.mod(autocorr_output_pix,2)==1) # must be odd!\n",
    "\n",
    "    n_features_simple = _fmaps_fn_simple.n_features\n",
    "    n_features_complex = _fmaps_fn_complex.n_features \n",
    "    fmaps_rez = _fmaps_fn_simple.resolutions_each_sf\n",
    "    \n",
    "    n_sf = len(fmaps_rez)\n",
    "    n_ori = int(n_features_complex/n_sf)\n",
    "    n_phases = 2\n",
    "    \n",
    "    # all pairs of different orientation channels.\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "\n",
    "    if include_autocorrs:\n",
    "        complex_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, autocorr_output_pix**2], device=device)\n",
    "        simple_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, n_phases, autocorr_output_pix**2], device=device)\n",
    "    else:\n",
    "        complex_feature_autocorrs = None\n",
    "        simple_feature_autocorrs = None\n",
    "    \n",
    "    if include_crosscorrs:\n",
    "        complex_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device)\n",
    "        simple_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_phases, n_ori_pairs], device=device)\n",
    "        complex_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_ori, n_ori], device=device)\n",
    "        simple_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_phases, n_ori, n_ori], device=device) # only done for pairs of neighboring SF.\n",
    "    else:\n",
    "        complex_within_scale_crosscorrs = None\n",
    "        simple_within_scale_crosscorrs = None\n",
    "        complex_across_scale_crosscorrs = None\n",
    "        simple_across_scale_crosscorrs = None\n",
    "        \n",
    "    if include_autocorrs or include_crosscorrs:\n",
    "        \n",
    "        x,y,sigma = prf_params\n",
    "\n",
    "        bb=-1\n",
    "        for batch_inds, batch_size_actual in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "            bb=bb+1\n",
    "\n",
    "            fmaps_complex = _fmaps_fn_complex(torch_utils._to_torch(images[batch_inds],device=device))   \n",
    "            fmaps_simple =  _fmaps_fn_simple(torch_utils._to_torch(images[batch_inds],device=device))\n",
    "\n",
    "            # First looping over frequency (scales)\n",
    "            for ff in range(n_sf):\n",
    "\n",
    "                # Scale specific things - get the prf at this resolution of interest\n",
    "                n_pix = fmaps_rez[ff]\n",
    "                g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "                spatial_weights = g[2][0]\n",
    "\n",
    "                patch_bbox_rect = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=False)\n",
    "                # for autocorrelation, forcing the input region to be square\n",
    "                patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True)\n",
    "\n",
    "                # Loop over orientation channels\n",
    "                xx=-1\n",
    "                for oo1 in range(n_ori):       \n",
    "\n",
    "\n",
    "                    # Simple cell responses - loop over two phases per orient.\n",
    "                    for pp in range(n_phases):\n",
    "                        filter_ind = n_phases*oo1+pp  # orients and phases are both listed in the same dimension of filters matrix               \n",
    "                        simple1 = fmaps_simple[ff][:,filter_ind,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                        # Simple cell autocorrelations.\n",
    "                        if include_autocorrs:\n",
    "                            auto_corr = weighted_auto_corr_2d(simple1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix, subtract_patch_mean = True, enforce_size=True, device=device)\n",
    "                            simple_feature_autocorrs[batch_inds,ff,oo1,pp,:] = torch.reshape(auto_corr, [batch_size_actual, autocorr_output_pix**2])\n",
    "\n",
    "                    # Complex cell responses\n",
    "                    complex1 = fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                    # Complex cell autocorrelation (correlation w spatially shifted versions of itself)\n",
    "                    if include_autocorrs:\n",
    "                        auto_corr = weighted_auto_corr_2d(complex1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix, subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "                        complex_feature_autocorrs[batch_inds,ff,oo1,:] = torch.reshape(auto_corr, [batch_size_actual, autocorr_output_pix**2])\n",
    "\n",
    "                    if include_crosscorrs:\n",
    "                        # Within-scale correlations - compare resp at orient==oo1 to responses at all other orientations, same scale.\n",
    "                        for oo2 in np.arange(oo1+1, n_ori):            \n",
    "                            xx = xx+1 \n",
    "                            assert(oo1==ori_pairs[xx,0] and oo2==ori_pairs[xx,1])\n",
    "\n",
    "                            complex2 = fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])      \n",
    "\n",
    "                            # Complex cell within-scale cross correlations\n",
    "                            cross_corr = weighted_cross_corr_2d(complex1, complex2, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "\n",
    "                            complex_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                            # Simple cell within-scale cross correlations\n",
    "                            for pp in range(n_phases):\n",
    "                                filter_ind = n_phases*oo2+pp\n",
    "                                simple2 = fmaps_simple[ff][:,filter_ind,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                                cross_corr = weighted_cross_corr_2d(simple1, simple2, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                simple_within_scale_crosscorrs[batch_inds,ff,pp,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                        # Cross-scale correlations - for these we care about same ori to same ori, so looping over all ori.\n",
    "                        # Only for neighboring scales, so the first level doesn't get one\n",
    "                        if ff>0:\n",
    "\n",
    "                            for oo2 in range(n_ori):\n",
    "\n",
    "                                # Complex cell response for neighboring scale\n",
    "                                complex2_neighborscale = fmaps_complex[ff-1][:,oo2,:,:].view([batch_size_actual,1,fmaps_rez[ff-1], -1])\n",
    "                                # Resize so that it can be compared w current scale\n",
    "                                complex2_neighborscale = torch.nn.functional.interpolate(complex2_neighborscale, [n_pix, n_pix], mode='bilinear', align_corners=True)\n",
    "\n",
    "                                cross_corr = weighted_cross_corr_2d(complex1, complex2_neighborscale, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                complex_across_scale_crosscorrs[batch_inds,ff-1, oo1, oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                                for pp in range(n_phases):\n",
    "                                    filter_ind = n_phases*oo2+pp\n",
    "                                    # Simple cell response for neighboring scale\n",
    "                                    simple2_neighborscale = fmaps_simple[ff-1][:,filter_ind,:,:].view([batch_size_actual,1,fmaps_rez[ff-1], -1])\n",
    "                                    simple2_neighborscale = torch.nn.functional.interpolate(simple2_neighborscale, [n_pix, n_pix], mode='bilinear', align_corners=True)\n",
    "\n",
    "                                    cross_corr = weighted_cross_corr_2d(simple1, simple2_neighborscale, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                    simple_across_scale_crosscorrs[batch_inds,ff-1, pp, oo1, oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "    if include_crosscorrs:\n",
    "        simple_within_scale_crosscorrs = torch.reshape(simple_within_scale_crosscorrs, [n_trials, -1])\n",
    "        simple_across_scale_crosscorrs = torch.reshape(simple_across_scale_crosscorrs, [n_trials, -1])\n",
    "        complex_within_scale_crosscorrs = torch.reshape(complex_within_scale_crosscorrs, [n_trials, -1])\n",
    "        complex_across_scale_crosscorrs = torch.reshape(complex_across_scale_crosscorrs, [n_trials, -1])\n",
    "    if include_autocorrs:\n",
    "        simple_feature_autocorrs = torch.reshape(simple_feature_autocorrs, [n_trials, -1])\n",
    "        complex_feature_autocorrs = torch.reshape(complex_feature_autocorrs, [n_trials, -1])\n",
    "\n",
    "    return complex_feature_autocorrs, simple_feature_autocorrs, complex_within_scale_crosscorrs, simple_within_scale_crosscorrs, complex_across_scale_crosscorrs, simple_across_scale_crosscorrs\n",
    "\n",
    "\n",
    "\n",
    "def weighted_auto_corr_2d(images, spatial_weights, patch_bbox=None, output_pix=None, subtract_patch_mean=False, enforce_size=False, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute autocorrelation of a batch of images, weighting the pixels based on the values in spatial_weights (could be for instance a pRF definition for a voxel).\n",
    "    Can optionally specify a square patch of the image to compute over, based on \"patch_bbox\" params. Otherwise use whole image.\n",
    "    Using fft method to compute, should be fast.\n",
    "    Input parameters:\n",
    "        patch_bbox: (optional) bounding box of the patch to use for this calculation. [xmin xmax ymin ymax], see get_bbox_from_prf\n",
    "        output_pix: the size of the autocorrelation matrix output by this function. If this is an even number, the output size is this value +1. Achieved by cropping out the center of the final autocorrelation \n",
    "            matrix  (note that the full image patch is still used in computing the autocorrelation, but just the center values are returned).\n",
    "            If None, then returns the full autocorrelation matrix (same size as image patch.)\n",
    "        subtract_patch_mean: subtract weighted mean of image before computing autocorr?\n",
    "        enforce_size: if image patch is smaller than desired output, should we pad w zeros so that it has to be same size?\n",
    "    Returns:\n",
    "        A matrix describing the correlation of the image and various spatially shifted versions of it.\n",
    "    Note this version is slightly different than the one in texture_utils.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')        \n",
    "    if isinstance(images, np.ndarray):\n",
    "        images = torch_utils._to_torch(images, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)\n",
    "            \n",
    "    if len(np.shape(images))==2:\n",
    "        # pretend the batch and channel dims exist, for 2D input only (3D won't work)\n",
    "        single_image=True\n",
    "        images = images.view([1,1,images.shape[0],-1])\n",
    "    else:\n",
    "        single_image=False\n",
    "        \n",
    "    # have to be same size\n",
    "    assert(images.shape[2]==spatial_weights.shape[0] and images.shape[3]==spatial_weights.shape[1])\n",
    "    # images is [batch_size x n_channels x nPix x nPix]\n",
    "    batch_size = images.shape[0]\n",
    "    n_channels = images.shape[1]    \n",
    "   \n",
    "    if patch_bbox is not None:    \n",
    "        [xmin, xmax, ymin, ymax] = patch_bbox\n",
    "        # first crop out the region of the image that's currently of interest\n",
    "        images = images[:,:,xmin:xmax, ymin:ymax]\n",
    "        # crop same region from spatial weights matrix\n",
    "        spatial_weights = spatial_weights[xmin:xmax, ymin:ymax]\n",
    "\n",
    "    # make sure these sum to 1\n",
    "    if not torch.sum(spatial_weights)==0.0:\n",
    "        spatial_weights = spatial_weights/torch.sum(spatial_weights)   \n",
    "   \n",
    "    spatial_weights = spatial_weights.view([1,1,spatial_weights.shape[0],-1]).expand([batch_size,n_channels,-1,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "    \n",
    "    # compute autocorrelation of this image patch\n",
    "    if subtract_patch_mean:\n",
    "\n",
    "        wmean = torch.sum(torch.sum(images * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean = wmean.view([batch_size,-1,1,1]).expand([-1,-1,images.shape[2],images.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        \n",
    "        weighted_images = (images - wmean) * torch.sqrt(spatial_weights) # square root of the weights here because they will get squared again in next operation\n",
    "        \n",
    "        auto_corr = torch.fft.fftshift(torch.real(torch.fft.ifft2(torch.abs(torch.fft.fft2(weighted_images, dim=[2,3]))**2, dim=[2,3])), dim=[2,3]);\n",
    "    else:\n",
    "        weighted_images = images * torch.sqrt(spatial_weights)\n",
    "        auto_corr = torch.fft.fftshift(torch.real(torch.fft.ifft2(torch.abs(torch.fft.fft2(weighted_images, dim=[2,3]))**2, dim=[2,3])), dim=[2,3]);\n",
    "\n",
    "    if output_pix is not None:\n",
    "\n",
    "        # crop out just the center region\n",
    "        new_center = int(np.floor(auto_corr.shape[2]/2))\n",
    "        n_pix_out = np.min([int(np.floor(output_pix/2)), np.min([new_center, auto_corr.shape[2]-new_center])])\n",
    "        auto_corr = auto_corr[:,:,new_center-n_pix_out:new_center+n_pix_out+1, new_center-n_pix_out:new_center+n_pix_out+1]        \n",
    "    \n",
    "    if enforce_size and not (np.shape(auto_corr)[2]==output_pix or np.shape(auto_corr)[2]==output_pix+1):\n",
    "        \n",
    "        # just pad w zeros if want same size.\n",
    "        pix_diff = output_pix - np.shape(auto_corr)[2]   \n",
    "        auto_corr = torch.nn.functional.pad(auto_corr, [int(np.floor(pix_diff/2)), int(np.ceil(pix_diff/2)), int(np.floor(pix_diff/2)), int(np.ceil(pix_diff/2))], mode='constant', value=0)\n",
    "        assert(np.shape(auto_corr)[2]==output_pix and np.shape(auto_corr)[3]==output_pix)\n",
    "\n",
    "    if single_image:\n",
    "        auto_corr = torch.squeeze(auto_corr)\n",
    "        \n",
    "    return auto_corr\n",
    "\n",
    "def weighted_cross_corr_2d(images1, images2, spatial_weights, patch_bbox=None, subtract_patch_mean=True, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute cross-correlation of two identically-sized images, weighting the pixels based on the values in spatial_weights (could be for instance a pRF definition for a voxel).\n",
    "    Can optionally specify a square patch of the image to compute over, based on \"patch_bbox\" params. Otherwise use whole image.\n",
    "    Basically a dot product of image values.\n",
    "    Input parameters:\n",
    "        patch_bbox: (optional) bounding box of the patch to use for this calculation. [xmin xmax ymin ymax], see get_bbox_from_prf\n",
    "        subtract_patch_mean: do you want to subtract the weighted mean of image patch before computing?\n",
    "    Returns:\n",
    "        A single value that captures correlation between images (zero spatial shift)\n",
    "    Note this version is slightly different than the one in texture_utils.\n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')  \n",
    "    if isinstance(images1, np.ndarray):\n",
    "        images1 = torch_utils._to_torch(images1, device)\n",
    "    if isinstance(images2, np.ndarray):\n",
    "        images2 = torch_utils._to_torch(images2, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)      \n",
    "    \n",
    "    if len(np.shape(images1))==2:\n",
    "        # pretend the batch and channel dims exist, for 2D input only (3D won't work)\n",
    "        single_image=True\n",
    "        images1 = images1.view([1,1,images1.shape[0],-1])\n",
    "        images2 = images2.view([1,1,images2.shape[0],-1])\n",
    "    else:\n",
    "        single_image=False\n",
    "        \n",
    "    # have to be same size\n",
    "    assert(images1.shape==images2.shape)\n",
    "    assert(images1.shape[2]==spatial_weights.shape[0] and images1.shape[3]==spatial_weights.shape[1])\n",
    "    assert(images2.shape[2]==spatial_weights.shape[0] and images2.shape[3]==spatial_weights.shape[1])\n",
    "    # images is [batch_size x n_channels x nPix x nPix]\n",
    "    batch_size = images1.shape[0]\n",
    "    n_channels = images1.shape[1]\n",
    "    \n",
    "\n",
    "    if patch_bbox is not None:\n",
    "        [xmin, xmax, ymin, ymax] = patch_bbox\n",
    "        # first crop out the region of the image that's currently of interest\n",
    "        images1 = images1[:,:,xmin:xmax, ymin:ymax]\n",
    "        images2 = images2[:,:,xmin:xmax, ymin:ymax]\n",
    "        # crop same region from spatial weights matrix\n",
    "        spatial_weights = spatial_weights[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    # make sure the wts sum to 1\n",
    "    if not torch.sum(spatial_weights)==0.0:\n",
    "        spatial_weights = spatial_weights/torch.sum(spatial_weights)\n",
    "    spatial_weights = spatial_weights.view([1,1,spatial_weights.shape[0],-1]).expand([batch_size,n_channels,-1,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "    \n",
    "    # compute cross-correlation\n",
    "    if subtract_patch_mean:\n",
    "        # subtract mean of each weighted image patch and take their dot product.\n",
    "        # this quantity is equal to weighted covariance (only true if mean-centered)\n",
    "        wmean1 = torch.sum(torch.sum(images1 * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean1 = wmean1.view([batch_size,-1,1,1]).expand([-1,-1,images1.shape[2],images1.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        wmean2 = torch.sum(torch.sum(images2 * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean2 = wmean2.view([batch_size,-1,1,1]).expand([-1,-1,images2.shape[2],images2.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        weighted_images1 = (images1 - wmean1) * torch.sqrt(spatial_weights) # square root of the weights here because they will get squared again in dot product operation.\n",
    "        weighted_images2 = (images2 - wmean2) * torch.sqrt(spatial_weights)\n",
    "\n",
    "        cross_corr = torch.sum(torch.sum(weighted_images1 * weighted_images2, dim=3), dim=2)    \n",
    "\n",
    "    else:\n",
    "        # dot product of raw (weighted) values\n",
    "        # this is closer to what scipy.signal.correlate2d will do (except this is weighted)\n",
    "        weighted_images1 = images1 * torch.sqrt(spatial_weights)\n",
    "        weighted_images2 = images2 * torch.sqrt(spatial_weights)\n",
    "        cross_corr = torch.sum(torch.sum(weighted_images1 * weighted_images2, dim=3), dim=2)      \n",
    "        \n",
    "    if single_image:\n",
    "        cross_corr = torch.squeeze(cross_corr)\n",
    "        \n",
    "    return cross_corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
