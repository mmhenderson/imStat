{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a319d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic modules\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "# import custom modules\n",
    "root_dir   = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(root_dir, 'code'))\n",
    "sys.path.append(os.path.join(root_dir, 'code', 'model_fitting'))\n",
    "from model_src import fwrf_fit, fwrf_predict\n",
    "import initialize_fitting\n",
    "\n",
    "fpX = np.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0e214c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=1\n",
    "roi=None\n",
    "\n",
    "ridge=1\n",
    "\n",
    "shuffle_images=False\n",
    "random_images=False\n",
    "random_voxel_data=False\n",
    "\n",
    "zscore_features=True\n",
    "nonlin_fn=False\n",
    "padding_mode='circular'\n",
    "\n",
    "n_ori=8\n",
    "n_sf=4\n",
    "up_to_sess=1\n",
    "debug=True\n",
    "\n",
    "fitting_type='simple_complex'\n",
    "include_crosscorrs=True\n",
    "include_autocorrs=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "926f7085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce GTX TITAN X\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n",
      "Will compute autocorrs\n",
      "\n",
      "Will compute crosscorrs\n",
      "\n",
      "Time Stamp: Jul-05-2021_1425\n",
      "\n",
      "Will save final output file to /user_data/mmhender/model_fits/S01/texture_ridge_8ori_4sf/Jul-05-2021_1425_DEBUG/\n",
      "\n",
      "3794 voxels of overlap between kastner and prf definitions, using prf defs\n",
      "unique values in retino labels:\n",
      "[-1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.\n",
      " 17. 18. 19. 20. 21. 22. 23. 24. 25.]\n",
      "0 voxels of overlap between face and place definitions, using place defs\n",
      "unique values in categ labels:\n",
      "[-1.  0. 26. 27. 28. 30. 31. 32. 33.]\n",
      "1535 voxels are defined (differently) in both retinotopic areas and category areas\n",
      "\n",
      "14913 voxels are defined across all areas, and will be used for analysis\n",
      "\n",
      "Loading numerical label/name mappings for all ROIs:\n",
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4']\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4', 'VO1', 'VO2', 'PHC1', 'PHC2', 'TO2', 'TO1', 'LO2', 'LO1', 'V3B', 'V3A', 'IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5', 'SPL1', 'FEF']\n",
      "[1, 2, 3, 4, 5]\n",
      "['OFA', 'FFA-1', 'FFA-2', 'mTL-faces', 'aTL-faces']\n",
      "[1, 2, 3]\n",
      "['OPA', 'PPA', 'RSC']\n",
      "\n",
      "Sizes of all defined ROIs in this subject:\n",
      "Region V1 has 2392 voxels. Includes subregions:\n",
      "['V1v', 'V1d']\n",
      "Region V2 has 2096 voxels. Includes subregions:\n",
      "['V2v', 'V2d']\n",
      "Region V3 has 1674 voxels. Includes subregions:\n",
      "['V3v', 'V3d']\n",
      "Region hV4 has 721 voxels. Includes subregions:\n",
      "['hV4']\n",
      "Region VO1-2 has 482 voxels. Includes subregions:\n",
      "['VO1', 'VO2']\n",
      "Region PHC1-2 has 382 voxels. Includes subregions:\n",
      "['PHC1', 'PHC2']\n",
      "Region LO1-2 has 488 voxels. Includes subregions:\n",
      "['LO2', 'LO1']\n",
      "Region TO1-2 has 339 voxels. Includes subregions:\n",
      "['TO2', 'TO1']\n",
      "Region V3ab has 965 voxels. Includes subregions:\n",
      "['V3B', 'V3A']\n",
      "Region IPS0-5 has 2155 voxels. Includes subregions:\n",
      "['IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5']\n",
      "Region SPL1 has 164 voxels. Includes subregions:\n",
      "['SPL1']\n",
      "Region FEF has 72 voxels. Includes subregions:\n",
      "['FEF']\n",
      "\n",
      "\n",
      "Region OFA has 355 voxels.\n",
      "Region FFA-1 has 484 voxels.\n",
      "Region FFA-2 has 310 voxels.\n",
      "Region mTL-faces has 0 voxels.\n",
      "Region aTL-faces has 159 voxels.\n",
      "Region OPA has 1611 voxels.\n",
      "Region PPA has 1033 voxels.\n",
      "Region RSC has 566 voxels.\n",
      "\n",
      "Loading images for subject 1\n",
      "\n",
      "image data size: (10000, 3, 227, 227) , dtype: uint8 , value range: 0 255\n",
      "Loading data for sessions 1:1...\n",
      "/lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR\n",
      "/lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR\n",
      "324\n",
      "/lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/betas_session01.nii.gz\n",
      "int16 -32768 32767 (750, 81, 104, 83)\n",
      "<beta> = 1.237, <sigma> = 1.391\n",
      "\n",
      "Size of full data set [nTrials x nVoxels] is:\n",
      "(750, 14913)\n",
      "Total number of voxels = 14913\n",
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n",
      "\n",
      "Possible lambda values are:\n",
      "[1.0000000e+00 4.2169652e+00 1.7782795e+01 7.4989418e+01 3.1622775e+02\n",
      " 1.3335215e+03 5.6234131e+03 2.3713736e+04 1.0000000e+05]\n"
     ]
    }
   ],
   "source": [
    "device = initialize_fitting.init_cuda()\n",
    "nsd_root, stim_root, beta_root, mask_root = initialize_fitting.get_paths()\n",
    "\n",
    "if ridge==True:       \n",
    "    # ridge regression, testing several positive lambda values (default)\n",
    "    model_name = 'texture_ridge_%dori_%dsf'%(n_ori, n_sf)\n",
    "else:        \n",
    "    # fixing lambda at zero, so it turns into ordinary least squares\n",
    "    model_name = 'texture_OLS_%dori_%dsf'%(n_ori, n_sf)\n",
    "\n",
    "if include_autocorrs==False:\n",
    "    print('Skipping autocorrs\\n')\n",
    "    model_name = model_name+'_no_autocorrelations'\n",
    "else:\n",
    "    print('Will compute autocorrs\\n')\n",
    "\n",
    "if include_crosscorrs==False:\n",
    "    print('Skipping crosscorrs\\n')\n",
    "    model_name = model_name+'_no_crosscorrelations'\n",
    "else:\n",
    "    print('Will compute crosscorrs\\n')   \n",
    "\n",
    "output_dir, fn2save = initialize_fitting.get_save_path(root_dir, subject, model_name, shuffle_images, random_images, random_voxel_data, debug)\n",
    "\n",
    "# decide what voxels to use  \n",
    "voxel_mask, voxel_index, voxel_roi, voxel_ncsnr, brain_nii_shape = initialize_fitting.get_voxel_info(mask_root, beta_root, subject, roi)\n",
    "\n",
    "# get all data and corresponding images, in two splits. always fixed set that gets left out\n",
    "trn_stim_data, trn_voxel_data, val_stim_single_trial_data, val_voxel_single_trial_data, \\\n",
    "    n_voxels, n_trials_val, image_order = initialize_fitting.get_data_splits(nsd_root, beta_root, stim_root, subject, voxel_mask, up_to_sess, \n",
    "                                                                             shuffle_images=shuffle_images, random_images=random_images, random_voxel_data=random_voxel_data)\n",
    "\n",
    "# Set up the filters\n",
    "_gaborizer_complex, _gaborizer_simple, _fmaps_fn_complex, _fmaps_fn_simple = initialize_fitting.get_feature_map_simple_complex_fn(n_ori, n_sf, padding_mode=padding_mode, device=device, nonlin_fn=nonlin_fn)\n",
    "\n",
    "# Params for the spatial aspect of the model (possible pRFs)\n",
    "#     aperture_rf_range=0.8 # using smaller range here because not sure what to do with RFs at edges...\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "# More params for fitting\n",
    "holdout_size, lambdas = initialize_fitting.get_fitting_pars(trn_voxel_data, zscore_features, ridge=ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "7d508491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_size = 619 (90.0%)\n",
      "dtype = <class 'numpy.float32'>\n",
      "device = cuda:0\n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "model 0\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.04753\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.90587\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.52780\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03235\n",
      "Final size of features concatenated is [688 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "model 1\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.04537\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.90793\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.52731\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03192\n",
      "Final size of features concatenated is [688 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "---------------------------------------\n",
      "total time = 3.693029s\n",
      "total throughput = 0.000248s/voxel\n",
      "voxel throughput = 0.000041s/voxel\n",
      "setup throughput = 0.003514s/model\n",
      "\n",
      "Done with training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### DO THE ACTUAL MODEL FITTING HERE ####\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "autocorr_output_pix=5\n",
    "n_prf_sd_out=2\n",
    "debug=True\n",
    "sample_batch_size=20\n",
    "voxel_batch_size=50\n",
    "\n",
    "include_crosscorrs = False\n",
    "include_autocorrs = False\n",
    "include_pixels = True\n",
    "include_simple = True\n",
    "include_complex = True\n",
    "\n",
    "feature_types_exclude = []\n",
    "if not include_pixels:\n",
    "    feature_types_exclude.append('pixel')\n",
    "if not include_simple:\n",
    "    feature_types_exclude.append('simple_feature_means')\n",
    "if not include_complex:\n",
    "    feature_types_exclude.append('complex_feature_means')\n",
    "if not include_autocorrs:\n",
    "    feature_types_exclude.append('autocorrs')\n",
    "if not include_crosscorrs:\n",
    "    feature_types_exclude.append('crosscorrs')\n",
    "\n",
    "    \n",
    "_texture_fn = texture_feature_extractor(_fmaps_fn_complex, _fmaps_fn_simple, sample_batch_size=sample_batch_size, feature_types_exclude=feature_types_exclude, autocorr_output_pix=autocorr_output_pix, n_prf_sd_out=n_prf_sd_out, aperture=aperture, device=device)\n",
    "\n",
    "best_losses, best_lambdas, best_params, feature_info = fit_texture_model_ridge(\n",
    "    trn_stim_data, trn_voxel_data, _texture_fn, models, lambdas, \\\n",
    "    zscore=zscore_features, voxel_batch_size=voxel_batch_size, holdout_size=holdout_size, shuffle=True, add_bias=True, debug=debug)\n",
    "# note there's also a shuffle param in the above fn call, that determines the nested heldout data for lambda and param selection. always using true.\n",
    "print('\\nDone with training\\n')\n",
    "\n",
    "_texture_fn.update_feature_list([])\n",
    "\n",
    "# best_losses, best_lambdas, best_params, feature_info = fit_texture_model_ridge(\n",
    "#     trn_stim_data, trn_voxel_data, _texture_fn, models, lambdas, \\\n",
    "#     zscore=zscore_features, voxel_batch_size=voxel_batch_size, holdout_size=holdout_size, shuffle=True, add_bias=True, debug=debug)\n",
    "# # note there's also a shuffle param in the above fn call, that determines the nested heldout data for lambda and param selection. always using true.\n",
    "# print('\\nDone with training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "fe158fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_texture_fn.feature_column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "3f1f7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from utils import numpy_utility, torch_utils\n",
    "from model_src import fwrf_fit\n",
    "\n",
    "class texture_feature_extractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Module to compute higher-order texture statistics of input images (similar to Portilla & Simoncelli style texture model), within specified area of space.\n",
    "    Builds off lower-level feature maps for various orientation/spatial frequency bands, extracted using the modules specified in '_fmaps_fn_complex' and '_fmaps_fn_simple'\n",
    "    Can specify different subsets of features to include (i.e. pixel-level stats, simple/complex cells, cross-correlations, auto-correlations)\n",
    "    Inputs to the forward pass are images and pRF parameters of interest [x,y,sigma]\n",
    "    \"\"\"\n",
    "    def __init__(self,_fmaps_fn_complex, _fmaps_fn_simple, sample_batch_size=100, feature_types_exclude=None, autocorr_output_pix=3, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "        \n",
    "        super(texture_feature_extractor, self).__init__()\n",
    "        \n",
    "        self.fmaps_fn_complex = _fmaps_fn_complex\n",
    "        self.fmaps_fn_simple = _fmaps_fn_simple\n",
    "        self.sample_batch_size = sample_batch_size\n",
    "        self.autocorr_output_pix = autocorr_output_pix\n",
    "        self.n_prf_sd_out = n_prf_sd_out\n",
    "        self.aperture = aperture\n",
    "        self.device = device       \n",
    "        \n",
    "        self.update_feature_list(feature_types_exclude)\n",
    "        \n",
    "    def update_feature_list(self, feature_types_exclude):\n",
    "        \n",
    "        feature_types_all = ['wmean','wvar','wskew','wkurt', 'complex_feature_means', 'simple_feature_means',\\\n",
    "                         'complex_feature_autocorrs','simple_feature_autocorrs',\\\n",
    "                         'complex_within_scale_crosscorrs','simple_within_scale_crosscorrs',\\\n",
    "                         'complex_across_scale_crosscorrs','simple_across_scale_crosscorrs']\n",
    "        feature_type_dims = [1,1,1,1,n_ori*n_sf, n_ori*n_sf*n_phases, \\\n",
    "                              n_ori*n_sf*autocorr_output_pix**2, n_ori*n_sf*n_phases*autocorr_output_pix**2, \\\n",
    "                              n_sf*n_ori_pairs, n_sf*n_ori_pairs*n_phases, (n_sf-1)*n_ori**2, (n_sf-1)*n_ori**2*n_phases]\n",
    "        \n",
    "        # decide which features to ignore, or use all features\n",
    "        self.feature_types_exclude = feature_types_exclude\n",
    "\n",
    "        # a few shorthands for ignoring sets of features at a time\n",
    "        if 'crosscorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['complex_within_scale_crosscorrs','simple_within_scale_crosscorrs','complex_across_scale_crosscorrs','simple_across_scale_crosscorrs'])\n",
    "        if 'autocorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['complex_feature_autocorrs','simple_feature_autocorrs'])\n",
    "        if 'pixel' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['wmean','wvar','wskew','wkurt'])\n",
    "\n",
    "        self.feature_types_include  = [ff for ff in feature_types_all if not ff in feature_types_exclude]\n",
    "        if len(self.feature_types_include)==0:\n",
    "            raise ValueError('you have specified too many features to exclude, and now you have no features left! aborting.')\n",
    "            \n",
    "        # how many features will be needed, in total?\n",
    "        self.n_features_total = np.sum([feature_type_dims[fi] for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in feature_types_exclude])\n",
    "        \n",
    "        # numbers that define which feature types are in which column\n",
    "        self.feature_column_labels = np.squeeze(np.concatenate([fi*np.ones([1,feature_type_dims[fi]]) for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in feature_types_exclude], axis=1).astype('int'))\n",
    "        assert(np.size(self.feature_column_labels)==self.n_features_total)\n",
    "\n",
    "    \n",
    "    def forward(self, images, prf_params):\n",
    "        \n",
    "        if isinstance(prf_params, torch.Tensor):\n",
    "            prf_params = torch_utils.get_value(prf_params)\n",
    "        assert(np.size(prf_params)==3)\n",
    "        prf_params = np.squeeze(prf_params)\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = torch_utils.get_value(images)\n",
    "\n",
    "        print('Computing pixel-level statistics...')    \n",
    "        t=time.time()\n",
    "        x,y,sigma = prf_params\n",
    "        n_pix=np.shape(images)[2]\n",
    "        g = numpy_utility.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=self.aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        wmean, wvar, wskew, wkurt = get_weighted_pixel_features(images, spatial_weights, device=device)\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "        print('Computing complex cell features...')\n",
    "        t = time.time()\n",
    "        complex_feature_means = fwrf_fit.get_features_in_prf(prf_params, self.fmaps_fn_complex , images=images, sample_batch_size=self.sample_batch_size, aperture=self.aperture, device=self.device, to_numpy=False)\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "        print('Computing simple cell features...')\n",
    "        t = time.time()\n",
    "        simple_feature_means = fwrf_fit.get_features_in_prf(prf_params,  self.fmaps_fn_simple, images=images, sample_batch_size=self.sample_batch_size, aperture=self.aperture,  device=self.device, to_numpy=False)\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "        # To save time, decide now whether any autocorrelation or cross-correlation features are desired. If not, will skip a bunch of the slower computations.     \n",
    "        self.include_crosscorrs = np.any(['crosscorr' in ff for ff in self.feature_types_include])\n",
    "        self.include_autocorrs = np.any(['autocorr' in ff for ff in self.feature_types_include])\n",
    "        \n",
    "        if self.include_autocorrs and self.include_crosscorrs:\n",
    "            print('Computing higher order correlations...')\n",
    "        elif self.include_crosscorrs:\n",
    "            print('Computing higher order correlations (SKIPPING AUTOCORRELATIONS)...')\n",
    "        elif self.include_autocorrs:\n",
    "            print('Computing higher order correlations (SKIPPING CROSSCORRELATIONS)...')\n",
    "        else:\n",
    "            print('SKIPPING HIGHER-ORDER CORRELATIONS...')    \n",
    "        t = time.time()\n",
    "        complex_feature_autocorrs, simple_feature_autocorrs, \\\n",
    "        complex_within_scale_crosscorrs, simple_within_scale_crosscorrs, \\\n",
    "        complex_across_scale_crosscorrs, simple_across_scale_crosscorrs = get_higher_order_features(self.fmaps_fn_complex, self.fmaps_fn_simple, images, prf_params=prf_params, \n",
    "                                                                                                    sample_batch_size=self.sample_batch_size, include_autocorrs=self.include_autocorrs, include_crosscorrs=self.include_crosscorrs, \n",
    "                                                                                                    autocorr_output_pix=self.autocorr_output_pix, n_prf_sd_out=self.n_prf_sd_out, \n",
    "                                                                                                    aperture=self.aperture,  device=self.device)\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "        all_feat = OrderedDict({'wmean': wmean, 'wvar':wvar, 'wskew':wskew, 'wkurt':wkurt, 'complex_feature_means':complex_feature_means, 'simple_feature_means':simple_feature_means, \n",
    "                    'complex_feature_autocorrs': complex_feature_autocorrs, 'simple_feature_autocorrs': simple_feature_autocorrs, \n",
    "                    'complex_within_scale_crosscorrs': complex_within_scale_crosscorrs, 'simple_within_scale_crosscorrs':simple_within_scale_crosscorrs,\n",
    "                    'complex_across_scale_crosscorrs': complex_across_scale_crosscorrs, 'simple_across_scale_crosscorrs':simple_across_scale_crosscorrs})\n",
    "\n",
    "        feature_names_full = list(all_feat.keys())\n",
    "        feature_names = [fname for fname in feature_names_full if fname in self.feature_types_include]\n",
    "        assert(feature_names==self.feature_types_include) # double check here that the order is correct\n",
    "        \n",
    "        for ff, feature_name in enumerate(feature_names):   \n",
    "            assert(all_feat[feature_name] is not None)\n",
    "            if ff==0:\n",
    "                all_feat_concat = all_feat[feature_name]\n",
    "            else:               \n",
    "                all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "\n",
    "        assert(all_feat_concat.shape[1]==self.n_features_total)\n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        print('Feature types included are:')\n",
    "        print(feature_names)\n",
    "\n",
    "        if torch.any(torch.isnan(all_feat_concat)):\n",
    "            print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "\n",
    "        return all_feat_concat, [self.feature_column_labels, feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26c3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "be3a3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model for validation...\n",
      "\n",
      "Creating FWRF texture model...\n",
      "\n",
      "Getting model predictions on validation set...\n",
      "\n",
      "samples [    0:19   ] of 62, voxels [     0:0     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00463\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06144\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03896\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03219\n",
      "Final size of features concatenated is [20 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "samples [   20:39   ] of 62, voxels [     0:0     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00458\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06040\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03625\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03219\n",
      "Final size of features concatenated is [20 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "samples [   40:59   ] of 62, voxels [     0:0     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00454\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06027\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03844\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03222\n",
      "Final size of features concatenated is [20 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "samples [   60:61   ] of 62, voxels [     0:0     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00363\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02560\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01705\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01124\n",
      "Final size of features concatenated is [2 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "samples [    0:19   ] of 62, voxels [     1:1     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00454\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06015\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03861\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03221\n",
      "Final size of features concatenated is [20 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "samples [   20:39   ] of 62, voxels [     1:1     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00454\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06033\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03843\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03226\n",
      "Final size of features concatenated is [20 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "samples [   40:59   ] of 62, voxels [     1:1     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00456\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06040\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03804\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03219\n",
      "Final size of features concatenated is [20 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "samples [   60:61   ] of 62, voxels [     1:1     ] of 14913Computing pixel-level statistics...\n",
      "time elapsed = 0.00371\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02369\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01749\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01122\n",
      "Final size of features concatenated is [2 x 100]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means']\n",
      "\n",
      "---------------------------------------\n",
      "total time = 1.010746s\n",
      "sample throughput = 0.016302s/sample\n",
      "voxel throughput = 0.000068s/voxel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14913 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating correlation coefficient on validation set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14913/14913 [00:03<00:00, 4074.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Validate model on held-out test set\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "val_cc, val_r2 = validate_texture_model(best_params, val_voxel_single_trial_data, val_stim_single_trial_data, _texture_fn, sample_batch_size, debug=debug, dtype=fpX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "ffa95889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variance partition, leaving out: wmean\n",
      "Remaining features are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35]\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "\n",
      "Getting model predictions on validation set...\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00481\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06640\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.04153\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03269\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00452\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06050\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03798\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03218\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00452\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05963\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03780\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03185\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00365\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02359\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01700\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01515\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00453\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06086\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03600\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03360\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00450\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06012\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03836\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03214\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00456\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06042\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03847\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03213\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00356\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02413\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01754\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01615\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wvar', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "\n",
      "---------------------------------------\n",
      "total time = 1.020676s\n",
      "sample throughput = 0.016463s/sample\n",
      "voxel throughput = 0.000068s/voxel\n",
      "\n",
      "Evaluating correlation coefficient on validation set...\n",
      "\n",
      "\n",
      "Variance partition, leaving out: wvar\n",
      "Remaining features are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35]\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "\n",
      "Getting model predictions on validation set...\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00468\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06448\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.04347\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03574\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00465\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06386\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.04006\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03368\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00458\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06303\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03802\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03187\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00363\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02418\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01759\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01585\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00464\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05791\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03844\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03232\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00460\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05984\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03806\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03177\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00443\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06043\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03812\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03177\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00365\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02367\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01707\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01506\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wskew', 'wkurt', 'complex_feature_means']\n",
      "\n",
      "---------------------------------------\n",
      "total time = 1.033452s\n",
      "sample throughput = 0.016669s/sample\n",
      "voxel throughput = 0.000069s/voxel\n",
      "\n",
      "Evaluating correlation coefficient on validation set...\n",
      "\n",
      "\n",
      "Variance partition, leaving out: wskew\n",
      "Remaining features are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35]\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "\n",
      "Getting model predictions on validation set...\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00475\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05995\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03797\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03180\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00461\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06020\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03803\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03215\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00463\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05987\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03798\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03185\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00365\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02368\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01708\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01128\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00462\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06083\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03874\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03180\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00461\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05989\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03804\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03177\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00462\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06006\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03810\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03179\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00366\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02371\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01711\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01345\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wkurt', 'complex_feature_means']\n",
      "\n",
      "---------------------------------------\n",
      "total time = 1.005506s\n",
      "sample throughput = 0.016218s/sample\n",
      "voxel throughput = 0.000067s/voxel\n",
      "\n",
      "Evaluating correlation coefficient on validation set...\n",
      "\n",
      "\n",
      "Variance partition, leaving out: wkurt\n",
      "Remaining features are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "[ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35]\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "\n",
      "Getting model predictions on validation set...\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00478\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06011\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03809\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03175\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00462\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05995\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03798\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03186\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00463\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05980\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03796\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03184\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00373\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02368\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01713\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01332\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00461\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05991\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03802\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03180\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00461\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05796\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03805\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03179\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00460\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06026\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03811\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03182\n",
      "Final size of features concatenated is [20 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00366\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02359\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01707\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01461\n",
      "Final size of features concatenated is [2 x 35]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'complex_feature_means']\n",
      "\n",
      "---------------------------------------\n",
      "total time = 1.003080s\n",
      "sample throughput = 0.016179s/sample\n",
      "voxel throughput = 0.000067s/voxel\n",
      "\n",
      "Evaluating correlation coefficient on validation set...\n",
      "\n",
      "\n",
      "Variance partition, leaving out: complex_feature_means\n",
      "Remaining features are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "[0 1 2 3]\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "\n",
      "Getting model predictions on validation set...\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00492\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06017\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03801\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03175\n",
      "Final size of features concatenated is [20 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00462\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05994\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03799\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03173\n",
      "Final size of features concatenated is [20 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00460\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05991\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03804\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03182\n",
      "Final size of features concatenated is [20 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00379\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02194\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01545\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01565\n",
      "Final size of features concatenated is [2 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00439\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06074\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03637\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03222\n",
      "Final size of features concatenated is [20 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00455\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.05792\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03841\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03213\n",
      "Final size of features concatenated is [20 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00458\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.06060\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.03850\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.03216\n",
      "Final size of features concatenated is [20 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.00361\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.02412\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.01541\n",
      "SKIPPING HIGHER-ORDER CORRELATIONS...\n",
      "time elapsed = 0.01567\n",
      "Final size of features concatenated is [2 x 4]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt']\n",
      "\n",
      "---------------------------------------\n",
      "total time = 1.001566s\n",
      "sample throughput = 0.016154s/sample\n",
      "voxel throughput = 0.000067s/voxel\n",
      "\n",
      "Evaluating correlation coefficient on validation set...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_cc, val_r2 = validate_texture_model_partial(best_params, val_voxel_single_trial_data, val_stim_single_trial_data, _texture_fn, sample_batch_size, debug=debug, dtype=fpX)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1595480",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "fc5e42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = torch.load('/user_data/mmhender/imStat/model_fits/S01/texture_ridge_8ori_4sf/Jul-06-2021_0332_DEBUG/all_fit_params')\n",
    "out = torch.load('/user_data/mmhender/imStat/model_fits/S01/texture_ridge_8ori_4sfno_pixelno_simpleno_autocorrelationsno_crosscorrelations/Jul-06-2021_0336_DEBUG/all_fit_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "b4a73db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['feature_info'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "5552d6a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-483-4ee898666b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_cc_partial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "out['val_cc_partial'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "06db3b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['feature_table_simple', 'sf_tuning_masks_simple', 'ori_tuning_masks_simple', 'cyc_per_stim_simple', 'orients_deg_simple', 'orient_filters_simple', 'feature_table_complex', 'sf_tuning_masks_complex', 'ori_tuning_masks_complex', 'cyc_per_stim_complex', 'orients_deg_complex', 'orient_filters_complex', 'aperture', 'aperture_rf_range', 'models', 'include_autocorrs', 'feature_info', 'voxel_mask', 'brain_nii_shape', 'image_order', 'voxel_index', 'voxel_roi', 'voxel_ncsnr', 'best_params', 'lambdas', 'best_lambdas', 'best_losses', 'val_cc', 'val_r2', 'val_cc_partial', 'val_r2_partial', 'features_each_model_val', 'voxel_feature_correlations_val', 'zscore_features', 'nonlin_fn', 'padding_mode', 'n_prf_sd_out', 'autocorr_output_pix', 'debug', 'up_to_sess'])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "b588ee76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 2]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch_utils.get_value(next(_fmaps_fn_complex.parameters())).dtype\n",
    "n_features_complex, fmaps_rez = fwrf_fit.get_fmaps_sizes(_fmaps_fn_complex, np.zeros([2,1,2,2]).astype(dtype), device)  \n",
    "n_features_simple, fmaps_rez = fwrf_fit.get_fmaps_sizes(_fmaps_fn_simple, np.zeros([2,1,2,2]).astype(dtype), device)    \n",
    "n_sf = len(fmaps_rez)\n",
    "n_ori = int(n_features_complex/n_sf)\n",
    "n_phase = int(n_features_simple/n_sf/n_ori)\n",
    "[n_sf, n_ori, n_phase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "43d4b40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wmean', 'wvar', 'wskew', 'wkurt']"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_texture_fn.feature_types_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "7a8d313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_texture_model(best_params, val_voxel_single_trial_data, val_stim_single_trial_data, _texture_fn, sample_batch_size, debug=False, dtype=np.float32):\n",
    "    \n",
    "    # EVALUATE PERFORMANCE ON VALIDATION SET\n",
    "    \n",
    "    print('\\nInitializing model for validation...\\n')\n",
    "    param_batch = [p[0:1] if p is not None else None for p in best_params]\n",
    "    # To initialize this module for prediction, need to take just first batch of voxels.\n",
    "    # Will eventually pass all voxels through in batches.\n",
    "    _fwd_model = texture_model(_texture_fn, param_batch, input_shape=val_stim_single_trial_data.shape)\n",
    "\n",
    "    print('\\nGetting model predictions on validation set...\\n')\n",
    "    val_voxel_pred = get_predictions_texture_model(val_stim_single_trial_data, _fwd_model, best_params, sample_batch_size=sample_batch_size, debug=debug)\n",
    "\n",
    "    # val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "    n_voxels = np.shape(val_voxel_single_trial_data)[1]\n",
    "    val_cc  = np.zeros(shape=(n_voxels), dtype=dtype)\n",
    "    val_r2 = np.zeros(shape=(n_voxels), dtype=dtype)\n",
    "    \n",
    "    print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "    for v in tqdm(range(n_voxels)):    \n",
    "        val_cc[v] = np.corrcoef(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])[0,1]  \n",
    "        val_r2[v] = get_r2(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])\n",
    "        \n",
    "    val_cc = np.nan_to_num(val_cc)\n",
    "    val_r2 = np.nan_to_num(val_r2)    \n",
    "    \n",
    "    return val_cc, val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "2144b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_crosscorrs = True\n",
    "include_autocorrs = True\n",
    "include_pixels = False\n",
    "include_simple = False\n",
    "include_complex = False\n",
    "\n",
    "feature_types_exclude = []\n",
    "if not include_pixels:\n",
    "    feature_types_exclude.append('pixel')\n",
    "if not include_simple:\n",
    "    feature_types_exclude.append('simple_feature_means')\n",
    "if not include_complex:\n",
    "    feature_types_exclude.append('complex_feature_means')\n",
    "if not include_autocorrs:\n",
    "    feature_types_exclude.append('autocorrs')\n",
    "if not include_crosscorrs:\n",
    "    feature_types_exclude.append('crosscorrs')\n",
    "\n",
    "    \n",
    "_texture_fn = texture_feature_extractor(_fmaps_fn_complex, _fmaps_fn_simple, sample_batch_size=sample_batch_size, feature_types_exclude=feature_types_exclude, autocorr_output_pix=autocorr_output_pix, n_prf_sd_out=n_prf_sd_out, aperture=aperture, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "aaa29663",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_column_labels = np.squeeze(np.concatenate([fi*np.ones([1,feature_type_dims[fi]]) for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in feature_types_exclude], axis=1).astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "31dd27ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  6,  6, ..., 11, 11, 11])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_column_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "6485aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.load('/user_data/mmhender/imStat/model_fits/S01/texture_ridge_8ori_4sf_no_pixel_no_simple_no_complex/Jul-06-2021_0352_DEBUG/all_fit_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "187c57b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['feature_info'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "f037841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variance partition, leaving out: complex_feature_autocorrs\n",
      "Remaining features are:\n",
      "['simple_feature_autocorrs', 'complex_within_scale_crosscorrs', 'simple_within_scale_crosscorrs', 'complex_across_scale_crosscorrs', 'simple_across_scale_crosscorrs']\n",
      "[   0    1    2 ... 3309 3310 3311]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 1 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-489-024bbc01d63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcolumns_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_feature_column_labels\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mparams_to_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_to_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_to_use\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mparams_to_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpars2use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_to_use\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 1 with size 100"
     ]
    }
   ],
   "source": [
    "# val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "n_voxels = np.shape(val_voxel_single_trial_data)[1]\n",
    "n_feature_types = len(_texture_fn.feature_types_include)\n",
    "val_cc  = np.zeros(shape=(n_voxels, n_feature_types), dtype=dtype)\n",
    "val_r2 = np.zeros(shape=(n_voxels, n_feature_types), dtype=dtype)\n",
    "\n",
    "orig_feature_column_labels = _texture_fn.feature_column_labels\n",
    "orig_excluded_features = _texture_fn.feature_types_exclude\n",
    "\n",
    "for ff, feat_name in enumerate(_texture_fn.feature_types_include):\n",
    "\n",
    "    print('\\nVariance partition, leaving out: %s'%feat_name)\n",
    "    _texture_fn.update_feature_list(orig_excluded_features+[feat_name])\n",
    "    print('Remaining features are:')\n",
    "    print(_texture_fn.feature_types_include)\n",
    "\n",
    "    # Choose columns of interest here, leaving out weights for one feature at a time\n",
    "    params_to_use = copy.deepcopy(best_params)\n",
    "    columns_to_use = np.where(orig_feature_column_labels!=ff)[0]\n",
    "    print(columns_to_use)\n",
    "    params_to_use[1] = params_to_use[1][:,columns_to_use]\n",
    "    if best_params[3] is not None:\n",
    "        params_to_use[3] = pars2use[3][:,columns_to_use]\n",
    "        params_to_use[4] = pars2use[4][:,columns_to_use]\n",
    "\n",
    "    print('\\nInitializing model...\\n')\n",
    "    param_batch = [p[0:1] if p is not None else None for p in params_to_use]\n",
    "\n",
    "    # To initialize this module for prediction, need to take just first batch of voxels.\n",
    "    # Will eventually pass all voxels through in batches.\n",
    "    _fwd_model = texture_model(_texture_fn, param_batch, input_shape=val_stim_single_trial_data.shape)\n",
    "\n",
    "    print('\\nGetting model predictions on validation set...\\n')\n",
    "    val_voxel_pred = get_predictions_texture_model(val_stim_single_trial_data, _fwd_model, params_to_use, sample_batch_size=sample_batch_size, debug=debug)\n",
    "\n",
    "    print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "    for v in range(n_voxels):    \n",
    "        val_cc[v,ff] = np.corrcoef(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])[0,1]  \n",
    "        val_r2[v,ff] = get_r2(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])\n",
    "\n",
    "val_cc = np.nan_to_num(val_cc)\n",
    "val_r2 = np.nan_to_num(val_r2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "9fba99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def validate_texture_model_partial(best_params, val_voxel_single_trial_data, val_stim_single_trial_data, _texture_fn, sample_batch_size, debug=False, dtype=np.float32):    \n",
    "    \n",
    "    \"\"\" \n",
    "    Evaluate trained model, leaving out a subset of features at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "    n_voxels = np.shape(val_voxel_single_trial_data)[1]\n",
    "    n_feature_types = len(_texture_fn.feature_types_include)\n",
    "    val_cc  = np.zeros(shape=(n_voxels, n_feature_types), dtype=dtype)\n",
    "    val_r2 = np.zeros(shape=(n_voxels, n_feature_types), dtype=dtype)\n",
    "\n",
    "    orig_feature_column_labels = _texture_fn.feature_column_labels\n",
    "    orig_excluded_features = _texture_fn.feature_types_exclude\n",
    "\n",
    "    for ff, feat_name in enumerate(_texture_fn.feature_types_include):\n",
    "\n",
    "        print('\\nVariance partition, leaving out: %s'%feat_name)\n",
    "        _texture_fn.update_feature_list(orig_excluded_features+[feat_name])\n",
    "        print('Remaining features are:')\n",
    "        print(_texture_fn.feature_types_include)\n",
    "\n",
    "        # Choose columns of interest here, leaving out weights for one feature at a time\n",
    "        params_to_use = copy.deepcopy(best_params)\n",
    "        columns_to_use = np.where(orig_feature_column_labels!=ff)[0]\n",
    "        print(columns_to_use)\n",
    "        params_to_use[1] = params_to_use[1][:,columns_to_use]\n",
    "        if best_params[3] is not None:\n",
    "            params_to_use[3] = pars2use[3][:,columns_to_use]\n",
    "            params_to_use[4] = pars2use[4][:,columns_to_use]\n",
    "\n",
    "        print('\\nInitializing model...\\n')\n",
    "        param_batch = [p[0:1] if p is not None else None for p in params_to_use]\n",
    "\n",
    "        # To initialize this module for prediction, need to take just first batch of voxels.\n",
    "        # Will eventually pass all voxels through in batches.\n",
    "        _fwd_model = texture_model(_texture_fn, param_batch, input_shape=val_stim_single_trial_data.shape)\n",
    "\n",
    "        print('\\nGetting model predictions on validation set...\\n')\n",
    "        val_voxel_pred = get_predictions_texture_model(val_stim_single_trial_data, _fwd_model, params_to_use, sample_batch_size=sample_batch_size, debug=debug)\n",
    "\n",
    "        print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "        for v in range(n_voxels):    \n",
    "            val_cc[v,ff] = np.corrcoef(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])[0,1]  \n",
    "            val_r2[v,ff] = get_r2(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])\n",
    "\n",
    "    val_cc = np.nan_to_num(val_cc)\n",
    "    val_r2 = np.nan_to_num(val_r2) \n",
    "    \n",
    "    return val_cc, val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b14156e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class texture_model(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module that predicts voxel responses based on texture features and encoding model weights.\n",
    "    Texture features are computed in the module specified by '_texture_fn'.\n",
    "    Currently written to work with just 1 voxel at a time. This is because the texture features are pRF-specific, \n",
    "    and have to be computed 1 pRF at a time. Could probably batch >1 voxel if they had same pRF params, though.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, _texture_fn, params, input_shape = (1,3,227,227)):\n",
    "        \n",
    "        super(texture_model, self).__init__()        \n",
    "#         print('Creating FWRF texture model...')\n",
    "        \n",
    "        self.texture_fn = _texture_fn       \n",
    "        self.voxel_batch_size = 1 # because of how this model is set up, can only do for one voxel at a time! slow.\n",
    "        device = next(_fmaps_fn_complex.parameters()).device\n",
    "      \n",
    "        models, weights, bias, features_mt, features_st, best_model_inds = params\n",
    "        _x = torch.empty((1,)+input_shape[1:], device=device).uniform_(0, 1)\n",
    "        _fmaps = _fmaps_fn_complex(_x)\n",
    "        n_features_complex, self.fmaps_rez = fwrf_fit.get_fmaps_sizes(_fmaps_fn_complex, _x, device)    \n",
    "        \n",
    "        self.models = nn.Parameter(torch.from_numpy(models).to(device), requires_grad=False)\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.from_numpy(weights).to(device), requires_grad=False)\n",
    "        self.bias = None\n",
    "        if bias is not None:\n",
    "            self.bias = nn.Parameter(torch.from_numpy(bias).to(device), requires_grad=False)\n",
    "      \n",
    "        self.features_m = None\n",
    "        self.features_s = None\n",
    "        if features_mt is not None:\n",
    "            self.features_m = nn.Parameter(torch.from_numpy(features_mt.T).to(device), requires_grad=False)\n",
    "        if features_st is not None:\n",
    "            self.features_s = nn.Parameter(torch.from_numpy(features_st.T).to(device), requires_grad=False)\n",
    "       \n",
    "    def load_voxel_block(self, *params):\n",
    "        # This takes a given set of parameters for the voxel batch of interest, and puts them \n",
    "        # into the right fields of the module so we can use them in a forward pass.\n",
    "        models = params[0]\n",
    "        assert(models.shape[0]==self.voxel_batch_size)\n",
    "        \n",
    "        torch_utils.set_value(self.models, models)\n",
    "\n",
    "        for _p,p in zip([self.weights, self.bias], params[1:3]):\n",
    "            if _p is not None:\n",
    "                if len(p)<_p.size()[0]:\n",
    "                    pp = np.zeros(shape=_p.size(), dtype=p.dtype)\n",
    "                    pp[:len(p)] = p\n",
    "                    torch_utils.set_value(_p, pp)\n",
    "                else:\n",
    "                    torch_utils.set_value(_p, p)\n",
    "                    \n",
    "        for _p,p in zip([self.features_m, self.features_s], params[3:]):\n",
    "            if _p is not None:\n",
    "                if len(p)<_p.size()[1]:\n",
    "                    pp = np.zeros(shape=(_p.size()[1], _p.size()[0]), dtype=p.dtype)\n",
    "                    pp[:len(p)] = p\n",
    "                    torch_utils.set_value(_p, pp.T)\n",
    "                else:\n",
    "                    torch_utils.set_value(_p, p.T)\n",
    "                    \n",
    "        \n",
    "    def forward(self, image_batch):\n",
    "        \n",
    "        all_feat_concat, feature_info = self.texture_fn(image_batch,self.models)\n",
    "        \n",
    "        _features = all_feat_concat.view([all_feat_concat.shape[0],-1,1]) # trials x features x 1\n",
    "#         _features = torch_utils._to_torch(all_feat_concat, device=self.weights.device).view([all_feat_concat.shape[0],-1,1]) # trials x features x 1\n",
    "       \n",
    "        if self.features_m is not None:    \n",
    "            # features_m is [nfeatures x nvoxels]\n",
    "            _features = _features - torch.tile(torch.unsqueeze(self.features_m, dim=0), [_features.shape[0], 1, 1])\n",
    "\n",
    "        if self.features_s is not None:\n",
    "            _features = _features/torch.tile(torch.unsqueeze(self.features_s, dim=0), [_features.shape[0], 1, 1])\n",
    "            _features[torch.isnan(_features)] = 0.0 # this applies in the pca case when last few columns of features are missing\n",
    "\n",
    "        # features is [#samples, #features, #voxels] - swap dims to [#voxels, #samples, features]\n",
    "        _features = torch.transpose(torch.transpose(_features, 0, 2), 1, 2)\n",
    "        # weights is [#voxels, #features]\n",
    "        # _r will be [#voxels, #samples, 1] - then [#samples, #voxels]\n",
    "        _r = torch.squeeze(torch.bmm(_features, torch.unsqueeze(self.weights, 2)), dim=2).t() \n",
    "  \n",
    "        if self.bias is not None:\n",
    "            _r = _r + torch.tile(torch.unsqueeze(self.bias, 0), [_r.shape[0],1])\n",
    "            \n",
    "        return _r\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "fbd20996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import numpy_utility, torch_utils\n",
    "from model_src import fwrf_fit, texture_statistics\n",
    "\n",
    "    \n",
    "def get_r2(actual,predicted):\n",
    "  \n",
    "    # calculate r2 for this fit.\n",
    "    ssres = np.sum(np.power((predicted - actual),2));\n",
    "#     print(ssres)\n",
    "    sstot = np.sum(np.power((actual - np.mean(actual)),2));\n",
    "#     print(sstot)\n",
    "    r2 = 1-(ssres/sstot)\n",
    "    \n",
    "    return r2\n",
    "    \n",
    "def get_predictions_texture_model(images, _fwd_model, params, sample_batch_size=100, debug=False):\n",
    "   \n",
    "    dtype = images.dtype.type\n",
    "    device = _fwd_model.weights.device\n",
    "    _params = [_p for _p in _fwd_model.parameters()]\n",
    "    voxel_batch_size = _fwd_model.voxel_batch_size\n",
    "    assert(voxel_batch_size==1) # this won't work with batches of >1 voxel\n",
    "    n_trials, n_voxels = len(images), len(params[0])\n",
    "\n",
    "    pred = np.full(fill_value=0, shape=(n_trials, n_voxels), dtype=dtype)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        vv=-1\n",
    "        ## Looping over voxels here in batches, will eventually go through all.\n",
    "        for rv, lv in numpy_utility.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "            vv=vv+1\n",
    "            if debug and vv>1:\n",
    "                break\n",
    "            # for this voxel batch, put the right parameters into the _fwrf_fn module\n",
    "            # so that we can do forward pass...\n",
    "            _fwd_model.load_voxel_block(*[p[rv] if p is not None else None for p in params])\n",
    "            pred_block = np.full(fill_value=0, shape=(n_trials, voxel_batch_size), dtype=dtype)\n",
    "            \n",
    "            # Now looping over validation set trials in batches\n",
    "            for rt, lt in numpy_utility.iterate_range(0, n_trials, sample_batch_size):\n",
    "#                 sys.stdout.write('\\rsamples [%5d:%-5d] of %d, voxels [%6d:%-6d] of %d' % (rt[0], rt[-1], n_trials, rv[0], rv[-1], n_voxels))\n",
    "                # Get predictions for this set of trials.\n",
    "               \n",
    "                pred_block[rt] = torch_utils.get_value(_fwd_model(torch_utils._to_torch(images[rt], device))) \n",
    "                \n",
    "            pred[:,rv] = pred_block[:,:lv]\n",
    "            \n",
    "    total_time = time.time() - start_time\n",
    "    print ('\\n---------------------------------------')\n",
    "    print ('total time = %fs' % total_time)\n",
    "    print ('sample throughput = %fs/sample' % (total_time / n_trials))\n",
    "    print ('voxel throughput = %fs/voxel' % (total_time / n_voxels))\n",
    "    sys.stdout.flush()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "76c20db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_higher_order_features(_fmaps_fn_complex, _fmaps_fn_simple, images, prf_params, sample_batch_size=20, include_autocorrs=True, include_crosscorrs=True, autocorr_output_pix=7, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute all higher-order features (cross-spatial and cross-feature correlations) for a batch of images.\n",
    "    Input the functions that define first level feature maps (simple and complex cells), and prf parameters.\n",
    "    Returns arrays of each higher order feature.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')    \n",
    "        \n",
    "    n_trials = np.shape(images)[0]\n",
    "    \n",
    "    assert(np.mod(autocorr_output_pix,2)==1) # must be odd!\n",
    "\n",
    "    n_features_simple, fmaps_rez = fwrf_fit.get_fmaps_sizes(_fmaps_fn_simple, images[0:sample_batch_size], device)\n",
    "    n_features_complex, fmaps_rez = fwrf_fit.get_fmaps_sizes(_fmaps_fn_complex, images[0:sample_batch_size], device)\n",
    "    \n",
    "    n_sf = len(fmaps_rez)\n",
    "    n_ori = int(n_features_complex/n_sf)\n",
    "    n_phases = 2\n",
    "    \n",
    "    # all pairs of different orientation channels.\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "\n",
    "    if include_autocorrs:\n",
    "        complex_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, autocorr_output_pix**2], device=device)\n",
    "        simple_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, n_phases, autocorr_output_pix**2], device=device)\n",
    "    else:\n",
    "        complex_feature_autocorrs = None\n",
    "        simple_feature_autocorrs = None\n",
    "    \n",
    "    if include_crosscorrs:\n",
    "        complex_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device)\n",
    "        simple_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_phases, n_ori_pairs], device=device)\n",
    "        complex_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_ori, n_ori], device=device)\n",
    "        simple_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_phases, n_ori, n_ori], device=device) # only done for pairs of neighboring SF.\n",
    "    else:\n",
    "        complex_within_scale_crosscorrs = None\n",
    "        simple_within_scale_crosscorrs = None\n",
    "        complex_across_scale_crosscorrs = None\n",
    "        simple_across_scale_crosscorrs = None\n",
    "        \n",
    "    if include_autocorrs or include_crosscorrs:\n",
    "        \n",
    "        x,y,sigma = prf_params\n",
    "\n",
    "        bb=-1\n",
    "        for batch_inds, batch_size_actual in numpy_utility.iterate_range(0, n_trials, sample_batch_size):\n",
    "            bb=bb+1\n",
    "\n",
    "            fmaps_complex = _fmaps_fn_complex(torch_utils._to_torch(images[batch_inds],device=device))   \n",
    "            fmaps_simple =  _fmaps_fn_simple(torch_utils._to_torch(images[batch_inds],device=device))\n",
    "\n",
    "            # First looping over frequency (scales)\n",
    "            for ff in range(n_sf):\n",
    "\n",
    "                # Scale specific things - get the prf at this resolution of interest\n",
    "                n_pix = fmaps_rez[ff]\n",
    "                g = numpy_utility.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "                spatial_weights = g[2][0]\n",
    "\n",
    "                patch_bbox_rect = get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=False)\n",
    "                # for autocorrelation, forcing the input region to be square\n",
    "                patch_bbox_square = get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True)\n",
    "\n",
    "                # Loop over orientation channels\n",
    "                xx=-1\n",
    "                for oo1 in range(n_ori):       \n",
    "\n",
    "\n",
    "                    # Simple cell responses - loop over two phases per orient.\n",
    "                    for pp in range(n_phases):\n",
    "                        filter_ind = n_phases*oo1+pp  # orients and phases are both listed in the same dimension of filters matrix               \n",
    "                        simple1 = fmaps_simple[ff][:,filter_ind,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                        # Simple cell autocorrelations.\n",
    "                        if include_autocorrs:\n",
    "                            auto_corr = weighted_auto_corr_2d(simple1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix, subtract_patch_mean = True, enforce_size=True, device=device)\n",
    "                            simple_feature_autocorrs[batch_inds,ff,oo1,pp,:] = torch.reshape(auto_corr, [batch_size_actual, autocorr_output_pix**2])\n",
    "\n",
    "                    # Complex cell responses\n",
    "                    complex1 = fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                    # Complex cell autocorrelation (correlation w spatially shifted versions of itself)\n",
    "                    if include_autocorrs:\n",
    "                        auto_corr = weighted_auto_corr_2d(complex1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix, subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "                        complex_feature_autocorrs[batch_inds,ff,oo1,:] = torch.reshape(auto_corr, [batch_size_actual, autocorr_output_pix**2])\n",
    "\n",
    "                    if include_crosscorrs:\n",
    "                        # Within-scale correlations - compare resp at orient==oo1 to responses at all other orientations, same scale.\n",
    "                        for oo2 in np.arange(oo1+1, n_ori):            \n",
    "                            xx = xx+1 \n",
    "                            assert(oo1==ori_pairs[xx,0] and oo2==ori_pairs[xx,1])\n",
    "\n",
    "                            complex2 = fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])      \n",
    "\n",
    "                            # Complex cell within-scale cross correlations\n",
    "                            cross_corr = weighted_cross_corr_2d(complex1, complex2, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "\n",
    "                            complex_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                            # Simple cell within-scale cross correlations\n",
    "                            for pp in range(n_phases):\n",
    "                                filter_ind = n_phases*oo2+pp\n",
    "                                simple2 = fmaps_simple[ff][:,filter_ind,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                                cross_corr = weighted_cross_corr_2d(simple1, simple2, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                simple_within_scale_crosscorrs[batch_inds,ff,pp,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                        # Cross-scale correlations - for these we care about same ori to same ori, so looping over all ori.\n",
    "                        # Only for neighboring scales, so the first level doesn't get one\n",
    "                        if ff>0:\n",
    "\n",
    "                            for oo2 in range(n_ori):\n",
    "\n",
    "                                # Complex cell response for neighboring scale\n",
    "                                complex2_neighborscale = fmaps_complex[ff-1][:,oo2,:,:].view([batch_size_actual,1,fmaps_rez[ff-1], -1])\n",
    "                                # Resize so that it can be compared w current scale\n",
    "                                complex2_neighborscale = torch.nn.functional.interpolate(complex2_neighborscale, [n_pix, n_pix], mode='bilinear', align_corners=True)\n",
    "\n",
    "                                cross_corr = weighted_cross_corr_2d(complex1, complex2_neighborscale, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                complex_across_scale_crosscorrs[batch_inds,ff-1, oo1, oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                                for pp in range(n_phases):\n",
    "                                    filter_ind = n_phases*oo2+pp\n",
    "                                    # Simple cell response for neighboring scale\n",
    "                                    simple2_neighborscale = fmaps_simple[ff-1][:,filter_ind,:,:].view([batch_size_actual,1,fmaps_rez[ff-1], -1])\n",
    "                                    simple2_neighborscale = torch.nn.functional.interpolate(simple2_neighborscale, [n_pix, n_pix], mode='bilinear', align_corners=True)\n",
    "\n",
    "                                    cross_corr = weighted_cross_corr_2d(simple1, simple2_neighborscale, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                    simple_across_scale_crosscorrs[batch_inds,ff-1, pp, oo1, oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "    if include_crosscorrs:\n",
    "        simple_within_scale_crosscorrs = torch.reshape(simple_within_scale_crosscorrs, [n_trials, -1])\n",
    "        simple_across_scale_crosscorrs = torch.reshape(simple_across_scale_crosscorrs, [n_trials, -1])\n",
    "        complex_within_scale_crosscorrs = torch.reshape(complex_within_scale_crosscorrs, [n_trials, -1])\n",
    "        complex_across_scale_crosscorrs = torch.reshape(complex_across_scale_crosscorrs, [n_trials, -1])\n",
    "    if include_autocorrs:\n",
    "        simple_feature_autocorrs = torch.reshape(simple_feature_autocorrs, [n_trials, -1])\n",
    "        complex_feature_autocorrs = torch.reshape(complex_feature_autocorrs, [n_trials, -1])\n",
    "\n",
    "    return complex_feature_autocorrs, simple_feature_autocorrs, complex_within_scale_crosscorrs, simple_within_scale_crosscorrs, complex_across_scale_crosscorrs, simple_across_scale_crosscorrs\n",
    "\n",
    "\n",
    "def get_bbox_from_prf(prf_params, image_size, n_prf_sd_out=2, verbose=False, force_square=False):\n",
    "    \"\"\"\n",
    "    For a given pRF center and size, calculate the square bounding box that captures a specified number of SDs from the center (default=2 SD)\n",
    "    Returns [xmin, xmax, ymin, ymax]\n",
    "    \"\"\"\n",
    "    x,y,sigma = prf_params\n",
    "    n_pix = image_size[0]\n",
    "    assert(image_size[1]==n_pix)\n",
    "    assert(sigma>0 and n_prf_sd_out>0)\n",
    "    \n",
    "    # decide on the window to use for correlations, based on prf parameters. Patch goes # SD from the center (2 by default).\n",
    "    # note this can't be < 1, even for the smallest choice of parameters (since rounding up). this way it won't be too small.\n",
    "    pix_from_center = int(np.ceil(sigma*n_prf_sd_out*n_pix))\n",
    "\n",
    "    # center goes [row ind, col ind]\n",
    "    center = np.array((n_pix/2  - y*n_pix, x*n_pix + n_pix/2)) # note that the x/y dims get swapped here because of how pRF parameters are defined.\n",
    "\n",
    "    # now defining the extent of the bbox. want to err on the side of making it too big, so taking floor/ceiling...\n",
    "    xmin = int(np.floor(center[0]-pix_from_center))\n",
    "    xmax = int(np.ceil(center[0]+pix_from_center))\n",
    "    ymin = int(np.floor(center[1]-pix_from_center))\n",
    "    ymax = int(np.ceil(center[1]+pix_from_center))\n",
    "\n",
    "    # cropping it to within the image bounds. Can end up being a rectangle rather than square.\n",
    "    [xmin, xmax, ymin, ymax] = np.maximum(np.minimum([xmin, xmax, ymin, ymax], n_pix), 0)\n",
    "\n",
    "    # decide if we want square or are ok with a rectangle\n",
    "    if force_square:\n",
    "        minside = np.min([xmax-xmin, ymax-ymin])\n",
    "        maxside = np.max([xmax-xmin, ymax-ymin])\n",
    "        if minside!=maxside:\n",
    "\n",
    "            if verbose:\n",
    "                print('trimming bbox to make it square')\n",
    "                print('original bbox was:')\n",
    "                print([xmin, xmax, ymin, ymax])\n",
    "\n",
    "            n2trim = [int(np.floor((maxside-minside)/2)), int(np.ceil((maxside-minside)/2))]\n",
    "\n",
    "            if np.argmin([xmax-xmin, ymax-ymin])==0:\n",
    "                ymin = ymin+n2trim[0]\n",
    "                ymax = ymax-n2trim[1]\n",
    "            else:\n",
    "                xmin = xmin+n2trim[0]\n",
    "                xmax = xmax-n2trim[1]\n",
    "\n",
    "        assert((xmax-xmin)==(ymax-ymin))\n",
    "\n",
    "    if verbose:\n",
    "        print('final bbox will be:')\n",
    "        print([xmin, xmax, ymin, ymax])\n",
    "        \n",
    "    # checking to see if the patch has become just one pixel. this can happen due to the cropping.\n",
    "    # if this happens, cross-correlations will give zero.\n",
    "    if ((xmax-xmin)<2 or (ymax-ymin)<2):\n",
    "        print('Warning: your patch only has one pixel (for n_pix: %d and prf params: [%.2f, %.2f, %.2f])\\n'%(n_pix,x,y,sigma))      \n",
    "        \n",
    "    return [xmin, xmax, ymin, ymax]\n",
    "\n",
    "\n",
    "def weighted_auto_corr_2d(images, spatial_weights, patch_bbox=None, output_pix=None, subtract_patch_mean=False, enforce_size=False, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute autocorrelation of a batch of images, weighting the pixels based on the values in spatial_weights (could be for instance a pRF definition for a voxel).\n",
    "    Can optionally specify a square patch of the image to compute over, based on \"patch_bbox\" params. Otherwise use whole image.\n",
    "    Using fft method to compute, should be fast.\n",
    "    Input parameters:\n",
    "        patch_bbox: (optional) bounding box of the patch to use for this calculation. [xmin xmax ymin ymax], see get_bbox_from_prf\n",
    "        output_pix: the size of the autocorrelation matrix output by this function. If this is an even number, the output size is this value +1. Achieved by cropping out the center of the final autocorrelation \n",
    "            matrix  (note that the full image patch is still used in computing the autocorrelation, but just the center values are returned).\n",
    "            If None, then returns the full autocorrelation matrix (same size as image patch.)\n",
    "        subtract_patch_mean: subtract weighted mean of image before computing autocorr?\n",
    "        enforce_size: if image patch is smaller than desired output, should we pad w zeros so that it has to be same size?\n",
    "    Returns:\n",
    "        A matrix describing the correlation of the image and various spatially shifted versions of it.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')        \n",
    "    if isinstance(images, np.ndarray):\n",
    "        images = torch_utils._to_torch(images, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)\n",
    "            \n",
    "    if len(np.shape(images))==2:\n",
    "        # pretend the batch and channel dims exist, for 2D input only (3D won't work)\n",
    "        single_image=True\n",
    "        images = images.view([1,1,images.shape[0],-1])\n",
    "    else:\n",
    "        single_image=False\n",
    "        \n",
    "    # have to be same size\n",
    "    assert(images.shape[2]==spatial_weights.shape[0] and images.shape[3]==spatial_weights.shape[1])\n",
    "    # images is [batch_size x n_channels x nPix x nPix]\n",
    "    batch_size = images.shape[0]\n",
    "    n_channels = images.shape[1]    \n",
    "   \n",
    "    if patch_bbox is not None:    \n",
    "        [xmin, xmax, ymin, ymax] = patch_bbox\n",
    "        # first crop out the region of the image that's currently of interest\n",
    "        images = images[:,:,xmin:xmax, ymin:ymax]\n",
    "        # crop same region from spatial weights matrix\n",
    "        spatial_weights = spatial_weights[xmin:xmax, ymin:ymax]\n",
    "\n",
    "    # make sure these sum to 1\n",
    "    if not torch.sum(spatial_weights)==0.0:\n",
    "        spatial_weights = spatial_weights/torch.sum(spatial_weights)   \n",
    "   \n",
    "    spatial_weights = spatial_weights.view([1,1,spatial_weights.shape[0],-1]).expand([batch_size,n_channels,-1,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "    \n",
    "    # compute autocorrelation of this image patch\n",
    "    if subtract_patch_mean:\n",
    "\n",
    "        wmean = torch.sum(torch.sum(images * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean = wmean.view([batch_size,-1,1,1]).expand([-1,-1,images.shape[2],images.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        \n",
    "        weighted_images = (images - wmean) * torch.sqrt(spatial_weights) # square root of the weights here because they will get squared again in next operation\n",
    "        \n",
    "        auto_corr = torch.fft.fftshift(torch.real(torch.fft.ifft2(torch.abs(torch.fft.fft2(weighted_images, dim=[2,3]))**2, dim=[2,3])), dim=[2,3]);\n",
    "    else:\n",
    "        weighted_images = images * torch.sqrt(spatial_weights)\n",
    "        auto_corr = torch.fft.fftshift(torch.real(torch.fft.ifft2(torch.abs(torch.fft.fft2(weighted_images, dim=[2,3]))**2, dim=[2,3])), dim=[2,3]);\n",
    "\n",
    "    if output_pix is not None:\n",
    "\n",
    "        # crop out just the center region\n",
    "        new_center = int(np.floor(auto_corr.shape[2]/2))\n",
    "        n_pix_out = np.min([int(np.floor(output_pix/2)), np.min([new_center, auto_corr.shape[2]-new_center])])\n",
    "        auto_corr = auto_corr[:,:,new_center-n_pix_out:new_center+n_pix_out+1, new_center-n_pix_out:new_center+n_pix_out+1]        \n",
    "    \n",
    "    if enforce_size and not (np.shape(auto_corr)[2]==output_pix or np.shape(auto_corr)[2]==output_pix+1):\n",
    "        \n",
    "        # just pad w zeros if want same size.\n",
    "        pix_diff = output_pix - np.shape(auto_corr)[2]   \n",
    "        auto_corr = torch.nn.functional.pad(auto_corr, [int(np.floor(pix_diff/2)), int(np.ceil(pix_diff/2)), int(np.floor(pix_diff/2)), int(np.ceil(pix_diff/2))], mode='constant', value=0)\n",
    "        assert(np.shape(auto_corr)[2]==output_pix and np.shape(auto_corr)[3]==output_pix)\n",
    "\n",
    "    if single_image:\n",
    "        auto_corr = torch.squeeze(auto_corr)\n",
    "        \n",
    "    return auto_corr\n",
    "\n",
    "def weighted_cross_corr_2d(images1, images2, spatial_weights, patch_bbox=None, subtract_patch_mean=True, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute cross-correlation of two identically-sized images, weighting the pixels based on the values in spatial_weights (could be for instance a pRF definition for a voxel).\n",
    "    Can optionally specify a square patch of the image to compute over, based on \"patch_bbox\" params. Otherwise use whole image.\n",
    "    Basically a dot product of image values.\n",
    "    Input parameters:\n",
    "        patch_bbox: (optional) bounding box of the patch to use for this calculation. [xmin xmax ymin ymax], see get_bbox_from_prf\n",
    "        subtract_patch_mean: do you want to subtract the weighted mean of image patch before computing?\n",
    "    Returns:\n",
    "        A single value that captures correlation between images (zero spatial shift)\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')  \n",
    "    if isinstance(images1, np.ndarray):\n",
    "        images1 = torch_utils._to_torch(images1, device)\n",
    "    if isinstance(images2, np.ndarray):\n",
    "        images2 = torch_utils._to_torch(images2, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)      \n",
    "    \n",
    "    if len(np.shape(images1))==2:\n",
    "        # pretend the batch and channel dims exist, for 2D input only (3D won't work)\n",
    "        single_image=True\n",
    "        images1 = images1.view([1,1,images1.shape[0],-1])\n",
    "        images2 = images2.view([1,1,images2.shape[0],-1])\n",
    "    else:\n",
    "        single_image=False\n",
    "        \n",
    "    # have to be same size\n",
    "    assert(images1.shape==images2.shape)\n",
    "    assert(images1.shape[2]==spatial_weights.shape[0] and images1.shape[3]==spatial_weights.shape[1])\n",
    "    assert(images2.shape[2]==spatial_weights.shape[0] and images2.shape[3]==spatial_weights.shape[1])\n",
    "    # images is [batch_size x n_channels x nPix x nPix]\n",
    "    batch_size = images1.shape[0]\n",
    "    n_channels = images1.shape[1]\n",
    "    \n",
    "\n",
    "    if patch_bbox is not None:\n",
    "        [xmin, xmax, ymin, ymax] = patch_bbox\n",
    "        # first crop out the region of the image that's currently of interest\n",
    "        images1 = images1[:,:,xmin:xmax, ymin:ymax]\n",
    "        images2 = images2[:,:,xmin:xmax, ymin:ymax]\n",
    "        # crop same region from spatial weights matrix\n",
    "        spatial_weights = spatial_weights[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    # make sure the wts sum to 1\n",
    "    if not torch.sum(spatial_weights)==0.0:\n",
    "        spatial_weights = spatial_weights/torch.sum(spatial_weights)\n",
    "    spatial_weights = spatial_weights.view([1,1,spatial_weights.shape[0],-1]).expand([batch_size,n_channels,-1,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "    \n",
    "    # compute cross-correlation\n",
    "    if subtract_patch_mean:\n",
    "        # subtract mean of each weighted image patch and take their dot product.\n",
    "        # this quantity is equal to weighted covariance (only true if mean-centered)\n",
    "        wmean1 = torch.sum(torch.sum(images1 * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean1 = wmean1.view([batch_size,-1,1,1]).expand([-1,-1,images1.shape[2],images1.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        wmean2 = torch.sum(torch.sum(images2 * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean2 = wmean2.view([batch_size,-1,1,1]).expand([-1,-1,images2.shape[2],images2.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        weighted_images1 = (images1 - wmean1) * torch.sqrt(spatial_weights) # square root of the weights here because they will get squared again in dot product operation.\n",
    "        weighted_images2 = (images2 - wmean2) * torch.sqrt(spatial_weights)\n",
    "\n",
    "        cross_corr = torch.sum(torch.sum(weighted_images1 * weighted_images2, dim=3), dim=2)    \n",
    "\n",
    "    else:\n",
    "        # dot product of raw (weighted) values\n",
    "        # this is closer to what scipy.signal.correlate2d will do (except this is weighted)\n",
    "        weighted_images1 = images1 * torch.sqrt(spatial_weights)\n",
    "        weighted_images2 = images2 * torch.sqrt(spatial_weights)\n",
    "        cross_corr = torch.sum(torch.sum(weighted_images1 * weighted_images2, dim=3), dim=2)      \n",
    "        \n",
    "    if single_image:\n",
    "        cross_corr = torch.squeeze(cross_corr)\n",
    "        \n",
    "    return cross_corr\n",
    "\n",
    "\n",
    "\n",
    "def get_weighted_pixel_features(image_batch, spatial_weights, device=None):\n",
    "    \"\"\"\n",
    "    Compute mean, variance, skewness, kurtosis of luminance values for each of a batch of images.\n",
    "    Input size is [batch_size x n_channels x npix x npix]\n",
    "    Spatial weights describes a weighting function, [npix x npix]\n",
    "    Returns [batch_size x n_channels] size array for each property.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(image_batch, np.ndarray):\n",
    "        image_batch = torch_utils._to_torch(image_batch, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)\n",
    "     \n",
    "    assert(image_batch.shape[2]==spatial_weights.shape[0] and image_batch.shape[3]==spatial_weights.shape[1])\n",
    "    assert(image_batch.shape[1]==1)\n",
    "    \n",
    "    batch_size = image_batch.shape[0]\n",
    "    n_channels = image_batch.shape[1]\n",
    "    n_pix = image_batch.shape[2]\n",
    "\n",
    "    image_batch = image_batch.view([batch_size, n_channels, n_pix**2])\n",
    "    spatial_weights = spatial_weights/torch.sum(spatial_weights)\n",
    "    spatial_weights = spatial_weights.view([1,1,n_pix**2]).expand([batch_size,n_channels,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "   \n",
    "    ims_weighted = image_batch * spatial_weights\n",
    "   \n",
    "    wmean = torch.sum(ims_weighted, axis=2).view([batch_size,-1,1])\n",
    "\n",
    "    wvar = torch.sum(spatial_weights * (image_batch - wmean.expand([-1,-1,n_pix**2]))**2, axis=2).view([batch_size,-1,1])\n",
    "    \n",
    "    wskew = torch.sum(spatial_weights *(image_batch - wmean.expand([-1,-1,n_pix**2]))**3 / (wvar**(3/2)), axis=2).view([batch_size,-1,1])\n",
    "    \n",
    "    wkurt = torch.sum(spatial_weights *(image_batch - wmean.expand([-1,-1,n_pix**2]))**4 / (wvar**(2)), axis=2).view([batch_size,-1,1])\n",
    "    \n",
    "    # correct for nans/inf values which happen when variance is very small (denominator)\n",
    "    wskew[torch.isnan(wskew)] = 0.0\n",
    "    wkurt[torch.isnan(wkurt)] = 0.0\n",
    "    wskew[torch.isinf(wskew)] = 0.0\n",
    "    wkurt[torch.isinf(wkurt)] = 0.0\n",
    "    \n",
    "    return torch.squeeze(wmean, dim=2), torch.squeeze(wvar, dim=2), torch.squeeze(wskew, dim=2), torch.squeeze(wkurt, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f03644ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fb67dc885e8>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_texture_fn.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e4adfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_texture_model_ridge(images, voxel_data, _texture_fn, models, lambdas, zscore=False, voxel_batch_size=100, \n",
    "                            holdout_size=100, shuffle=True, add_bias=False, debug=False):\n",
    "   \n",
    "    \"\"\"\n",
    "    Solve for encoding model weights using ridge regression.\n",
    "    Inputs:\n",
    "        images: the training images, [n_trials x 1 x height x width]\n",
    "        voxel_data: the training voxel data, [n_trials x n_voxels]\n",
    "        _texture_fn: module that maps from images to texture model features\n",
    "        models: the list of possible pRFs to test, columns are [x, y, sigma]\n",
    "        lambdas: ridge lambda parameters to test\n",
    "        zscore: want to zscore each column of feature matrix before fitting?\n",
    "        voxel_batch_size: how many voxels to use at a time for model fitting\n",
    "        holdout_size: how many training trials to hold out for computing loss/lambda selection?\n",
    "        shuffle: do we shuffle training data order before holding trials out?\n",
    "        add_bias: add a column of ones to feature matrix, for an additive bias?\n",
    "        debug: want to run a shortened version of this, to test it?\n",
    "    Outputs:\n",
    "        best_losses: loss value for each voxel (with best pRF and best lambda), eval on held out set\n",
    "        best_lambdas: best lambda for each voxel (chosen based on loss w held out set)\n",
    "        best_params: \n",
    "            [0] best pRF for each voxel [x,y,sigma]\n",
    "            [1] best weights for each voxel/feature\n",
    "            [2] if add_bias=True, best bias value for each voxel\n",
    "            [3] if zscore=True, the mean of each feature before z-score\n",
    "            [4] if zscore=True, the std of each feature before z-score\n",
    "            [5] index of the best pRF for each voxel (i.e. index of row in \"models\")\n",
    "        feature_info: describes types of features in texture model, see texture_feature_extractor.py\n",
    "        \n",
    "    \"\"\"\n",
    "   \n",
    "    dtype = images.dtype.type\n",
    "    device = next(_texture_fn.parameters()).device\n",
    "    trn_size = len(voxel_data) - holdout_size\n",
    "    assert trn_size>0, 'Training size needs to be greater than zero'\n",
    "    \n",
    "    print ('trn_size = %d (%.1f%%)' % (trn_size, float(trn_size)*100/len(voxel_data)))\n",
    "    print ('dtype = %s' % dtype)\n",
    "    print ('device = %s' % device)\n",
    "    print ('---------------------------------------')\n",
    "    \n",
    "    # First do shuffling of data and define set to hold out\n",
    "    n_trials = len(images)\n",
    "    n_prfs = len(models)\n",
    "    n_voxels = voxel_data.shape[1]\n",
    "    order = np.arange(len(voxel_data), dtype=int)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(order)\n",
    "    images = images[order]\n",
    "    voxel_data = voxel_data[order]  \n",
    "    trn_data = voxel_data[:trn_size]\n",
    "    out_data = voxel_data[trn_size:]\n",
    "\n",
    "    n_features_total = _texture_fn.n_features_total\n",
    "        \n",
    "    # Create full model value buffers    \n",
    "    best_models = np.full(shape=(n_voxels,), fill_value=-1, dtype=int)   \n",
    "    best_lambdas = np.full(shape=(n_voxels,), fill_value=-1, dtype=int)\n",
    "    best_losses = np.full(fill_value=np.inf, shape=(n_voxels), dtype=dtype)\n",
    "    best_w_params = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "\n",
    "    if add_bias:\n",
    "        best_w_params = np.concatenate([best_w_params, np.ones(shape=(len(best_w_params),1), dtype=dtype)], axis=1)\n",
    "    features_mean = None\n",
    "    features_std = None\n",
    "    if zscore:\n",
    "        features_mean = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "        features_std  = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    vox_loop_time = 0\n",
    "    print ('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Looping over models (here models are different spatial RF definitions)\n",
    "        for m,(x,y,sigma) in enumerate(models):\n",
    "            if debug and m>1:\n",
    "                break\n",
    "            print('\\nmodel %d\\n'%m)\n",
    "            t = time.time()   \n",
    "            \n",
    "            # Get features for the desired pRF, across all trn set image   \n",
    "        \n",
    "            all_feat_concat, feature_info = _texture_fn(images, [x,y,sigma])\n",
    "            \n",
    "            features = torch_utils.get_value(all_feat_concat)\n",
    "            \n",
    "            elapsed = time.time() - t\n",
    "        \n",
    "            if zscore:  \n",
    "                features_m = np.mean(features, axis=0, keepdims=True) #[:trn_size]\n",
    "                features_s = np.std(features, axis=0, keepdims=True) + 1e-6          \n",
    "                features -= features_m\n",
    "                features /= features_s    \n",
    "                \n",
    "            if add_bias:\n",
    "                features = np.concatenate([features, np.ones(shape=(len(features), 1), dtype=dtype)], axis=1)\n",
    "            \n",
    "            # separate design matrix into training/held out data (for lambda selection)\n",
    "            trn_features = features[:trn_size]\n",
    "            out_features = features[trn_size:]   \n",
    "\n",
    "            # Send matrices to gpu\n",
    "            _xtrn = torch_utils._to_torch(trn_features, device=device)\n",
    "            _xout = torch_utils._to_torch(out_features, device=device)   \n",
    "            \n",
    "            # Do part of the matrix math involved in ridge regression optimization out of the loop, \n",
    "            # because this part will be same for all the voxels.\n",
    "            _cof = _cofactor_fn_cpu(_xtrn, lambdas)\n",
    "            \n",
    "            # Now looping over batches of voxels (only reason is because can't store all in memory at same time)\n",
    "            vox_start = time.time()\n",
    "            for rv,lv in numpy_utility.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "                sys.stdout.write('\\rfitting model %4d of %-4d, voxels [%6d:%-6d] of %d' % (m, n_prfs, rv[0], rv[-1], n_voxels))\n",
    "\n",
    "                # Send matrices to gpu\n",
    "                _vtrn = torch_utils._to_torch(trn_data[:,rv], device=device)\n",
    "                _vout = torch_utils._to_torch(out_data[:,rv], device=device)\n",
    "\n",
    "                # Here is where optimization happens - relatively simple matrix math inside loss fn.\n",
    "                _betas, _loss = _loss_fn(_cof, _vtrn, _xout, _vout) #   [#lambda, #feature, #voxel, ], [#lambda, #voxel]\n",
    "                # Now have a set of weights (in betas) and a loss value for every voxel and every lambda. \n",
    "                # goal is then to choose for each voxel, what is the best lambda and what weights went with that lambda.\n",
    "                \n",
    "                # first choose best lambda value and the loss that went with it.\n",
    "                _values, _select = torch.min(_loss, dim=0)\n",
    "                betas = torch_utils.get_value(_betas)\n",
    "                values, select = torch_utils.get_value(_values), torch_utils.get_value(_select)\n",
    "\n",
    "                # comparing this loss to the other models for each voxel (e.g. the other RF position/sizes)\n",
    "                imp = values<best_losses[rv]\n",
    "                \n",
    "                if np.sum(imp)>0:                    \n",
    "                    # for whichever voxels had improvement relative to previous models, save parameters now\n",
    "                    # this means we won't have to save all params for all models, just best.\n",
    "                    arv = np.array(rv)[imp]\n",
    "                    li = select[imp]\n",
    "                    best_lambdas[arv] = li\n",
    "                    best_losses[arv] = values[imp]\n",
    "                    best_models[arv] = m\n",
    "                    if zscore:\n",
    "                        features_mean[arv] = features_m # broadcast over updated voxels\n",
    "                        features_std[arv]  = features_s\n",
    "                    # taking the weights associated with the best lambda value\n",
    "                    best_w_params[arv,:] = numpy_utility.select_along_axis(betas[:,:,imp], li, run_axis=2, choice_axis=0).T\n",
    "              \n",
    "            vox_loop_time += (time.time() - vox_start)\n",
    "            elapsed = (time.time() - vox_start)\n",
    "\n",
    "    # Print information about how fitting went...\n",
    "    total_time = time.time() - start_time\n",
    "    inv_time = total_time - vox_loop_time\n",
    "    return_params = [best_w_params[:,:n_features_total],]\n",
    "    if add_bias:\n",
    "        return_params += [best_w_params[:,-1],]\n",
    "    else: \n",
    "        return_params += [None,]\n",
    "    print ('\\n---------------------------------------')\n",
    "    print ('total time = %fs' % total_time)\n",
    "    print ('total throughput = %fs/voxel' % (total_time / n_voxels))\n",
    "    print ('voxel throughput = %fs/voxel' % (vox_loop_time / n_voxels))\n",
    "    print ('setup throughput = %fs/model' % (inv_time / n_prfs))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    best_params = [models[best_models],]+return_params+[features_mean, features_std]+[best_models]\n",
    "    \n",
    "    return best_losses, best_lambdas, best_params, feature_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import numpy_utility, torch_utils\n",
    "from model_src import texture_statistics\n",
    "\n",
    " \n",
    "\n",
    "def _cofactor_fn_cpu(_x, lambdas):\n",
    "    '''\n",
    "    Generating a matrix needed to solve ridge regression model for each lambda value.\n",
    "    Ridge regression (Tikhonov) solution is :\n",
    "    w = (X^T*X + I*lambda)^-1 * X^T * Y\n",
    "    This func will return (X^T*X + I*lambda)^-1 * X^T. \n",
    "    So once we have that, can just multiply by training data (Y) to get weights.\n",
    "    returned size is [nLambdas x nFeatures x nTrials]\n",
    "    This version makes sure that the torch inverse operation is done on the cpu, and in floating point-64 precision.\n",
    "    Otherwise get bad results for small lambda values. This seems to be a torch-specific bug.\n",
    "    \n",
    "    '''\n",
    "    device_orig = _x.device\n",
    "    type_orig = _x.dtype\n",
    "    # switch to this specific format which works with inverse\n",
    "    _x = _x.to('cpu').to(torch.float64)\n",
    "    _f = torch.stack([(torch.mm(torch.t(_x), _x) + torch.eye(_x.size()[1], device='cpu', dtype=torch.float64) * l).inverse() for l in lambdas], axis=0) \n",
    "    \n",
    "    # [#lambdas, #feature, #feature] \n",
    "    cof = torch.tensordot(_f, _x, dims=[[2],[1]]) # [#lambdas, #feature, #sample]\n",
    "    \n",
    "    # put back to whatever way it was before, so that we can continue with other operations as usual\n",
    "    return cof.to(device_orig).to(type_orig)\n",
    "\n",
    "\n",
    "\n",
    "def _loss_fn(_cofactor, _vtrn, _xout, _vout):\n",
    "    '''\n",
    "    Calculate loss given \"cofactor\" from cofactor_fn, training data, held-out design matrix, held out data.\n",
    "    returns weights (betas) based on equation\n",
    "    w = (X^T*X + I*lambda)^-1 * X^T * Y\n",
    "    also returns loss for these weights w the held out data. SSE is loss func here.\n",
    "    '''\n",
    "\n",
    "    _beta = torch.tensordot(_cofactor, _vtrn, dims=[[2], [0]]) # [#lambdas, #feature, #voxel]\n",
    "    _pred = torch.tensordot(_xout, _beta, dims=[[1],[1]]) # [#samples, #lambdas, #voxels]\n",
    "    _loss = torch.sum(torch.pow(_vout[:,None,:] - _pred, 2), dim=0) # [#lambdas, #voxels]\n",
    "    return _beta, _loss\n",
    "\n",
    "\n",
    "def get_fmaps_sizes(_fmaps_fn, image_batch, device):\n",
    "    \"\"\" \n",
    "    Passing a batch of images through feature maps, in order to compute sizes.\n",
    "    Returns number of total features across all groups of maps, and the resolution of each map group.\n",
    "    \"\"\"\n",
    "    n_features = 0\n",
    "    _x = torch.tensor(image_batch).to(device) # the input variable.\n",
    "    _fmaps = _fmaps_fn(_x)\n",
    "    resolutions_each_sf = []\n",
    "    for k,_fm in enumerate(_fmaps):\n",
    "        n_features = n_features + _fm.size()[1]\n",
    "        resolutions_each_sf.append(_fm.size()[2])\n",
    "    \n",
    "    return n_features, resolutions_each_sf\n",
    "\n",
    "\n",
    "\n",
    "def get_features_in_prf(prf_params, _fmaps_fn, images, sample_batch_size, aperture, device, to_numpy=True):\n",
    "    \"\"\"\n",
    "    For a given set of images and a specified pRF position and size, compute the\n",
    "    activation in each feature map channel. Returns [nImages x nFeatures]\n",
    "    \"\"\"\n",
    "    \n",
    "    dtype = images.dtype.type\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        x,y,sigma = prf_params\n",
    "        n_trials = images.shape[0]\n",
    "\n",
    "        # pass first batch of images through feature map, just to get sizes.\n",
    "        n_features, fmaps_rez = get_fmaps_sizes(_fmaps_fn, images[0:sample_batch_size], device)\n",
    "\n",
    "        features = np.zeros(shape=(n_trials, n_features), dtype=dtype)\n",
    "        if to_numpy==False:\n",
    "             features = torch_utils._to_torch(features, device=device)\n",
    "                \n",
    "        # Define the RF for this \"model\" version - at several resolutions.\n",
    "        _prfs = [torch_utils._to_torch(numpy_utility.make_gaussian_mass(x, y, sigma, n_pix, size=aperture, \\\n",
    "                                  dtype=dtype)[2], device=device) for n_pix in fmaps_rez]\n",
    "\n",
    "        # To make full design matrix for all trials, first looping over trials in batches to get the features\n",
    "        # Only reason to loop is memory constraints, because all trials is big matrices.\n",
    "        t = time.time()\n",
    "        n_batches = np.ceil(n_trials/sample_batch_size)\n",
    "        bb=-1\n",
    "        for rt,rl in numpy_utility.iterate_range(0, n_trials, sample_batch_size):\n",
    "\n",
    "            bb=bb+1\n",
    "#             sys.stdout.write('\\rbatch %d of %d'%(bb,n_batches))\n",
    "            # multiplying feature maps by RFs here. \n",
    "            # we have one specified RF position for this version of the model. \n",
    "            # Feature maps in _fm go [nTrials x nFeatures(orientations) x nPixels x nPixels]\n",
    "            # spatial RFs in _prfs go [nPixels x nPixels]\n",
    "            # purpose of the for looping within this statement is to loop over map resolutions \n",
    "            # (e.g. spatial frequencies in model)\n",
    "            # output _features is [nTrials x nFeatures*nResolutions], so a 2D matrix. \n",
    "            # Combining features/resolutions here finally, so we can solve for weights \n",
    "            # in that full orient x SF feature space.\n",
    "\n",
    "            # then combine this with the other \"batches\" of trials to make a full \"model space tensor\"\n",
    "            # features is [nTrialsTotal x nFeatures*nResolutions]\n",
    "\n",
    "            # note this is concatenating SFs together from low to high - \n",
    "            # cycles through all orient channels in order for first SF, then again for next SF.\n",
    "            _features = torch.cat([torch.tensordot(_fm, _prf, dims=[[2,3], [0,1]]) \\\n",
    "                                   for _fm,_prf in zip(_fmaps_fn(torch_utils._to_torch(images[rt], \\\n",
    "                                           device=device)), _prfs)], dim=1) # [#samples, #features]\n",
    "\n",
    "            # Add features for this batch to full design matrix over all trials\n",
    "            if to_numpy:\n",
    "                features[rt] = torch_utils.get_value(_features)\n",
    "            else:\n",
    "                features[rt] = _features\n",
    "                \n",
    "        elapsed = time.time() - t\n",
    "#         print('\\nComputing features took %d sec'%elapsed)\n",
    "        \n",
    "    return features\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
