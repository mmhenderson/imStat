{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4c538e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic modules\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "# import custom modules\n",
    "root_dir   = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(root_dir, 'code'))\n",
    "sys.path.append(os.path.join(root_dir, 'code', 'model_fitting'))\n",
    "from model_src import fwrf_fit, fwrf_predict\n",
    "import initialize_fitting\n",
    "\n",
    "fpX = np.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f409f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=1\n",
    "roi=None\n",
    "\n",
    "ridge=1\n",
    "\n",
    "shuffle_images=False\n",
    "random_images=False\n",
    "random_voxel_data=False\n",
    "\n",
    "zscore_features=True\n",
    "nonlin_fn=False\n",
    "padding_mode='circular'\n",
    "\n",
    "n_ori=8\n",
    "n_sf=4\n",
    "up_to_sess=1\n",
    "debug=True\n",
    "\n",
    "fitting_type='simple_complex'\n",
    "include_crosscorrs=True\n",
    "include_autocorrs=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8cf00dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce GTX TITAN X\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n",
      "Will compute autocorrs\n",
      "\n",
      "Will compute crosscorrs\n",
      "\n",
      "Time Stamp: Jul-05-2021_1425\n",
      "\n",
      "Will save final output file to /user_data/mmhender/model_fits/S01/texture_ridge_8ori_4sf/Jul-05-2021_1425_DEBUG/\n",
      "\n",
      "3794 voxels of overlap between kastner and prf definitions, using prf defs\n",
      "unique values in retino labels:\n",
      "[-1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.\n",
      " 17. 18. 19. 20. 21. 22. 23. 24. 25.]\n",
      "0 voxels of overlap between face and place definitions, using place defs\n",
      "unique values in categ labels:\n",
      "[-1.  0. 26. 27. 28. 30. 31. 32. 33.]\n",
      "1535 voxels are defined (differently) in both retinotopic areas and category areas\n",
      "\n",
      "14913 voxels are defined across all areas, and will be used for analysis\n",
      "\n",
      "Loading numerical label/name mappings for all ROIs:\n",
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4']\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4', 'VO1', 'VO2', 'PHC1', 'PHC2', 'TO2', 'TO1', 'LO2', 'LO1', 'V3B', 'V3A', 'IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5', 'SPL1', 'FEF']\n",
      "[1, 2, 3, 4, 5]\n",
      "['OFA', 'FFA-1', 'FFA-2', 'mTL-faces', 'aTL-faces']\n",
      "[1, 2, 3]\n",
      "['OPA', 'PPA', 'RSC']\n",
      "\n",
      "Sizes of all defined ROIs in this subject:\n",
      "Region V1 has 2392 voxels. Includes subregions:\n",
      "['V1v', 'V1d']\n",
      "Region V2 has 2096 voxels. Includes subregions:\n",
      "['V2v', 'V2d']\n",
      "Region V3 has 1674 voxels. Includes subregions:\n",
      "['V3v', 'V3d']\n",
      "Region hV4 has 721 voxels. Includes subregions:\n",
      "['hV4']\n",
      "Region VO1-2 has 482 voxels. Includes subregions:\n",
      "['VO1', 'VO2']\n",
      "Region PHC1-2 has 382 voxels. Includes subregions:\n",
      "['PHC1', 'PHC2']\n",
      "Region LO1-2 has 488 voxels. Includes subregions:\n",
      "['LO2', 'LO1']\n",
      "Region TO1-2 has 339 voxels. Includes subregions:\n",
      "['TO2', 'TO1']\n",
      "Region V3ab has 965 voxels. Includes subregions:\n",
      "['V3B', 'V3A']\n",
      "Region IPS0-5 has 2155 voxels. Includes subregions:\n",
      "['IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5']\n",
      "Region SPL1 has 164 voxels. Includes subregions:\n",
      "['SPL1']\n",
      "Region FEF has 72 voxels. Includes subregions:\n",
      "['FEF']\n",
      "\n",
      "\n",
      "Region OFA has 355 voxels.\n",
      "Region FFA-1 has 484 voxels.\n",
      "Region FFA-2 has 310 voxels.\n",
      "Region mTL-faces has 0 voxels.\n",
      "Region aTL-faces has 159 voxels.\n",
      "Region OPA has 1611 voxels.\n",
      "Region PPA has 1033 voxels.\n",
      "Region RSC has 566 voxels.\n",
      "\n",
      "Loading images for subject 1\n",
      "\n",
      "image data size: (10000, 3, 227, 227) , dtype: uint8 , value range: 0 255\n",
      "Loading data for sessions 1:1...\n",
      "/lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR\n",
      "/lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR\n",
      "324\n",
      "/lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/betas_session01.nii.gz\n",
      "int16 -32768 32767 (750, 81, 104, 83)\n",
      "<beta> = 1.237, <sigma> = 1.391\n",
      "\n",
      "Size of full data set [nTrials x nVoxels] is:\n",
      "(750, 14913)\n",
      "Total number of voxels = 14913\n",
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n",
      "\n",
      "Possible lambda values are:\n",
      "[1.0000000e+00 4.2169652e+00 1.7782795e+01 7.4989418e+01 3.1622775e+02\n",
      " 1.3335215e+03 5.6234131e+03 2.3713736e+04 1.0000000e+05]\n"
     ]
    }
   ],
   "source": [
    "device = initialize_fitting.init_cuda()\n",
    "nsd_root, stim_root, beta_root, mask_root = initialize_fitting.get_paths()\n",
    "\n",
    "if ridge==True:       \n",
    "    # ridge regression, testing several positive lambda values (default)\n",
    "    model_name = 'texture_ridge_%dori_%dsf'%(n_ori, n_sf)\n",
    "else:        \n",
    "    # fixing lambda at zero, so it turns into ordinary least squares\n",
    "    model_name = 'texture_OLS_%dori_%dsf'%(n_ori, n_sf)\n",
    "\n",
    "if include_autocorrs==False:\n",
    "    print('Skipping autocorrs\\n')\n",
    "    model_name = model_name+'_no_autocorrelations'\n",
    "else:\n",
    "    print('Will compute autocorrs\\n')\n",
    "\n",
    "if include_crosscorrs==False:\n",
    "    print('Skipping crosscorrs\\n')\n",
    "    model_name = model_name+'_no_crosscorrelations'\n",
    "else:\n",
    "    print('Will compute crosscorrs\\n')   \n",
    "\n",
    "output_dir, fn2save = initialize_fitting.get_save_path(root_dir, subject, model_name, shuffle_images, random_images, random_voxel_data, debug)\n",
    "\n",
    "# decide what voxels to use  \n",
    "voxel_mask, voxel_index, voxel_roi, voxel_ncsnr, brain_nii_shape = initialize_fitting.get_voxel_info(mask_root, beta_root, subject, roi)\n",
    "\n",
    "# get all data and corresponding images, in two splits. always fixed set that gets left out\n",
    "trn_stim_data, trn_voxel_data, val_stim_single_trial_data, val_voxel_single_trial_data, \\\n",
    "    n_voxels, n_trials_val, image_order = initialize_fitting.get_data_splits(nsd_root, beta_root, stim_root, subject, voxel_mask, up_to_sess, \n",
    "                                                                             shuffle_images=shuffle_images, random_images=random_images, random_voxel_data=random_voxel_data)\n",
    "\n",
    "# Set up the filters\n",
    "_gaborizer_complex, _gaborizer_simple, _fmaps_fn_complex, _fmaps_fn_simple = initialize_fitting.get_feature_map_simple_complex_fn(n_ori, n_sf, padding_mode=padding_mode, device=device, nonlin_fn=nonlin_fn)\n",
    "\n",
    "# Params for the spatial aspect of the model (possible pRFs)\n",
    "#     aperture_rf_range=0.8 # using smaller range here because not sure what to do with RFs at edges...\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "# More params for fitting\n",
    "holdout_size, lambdas = initialize_fitting.get_fitting_pars(trn_voxel_data, zscore_features, ridge=ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a5b547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_size = 619 (90.0%)\n",
      "dtype = <class 'numpy.float32'>\n",
      "device = cuda:0\n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "model 0\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.04994\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.92439\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.52470\n",
      "Computing higher order correlations...\n",
      "time elapsed = 25.51989\n",
      "Final size of features concatenated is [688 x 3412]\n",
      "Feature types included are:\n",
      "['wmean', 'wvar', 'wskew', 'wkurt', 'complex_feature_means', 'simple_feature_means', 'complex_feature_autocorrs', 'simple_feature_autocorrs', 'complex_within_scale_crosscorrs', 'simple_within_scale_crosscorrs', 'complex_across_scale_crosscorrs', 'simple_across_scale_crosscorrs']\n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "model 1\n",
      "\n",
      "Computing pixel-level statistics...\n",
      "time elapsed = 0.07051\n",
      "Computing complex cell features...\n",
      "time elapsed = 0.92580\n",
      "Computing simple cell features...\n",
      "time elapsed = 0.52654\n",
      "Computing higher order correlations...\n",
      "torch.Size([20, 1, 5, 6])\n",
      "torch.Size([20, 1, 2, 3])\n",
      "5\n",
      "3\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-55a33dbf3ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrn_stim_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_voxel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fmaps_fn_complex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fmaps_fn_simple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maperture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maperture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_autocorrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_autocorrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_crosscorrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_crosscorrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocorr_output_pix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautocorr_output_pix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_prf_sd_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_prf_sd_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzscore_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     voxel_batch_size=voxel_batch_size, holdout_size=holdout_size, shuffle=True, add_bias=True, debug=debug)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# note there's also a shuffle param in the above fn call, that determines the nested heldout data for lambda and param selection. always using true.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nDone with training\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-8face041e49c>\u001b[0m in \u001b[0;36mfit_texture_model_ridge\u001b[0;34m(images, voxel_data, _fmaps_fn_complex, _fmaps_fn_simple, models, lambdas, aperture, include_autocorrs, include_crosscorrs, autocorr_output_pix, n_prf_sd_out, zscore, sample_batch_size, voxel_batch_size, holdout_size, shuffle, add_bias, debug)\u001b[0m\n\u001b[1;32m     78\u001b[0m             all_feat_concat, feature_info = compute_all_texture_features(_fmaps_fn_complex, _fmaps_fn_simple, images, \n\u001b[1;32m     79\u001b[0m                                                                          \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_autocorrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_crosscorrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocorr_output_pix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                                                                          n_prf_sd_out, aperture, device=device)\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_feat_concat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#             print(np.any(np.isnan(features)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-0494b593d77a>\u001b[0m in \u001b[0;36mcompute_all_texture_features\u001b[0;34m(_fmaps_fn_complex, _fmaps_fn_simple, images, prf_params, sample_batch_size, include_autocorrs, include_crosscorrs, autocorr_output_pix, n_prf_sd_out, aperture, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m                                                                                                 \u001b[0msample_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_autocorrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_autocorrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_crosscorrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_crosscorrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                                                                                 \u001b[0mautocorr_output_pix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautocorr_output_pix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_prf_sd_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_prf_sd_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                                                                                                 aperture=aperture,  device=device)\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time elapsed = %.5f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-0494b593d77a>\u001b[0m in \u001b[0;36mget_higher_order_features\u001b[0;34m(_fmaps_fn_complex, _fmaps_fn_simple, images, prf_params, sample_batch_size, include_autocorrs, include_crosscorrs, autocorr_output_pix, n_prf_sd_out, aperture, device)\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;31m# Simple cell autocorrelations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0minclude_autocorrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                             \u001b[0mauto_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_auto_corr_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_bbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_bbox_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_pix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautocorr_output_pix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtract_patch_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                             \u001b[0msimple_feature_autocorrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_inds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moo1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocorr_output_pix\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-865b87cd4900>\u001b[0m in \u001b[0;36mweighted_auto_corr_2d\u001b[0;34m(images, spatial_weights, patch_bbox, output_pix, subtract_patch_mean, enforce_size, device)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_pix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpix_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_corr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0moutput_pix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_corr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0moutput_pix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msingle_image\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### DO THE ACTUAL MODEL FITTING HERE ####\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "autocorr_output_pix=5\n",
    "n_prf_sd_out=2\n",
    "debug=True\n",
    "sample_batch_size=20\n",
    "voxel_batch_size=50\n",
    "include_autocorrs = True\n",
    "\n",
    "best_losses, best_lambdas, best_params, feature_info = fit_texture_model_ridge(\n",
    "    trn_stim_data, trn_voxel_data, _fmaps_fn_complex, _fmaps_fn_simple, models, lambdas, \\\n",
    "    aperture=aperture, include_autocorrs = include_autocorrs, include_crosscorrs = include_crosscorrs, autocorr_output_pix=autocorr_output_pix, n_prf_sd_out=n_prf_sd_out, zscore=zscore_features, sample_batch_size=sample_batch_size, \\\n",
    "    voxel_batch_size=voxel_batch_size, holdout_size=holdout_size, shuffle=True, add_bias=True, debug=debug)\n",
    "# note there's also a shuffle param in the above fn call, that determines the nested heldout data for lambda and param selection. always using true.\n",
    "print('\\nDone with training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece388cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_texture_model_ridge(images, voxel_data, _fmaps_fn_complex, _fmaps_fn_simple, models, lambdas, \n",
    "                            aperture=1.0, include_autocorrs=True, include_crosscorrs=True, autocorr_output_pix=5, n_prf_sd_out=2, zscore=False, sample_batch_size=100, voxel_batch_size=100, \n",
    "                            holdout_size=100, shuffle=True, add_bias=False, debug=False):\n",
    "   \n",
    "   \n",
    "    dtype = images.dtype.type\n",
    "    device = next(_fmaps_fn_complex.parameters()).device\n",
    "    trn_size = len(voxel_data) - holdout_size\n",
    "    assert trn_size>0, 'Training size needs to be greater than zero'\n",
    "    \n",
    "    print ('trn_size = %d (%.1f%%)' % (trn_size, float(trn_size)*100/len(voxel_data)))\n",
    "    print ('dtype = %s' % dtype)\n",
    "    print ('device = %s' % device)\n",
    "    print ('---------------------------------------')\n",
    "    \n",
    "    # First do shuffling of data and define set to hold out\n",
    "    n_trials = len(images)\n",
    "    n_prfs = len(models)\n",
    "    n_voxels = voxel_data.shape[1]\n",
    "    order = np.arange(len(voxel_data), dtype=int)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(order)\n",
    "    images = images[order]\n",
    "    voxel_data = voxel_data[order]  \n",
    "    trn_data = voxel_data[:trn_size]\n",
    "    out_data = voxel_data[trn_size:]\n",
    "    \n",
    "    # Looping over the feature maps once with a batch of images, to get their sizes\n",
    "    n_features_complex, fmaps_rez = get_fmaps_sizes(_fmaps_fn_complex, images[0:sample_batch_size], device)\n",
    "    n_sf = len(fmaps_rez)\n",
    "    n_ori = int(n_features_complex/n_sf)\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "    n_phases=2\n",
    "    \n",
    "    # how many features will be needed, in total?\n",
    "    if include_autocorrs and include_crosscorrs:\n",
    "        n_features_total = 4 + n_ori*n_sf + n_ori*n_sf*n_phases \\\n",
    "                + n_ori*n_sf*autocorr_output_pix**2 + n_ori*n_sf*n_phases*autocorr_output_pix**2 \\\n",
    "                + n_sf*n_ori_pairs + n_sf*n_ori_pairs*n_phases + (n_sf-1)*n_ori**2 + (n_sf-1)*n_ori**2*n_phases\n",
    "    elif include_crosscorrs:\n",
    "        n_features_total = 4 + n_ori*n_sf + n_ori*n_sf*n_phases \\\n",
    "                + n_sf*n_ori_pairs + n_sf*n_ori_pairs*n_phases + (n_sf-1)*n_ori**2 + (n_sf-1)*n_ori**2*n_phases\n",
    "    elif include_autocorrs:\n",
    "        n_features_total = 4 + n_ori*n_sf + n_ori*n_sf*n_phases \\\n",
    "                + n_ori*n_sf*autocorr_output_pix**2 + n_ori*n_sf*n_phases*autocorr_output_pix**2\n",
    "    else:\n",
    "        n_features_total = 4 + n_ori*n_sf + n_ori*n_sf*n_phases\n",
    "        \n",
    "    # Create full model value buffers    \n",
    "    best_models = np.full(shape=(n_voxels,), fill_value=-1, dtype=int)   \n",
    "    best_lambdas = np.full(shape=(n_voxels,), fill_value=-1, dtype=int)\n",
    "    best_losses = np.full(fill_value=np.inf, shape=(n_voxels), dtype=dtype)\n",
    "    best_w_params = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "\n",
    "    if add_bias:\n",
    "        best_w_params = np.concatenate([best_w_params, np.ones(shape=(len(best_w_params),1), dtype=dtype)], axis=1)\n",
    "    features_mean = None\n",
    "    features_std = None\n",
    "    if zscore:\n",
    "        features_mean = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "        features_std  = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    vox_loop_time = 0\n",
    "    print ('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Looping over models (here models are different spatial RF definitions)\n",
    "        for m,(x,y,sigma) in enumerate(models):\n",
    "            if debug and m>1:\n",
    "                break\n",
    "            print('\\nmodel %d\\n'%m)\n",
    "            t = time.time()            \n",
    "            # Get features for the desired pRF, across all trn set image     \n",
    "            all_feat_concat, feature_info = texture_statistics.compute_all_texture_features(_fmaps_fn_complex, _fmaps_fn_simple, images, \n",
    "                                                                         [x,y,sigma], sample_batch_size, include_autocorrs, include_crosscorrs, autocorr_output_pix, \n",
    "                                                                         n_prf_sd_out, aperture, device=device)\n",
    "            features = all_feat_concat\n",
    "#             print(np.any(np.isnan(features)))\n",
    "#             features = get_features_in_prf((x,y,sigma), _fmaps_fn, images, sample_batch_size, aperture, device)     \n",
    "            elapsed = time.time() - t\n",
    "        \n",
    "            if zscore:  \n",
    "                features_m = np.mean(features, axis=0, keepdims=True) #[:trn_size]\n",
    "                features_s = np.std(features, axis=0, keepdims=True) + 1e-6          \n",
    "                features -= features_m\n",
    "                features /= features_s    \n",
    "                \n",
    "            if add_bias:\n",
    "                features = np.concatenate([features, np.ones(shape=(len(features), 1), dtype=dtype)], axis=1)\n",
    "            \n",
    "            # separate design matrix into training/held out data (for lambda selection)\n",
    "            trn_features = features[:trn_size]\n",
    "            out_features = features[trn_size:]   \n",
    "\n",
    "#             print(features)\n",
    "            \n",
    "            # Send matrices to gpu\n",
    "            _xtrn = torch_utils._to_torch(trn_features, device=device)\n",
    "            _xout = torch_utils._to_torch(out_features, device=device)   \n",
    "            \n",
    "            # Do part of the matrix math involved in ridge regression optimization out of the loop, \n",
    "            # because this part will be same for all the voxels.\n",
    "#             _cof = _cofactor_fn(_xtrn, lambdas, device=device)\n",
    "            _cof = _cofactor_fn_cpu(_xtrn, lambdas)\n",
    "            \n",
    "            # Now looping over batches of voxels (only reason is because can't store all in memory at same time)\n",
    "            vox_start = time.time()\n",
    "            for rv,lv in numpy_utility.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "                sys.stdout.write('\\rfitting model %4d of %-4d, voxels [%6d:%-6d] of %d' % (m, n_prfs, rv[0], rv[-1], n_voxels))\n",
    "\n",
    "                # Send matrices to gpu\n",
    "                _vtrn = torch_utils._to_torch(trn_data[:,rv], device=device)\n",
    "                _vout = torch_utils._to_torch(out_data[:,rv], device=device)\n",
    "\n",
    "                # Here is where optimization happens - relatively simple matrix math inside loss fn.\n",
    "                _betas, _loss = _loss_fn(_cof, _vtrn, _xout, _vout) #   [#lambda, #feature, #voxel, ], [#lambda, #voxel]\n",
    "                # Now have a set of weights (in betas) and a loss value for every voxel and every lambda. \n",
    "                # goal is then to choose for each voxel, what is the best lambda and what weights went with that lambda.\n",
    "                \n",
    "                # first choose best lambda value and the loss that went with it.\n",
    "                _values, _select = torch.min(_loss, dim=0)\n",
    "                betas = torch_utils.get_value(_betas)\n",
    "                values, select = torch_utils.get_value(_values), torch_utils.get_value(_select)\n",
    "\n",
    "                # comparing this loss to the other models for each voxel (e.g. the other RF position/sizes)\n",
    "                imp = values<best_losses[rv]\n",
    "                \n",
    "                if np.sum(imp)>0:                    \n",
    "                    # for whichever voxels had improvement relative to previous models, save parameters now\n",
    "                    # this means we won't have to save all params for all models, just best.\n",
    "                    arv = np.array(rv)[imp]\n",
    "                    li = select[imp]\n",
    "                    best_lambdas[arv] = li\n",
    "                    best_losses[arv] = values[imp]\n",
    "                    best_models[arv] = m\n",
    "                    if zscore:\n",
    "                        features_mean[arv] = features_m # broadcast over updated voxels\n",
    "                        features_std[arv]  = features_s\n",
    "                    # taking the weights associated with the best lambda value\n",
    "                    best_w_params[arv,:] = numpy_utility.select_along_axis(betas[:,:,imp], li, run_axis=2, choice_axis=0).T\n",
    "              \n",
    "            vox_loop_time += (time.time() - vox_start)\n",
    "            elapsed = (time.time() - vox_start)\n",
    "\n",
    "    # Print information about how fitting went...\n",
    "    total_time = time.time() - start_time\n",
    "    inv_time = total_time - vox_loop_time\n",
    "    return_params = [best_w_params[:,:n_features_total],]\n",
    "    if add_bias:\n",
    "        return_params += [best_w_params[:,-1],]\n",
    "    else: \n",
    "        return_params += [None,]\n",
    "    print ('\\n---------------------------------------')\n",
    "    print ('total time = %fs' % total_time)\n",
    "    print ('total throughput = %fs/voxel' % (total_time / n_voxels))\n",
    "    print ('voxel throughput = %fs/voxel' % (vox_loop_time / n_voxels))\n",
    "    print ('setup throughput = %fs/model' % (inv_time / n_prfs))\n",
    "    sys.stdout.flush()\n",
    "    return best_losses, best_lambdas, [models[best_models],]+return_params+[features_mean, features_std]+[best_models], feature_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from utils import numpy_utility, torch_utils\n",
    "from model_src import fwrf_fit\n",
    "\n",
    "def compute_all_texture_features(_fmaps_fn_complex, _fmaps_fn_simple, images, prf_params, sample_batch_size=100, include_autocorrs=True, include_crosscorrs=True, autocorr_output_pix=3, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "\n",
    "   \n",
    "    if isinstance(prf_params, torch.Tensor):\n",
    "        prf_params = torch_utils.get_value(prf_params)\n",
    "    assert(np.size(prf_params)==3)\n",
    "    prf_params = np.squeeze(prf_params)\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = torch_utils.get_value(images)\n",
    "   \n",
    "\n",
    "    print('Computing pixel-level statistics...')    \n",
    "    t=time.time()\n",
    "    x,y,sigma = prf_params\n",
    "    n_pix=np.shape(images)[2]\n",
    "    g = numpy_utility.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "    spatial_weights = g[2][0]\n",
    "    wmean, wvar, wskew, wkurt = get_weighted_pixel_features(images, spatial_weights, device=device)\n",
    "    elapsed =  time.time() - t\n",
    "    print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    print('Computing complex cell features...')\n",
    "    t = time.time()\n",
    "    complex_feature_means = fwrf_fit.get_features_in_prf(prf_params, _fmaps_fn_complex , images=images, sample_batch_size=sample_batch_size, aperture=aperture, device=device, to_numpy=False)\n",
    "    elapsed =  time.time() - t\n",
    "    print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    print('Computing simple cell features...')\n",
    "    t = time.time()\n",
    "    simple_feature_means = fwrf_fit.get_features_in_prf(prf_params,  _fmaps_fn_simple, images=images, sample_batch_size=sample_batch_size, aperture=aperture,  device=device, to_numpy=False)\n",
    "    elapsed =  time.time() - t\n",
    "    print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    \n",
    "    if include_autocorrs and include_crosscorrs:\n",
    "        print('Computing higher order correlations...')\n",
    "    elif include_crosscorrs:\n",
    "        print('Computing higher order correlations (SKIPPING AUTOCORRELATIONS)...')\n",
    "    elif include_autocorrs:\n",
    "        print('Computing higher order correlations (SKIPPING CROSSCORRELATIONS)...')\n",
    "    else:\n",
    "        print('SKIPPING HIGHER-ORDER CORRELATIONS...')    \n",
    "    t = time.time()\n",
    "    complex_feature_autocorrs, simple_feature_autocorrs, \\\n",
    "    complex_within_scale_crosscorrs, simple_within_scale_crosscorrs, \\\n",
    "    complex_across_scale_crosscorrs, simple_across_scale_crosscorrs = get_higher_order_features(_fmaps_fn_complex, _fmaps_fn_simple, images, prf_params=prf_params, \n",
    "                                                                                                sample_batch_size=sample_batch_size, include_autocorrs=include_autocorrs, include_crosscorrs=include_crosscorrs, \n",
    "                                                                                                autocorr_output_pix=autocorr_output_pix, n_prf_sd_out=n_prf_sd_out, \n",
    "                                                                                                aperture=aperture,  device=device)\n",
    "    elapsed =  time.time() - t\n",
    "    print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    n_trials = len(wmean)\n",
    "    if include_autocorrs and include_crosscorrs:\n",
    "        all_feat = OrderedDict({'wmean': wmean, 'wvar':wvar, 'wskew':wskew, 'wkurt':wkurt, 'complex_feature_means':complex_feature_means, 'simple_feature_means':simple_feature_means, \n",
    "                'complex_feature_autocorrs': torch.reshape(complex_feature_autocorrs, [n_trials,-1]), 'simple_feature_autocorrs': torch.reshape(simple_feature_autocorrs, [n_trials,-1]), \n",
    "                'complex_within_scale_crosscorrs': torch.reshape(complex_within_scale_crosscorrs, [n_trials,-1]), 'simple_within_scale_crosscorrs': torch.reshape(simple_within_scale_crosscorrs, [n_trials,-1]),\n",
    "                'complex_across_scale_crosscorrs': torch.reshape(complex_across_scale_crosscorrs, [n_trials,-1]), 'simple_across_scale_crosscorrs':torch.reshape(simple_across_scale_crosscorrs, [n_trials,-1])})\n",
    "    elif include_crosscorrs:\n",
    "        all_feat = OrderedDict({'wmean': wmean, 'wvar':wvar, 'wskew':wskew, 'wkurt':wkurt,  'complex_feature_means':complex_feature_means, 'simple_feature_means':simple_feature_means,                 \n",
    "                'complex_within_scale_crosscorrs': torch.reshape(complex_within_scale_crosscorrs, [n_trials,-1]), 'simple_within_scale_crosscorrs': torch.reshape(simple_within_scale_crosscorrs, [n_trials,-1]),\n",
    "                'complex_across_scale_crosscorrs': torch.reshape(complex_across_scale_crosscorrs, [n_trials,-1]), 'simple_across_scale_crosscorrs':torch.reshape(simple_across_scale_crosscorrs, [n_trials,-1])})\n",
    "    elif include_autocorrs:\n",
    "        all_feat = OrderedDict({'wmean': wmean, 'wvar':wvar, 'wskew':wskew, 'wkurt':wkurt, 'complex_feature_means':complex_feature_means, 'simple_feature_means':simple_feature_means, \n",
    "                'complex_feature_autocorrs': torch.reshape(complex_feature_autocorrs, [n_trials,-1]), 'simple_feature_autocorrs': torch.reshape(simple_feature_autocorrs, [n_trials,-1]) })\n",
    "    else:\n",
    "        all_feat = OrderedDict({'wmean': wmean, 'wvar':wvar, 'wskew':wskew, 'wkurt':wkurt, 'complex_feature_means':complex_feature_means, 'simple_feature_means':simple_feature_means})\n",
    "\n",
    "    feature_names = list(all_feat.keys())\n",
    "\n",
    "    for ff, feature_name in enumerate(feature_names): \n",
    "        if ff==0:\n",
    "            all_feat_concat = all_feat[feature_name]\n",
    "            feature_type_labels = ff*np.ones([1,all_feat[feature_name].shape[1]])\n",
    "        else:               \n",
    "            all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "            feature_type_labels = np.concatenate((feature_type_labels, ff*np.ones([1,all_feat[feature_name].shape[1]])), axis=1)\n",
    "                \n",
    "    print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "    print('Feature types included are:')\n",
    "    print(feature_names)\n",
    "    \n",
    "    if torch.any(torch.isnan(all_feat_concat)):\n",
    "        print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "            \n",
    "    return torch_utils.get_value(all_feat_concat), [feature_type_labels, feature_names]\n",
    "\n",
    "def get_higher_order_features(_fmaps_fn_complex, _fmaps_fn_simple, images, prf_params, sample_batch_size=20, include_autocorrs=True, include_crosscorrs=True, autocorr_output_pix=7, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute all higher-order features (cross-spatial and cross-feature correlations) for a batch of images.\n",
    "    Input the functions that define first level feature maps (simple and complex cells), and prf parameters.\n",
    "    Returns arrays of each higher order feature.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')    \n",
    "        \n",
    "    n_trials = np.shape(images)[0]\n",
    "    \n",
    "    assert(np.mod(autocorr_output_pix,2)==1) # must be odd!\n",
    "\n",
    "    n_features_simple, fmaps_rez = fwrf_fit.get_fmaps_sizes(_fmaps_fn_simple, images[0:sample_batch_size], device)\n",
    "    n_features_complex, fmaps_rez = fwrf_fit.get_fmaps_sizes(_fmaps_fn_complex, images[0:sample_batch_size], device)\n",
    "    \n",
    "    n_sf = len(fmaps_rez)\n",
    "    n_ori = int(n_features_complex/n_sf)\n",
    "    n_phases = 2\n",
    "    \n",
    "    # all pairs of different orientation channels.\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "\n",
    "    if include_autocorrs:\n",
    "        complex_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, autocorr_output_pix**2], device=device)\n",
    "        simple_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, n_phases, autocorr_output_pix**2], device=device)\n",
    "    else:\n",
    "        complex_feature_autocorrs = None\n",
    "        simple_feature_autocorrs = None\n",
    "    \n",
    "    if include_crosscorrs:\n",
    "        complex_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device)\n",
    "        simple_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_phases, n_ori_pairs], device=device)\n",
    "        complex_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_ori, n_ori], device=device)\n",
    "        simple_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_phases, n_ori, n_ori], device=device) # only done for pairs of neighboring SF.\n",
    "    else:\n",
    "        complex_within_scale_crosscorrs = None\n",
    "        simple_within_scale_crosscorrs = None\n",
    "        complex_across_scale_crosscorrs = None\n",
    "        simple_across_scale_crosscorrs = None\n",
    "        \n",
    "    if include_autocorrs or include_crosscorrs:\n",
    "        \n",
    "        x,y,sigma = prf_params\n",
    "\n",
    "        bb=-1\n",
    "        for batch_inds, batch_size_actual in numpy_utility.iterate_range(0, n_trials, sample_batch_size):\n",
    "            bb=bb+1\n",
    "\n",
    "            fmaps_complex = _fmaps_fn_complex(torch_utils._to_torch(images[batch_inds],device=device))   \n",
    "            fmaps_simple =  _fmaps_fn_simple(torch_utils._to_torch(images[batch_inds],device=device))\n",
    "\n",
    "            # First looping over frequency (scales)\n",
    "            for ff in range(n_sf):\n",
    "\n",
    "                # Scale specific things - get the prf at this resolution of interest\n",
    "                n_pix = fmaps_rez[ff]\n",
    "                g = numpy_utility.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "                spatial_weights = g[2][0]\n",
    "\n",
    "                patch_bbox_rect = get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=False)\n",
    "                # for autocorrelation, forcing the input region to be square\n",
    "                patch_bbox_square = get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True)\n",
    "\n",
    "                # Loop over orientation channels\n",
    "                xx=-1\n",
    "                for oo1 in range(n_ori):       \n",
    "\n",
    "\n",
    "                    # Simple cell responses - loop over two phases per orient.\n",
    "                    for pp in range(n_phases):\n",
    "                        filter_ind = n_phases*oo1+pp  # orients and phases are both listed in the same dimension of filters matrix               \n",
    "                        simple1 = fmaps_simple[ff][:,filter_ind,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                        # Simple cell autocorrelations.\n",
    "                        if include_autocorrs:\n",
    "                            auto_corr = weighted_auto_corr_2d(simple1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix, subtract_patch_mean = True, enforce_size=True, device=device)\n",
    "                            simple_feature_autocorrs[batch_inds,ff,oo1,pp,:] = torch.reshape(auto_corr, [batch_size_actual, autocorr_output_pix**2])\n",
    "\n",
    "                    # Complex cell responses\n",
    "                    complex1 = fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                    # Complex cell autocorrelation (correlation w spatially shifted versions of itself)\n",
    "                    if include_autocorrs:\n",
    "                        auto_corr = weighted_auto_corr_2d(complex1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix, subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "                        complex_feature_autocorrs[batch_inds,ff,oo1,:] = torch.reshape(auto_corr, [batch_size_actual, autocorr_output_pix**2])\n",
    "\n",
    "                    if include_crosscorrs:\n",
    "                        # Within-scale correlations - compare resp at orient==oo1 to responses at all other orientations, same scale.\n",
    "                        for oo2 in np.arange(oo1+1, n_ori):            \n",
    "                            xx = xx+1 \n",
    "                            assert(oo1==ori_pairs[xx,0] and oo2==ori_pairs[xx,1])\n",
    "\n",
    "                            complex2 = fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])      \n",
    "\n",
    "                            # Complex cell within-scale cross correlations\n",
    "                            cross_corr = weighted_cross_corr_2d(complex1, complex2, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "\n",
    "                            complex_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                            # Simple cell within-scale cross correlations\n",
    "                            for pp in range(n_phases):\n",
    "                                filter_ind = n_phases*oo2+pp\n",
    "                                simple2 = fmaps_simple[ff][:,filter_ind,:,:].view([batch_size_actual,1,n_pix,n_pix])\n",
    "\n",
    "                                cross_corr = weighted_cross_corr_2d(simple1, simple2, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                simple_within_scale_crosscorrs[batch_inds,ff,pp,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                        # Cross-scale correlations - for these we care about same ori to same ori, so looping over all ori.\n",
    "                        # Only for neighboring scales, so the first level doesn't get one\n",
    "                        if ff>0:\n",
    "\n",
    "                            for oo2 in range(n_ori):\n",
    "\n",
    "                                # Complex cell response for neighboring scale\n",
    "                                complex2_neighborscale = fmaps_complex[ff-1][:,oo2,:,:].view([batch_size_actual,1,fmaps_rez[ff-1], -1])\n",
    "                                # Resize so that it can be compared w current scale\n",
    "                                complex2_neighborscale = torch.nn.functional.interpolate(complex2_neighborscale, [n_pix, n_pix], mode='bilinear', align_corners=True)\n",
    "\n",
    "                                cross_corr = weighted_cross_corr_2d(complex1, complex2_neighborscale, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                complex_across_scale_crosscorrs[batch_inds,ff-1, oo1, oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                                for pp in range(n_phases):\n",
    "                                    filter_ind = n_phases*oo2+pp\n",
    "                                    # Simple cell response for neighboring scale\n",
    "                                    simple2_neighborscale = fmaps_simple[ff-1][:,filter_ind,:,:].view([batch_size_actual,1,fmaps_rez[ff-1], -1])\n",
    "                                    simple2_neighborscale = torch.nn.functional.interpolate(simple2_neighborscale, [n_pix, n_pix], mode='bilinear', align_corners=True)\n",
    "\n",
    "                                    cross_corr = weighted_cross_corr_2d(simple1, simple2_neighborscale, spatial_weights, patch_bbox=patch_bbox_rect, subtract_patch_mean = True, device=device)\n",
    "                                    simple_across_scale_crosscorrs[batch_inds,ff-1, pp, oo1, oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "\n",
    "\n",
    "    return complex_feature_autocorrs, simple_feature_autocorrs, complex_within_scale_crosscorrs, simple_within_scale_crosscorrs, complex_across_scale_crosscorrs, simple_across_scale_crosscorrs\n",
    "\n",
    "\n",
    "def get_bbox_from_prf(prf_params, image_size, n_prf_sd_out=2, verbose=False, force_square=False):\n",
    "    \"\"\"\n",
    "    For a given pRF center and size, calculate the square bounding box that captures a specified number of SDs from the center (default=2 SD)\n",
    "    Returns [xmin, xmax, ymin, ymax]\n",
    "    \"\"\"\n",
    "    x,y,sigma = prf_params\n",
    "    n_pix = image_size[0]\n",
    "    assert(image_size[1]==n_pix)\n",
    "    assert(sigma>0 and n_prf_sd_out>0)\n",
    "    \n",
    "    # decide on the window to use for correlations, based on prf parameters. Patch goes # SD from the center (2 by default).\n",
    "    # note this can't be < 1, even for the smallest choice of parameters (since rounding up). this way it won't be too small.\n",
    "    pix_from_center = int(np.ceil(sigma*n_prf_sd_out*n_pix))\n",
    "\n",
    "    # center goes [row ind, col ind]\n",
    "    center = np.array((n_pix/2  - y*n_pix, x*n_pix + n_pix/2)) # note that the x/y dims get swapped here because of how pRF parameters are defined.\n",
    "\n",
    "    # now defining the extent of the bbox. want to err on the side of making it too big, so taking floor/ceiling...\n",
    "    xmin = int(np.floor(center[0]-pix_from_center))\n",
    "    xmax = int(np.ceil(center[0]+pix_from_center))\n",
    "    ymin = int(np.floor(center[1]-pix_from_center))\n",
    "    ymax = int(np.ceil(center[1]+pix_from_center))\n",
    "\n",
    "    # cropping it to within the image bounds. Can end up being a rectangle rather than square.\n",
    "    [xmin, xmax, ymin, ymax] = np.maximum(np.minimum([xmin, xmax, ymin, ymax], n_pix), 0)\n",
    "\n",
    "    # decide if we want square or are ok with a rectangle\n",
    "    if force_square:\n",
    "        minside = np.min([xmax-xmin, ymax-ymin])\n",
    "        maxside = np.max([xmax-xmin, ymax-ymin])\n",
    "        if minside!=maxside:\n",
    "\n",
    "            if verbose:\n",
    "                print('trimming bbox to make it square')\n",
    "                print('original bbox was:')\n",
    "                print([xmin, xmax, ymin, ymax])\n",
    "\n",
    "            n2trim = [int(np.floor((maxside-minside)/2)), int(np.ceil((maxside-minside)/2))]\n",
    "\n",
    "            if np.argmin([xmax-xmin, ymax-ymin])==0:\n",
    "                ymin = ymin+n2trim[0]\n",
    "                ymax = ymax-n2trim[1]\n",
    "            else:\n",
    "                xmin = xmin+n2trim[0]\n",
    "                xmax = xmax-n2trim[1]\n",
    "\n",
    "        assert((xmax-xmin)==(ymax-ymin))\n",
    "\n",
    "    if verbose:\n",
    "        print('final bbox will be:')\n",
    "        print([xmin, xmax, ymin, ymax])\n",
    "        \n",
    "    # checking to see if the patch has become just one pixel. this can happen due to the cropping.\n",
    "    # if this happens, cross-correlations will give zero.\n",
    "    if ((xmax-xmin)<2 or (ymax-ymin)<2):\n",
    "        print('Warning: your patch only has one pixel (for n_pix: %d and prf params: [%.2f, %.2f, %.2f])\\n'%(n_pix,x,y,sigma))      \n",
    "        \n",
    "    return [xmin, xmax, ymin, ymax]\n",
    "\n",
    "\n",
    "def weighted_auto_corr_2d(images, spatial_weights, patch_bbox=None, output_pix=None, subtract_patch_mean=False, enforce_size=False, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute autocorrelation of a batch of images, weighting the pixels based on the values in spatial_weights (could be for instance a pRF definition for a voxel).\n",
    "    Can optionally specify a square patch of the image to compute over, based on \"patch_bbox\" params. Otherwise use whole image.\n",
    "    Using fft method to compute, should be fast.\n",
    "    Input parameters:\n",
    "        patch_bbox: (optional) bounding box of the patch to use for this calculation. [xmin xmax ymin ymax], see get_bbox_from_prf\n",
    "        output_pix: the size of the autocorrelation matrix output by this function. If this is an even number, the output size is this value +1. Achieved by cropping out the center of the final autocorrelation \n",
    "            matrix  (note that the full image patch is still used in computing the autocorrelation, but just the center values are returned).\n",
    "            If None, then returns the full autocorrelation matrix (same size as image patch.)\n",
    "        subtract_patch_mean: subtract weighted mean of image before computing autocorr?\n",
    "        enforce_size: if image patch is smaller than desired output, should we pad w zeros so that it has to be same size?\n",
    "    Returns:\n",
    "        A matrix describing the correlation of the image and various spatially shifted versions of it.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')        \n",
    "    if isinstance(images, np.ndarray):\n",
    "        images = torch_utils._to_torch(images, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)\n",
    "            \n",
    "    if len(np.shape(images))==2:\n",
    "        # pretend the batch and channel dims exist, for 2D input only (3D won't work)\n",
    "        single_image=True\n",
    "        images = images.view([1,1,images.shape[0],-1])\n",
    "    else:\n",
    "        single_image=False\n",
    "        \n",
    "    # have to be same size\n",
    "    assert(images.shape[2]==spatial_weights.shape[0] and images.shape[3]==spatial_weights.shape[1])\n",
    "    # images is [batch_size x n_channels x nPix x nPix]\n",
    "    batch_size = images.shape[0]\n",
    "    n_channels = images.shape[1]    \n",
    "   \n",
    "    if patch_bbox is not None:    \n",
    "        [xmin, xmax, ymin, ymax] = patch_bbox\n",
    "        # first crop out the region of the image that's currently of interest\n",
    "        images = images[:,:,xmin:xmax, ymin:ymax]\n",
    "        # crop same region from spatial weights matrix\n",
    "        spatial_weights = spatial_weights[xmin:xmax, ymin:ymax]\n",
    "\n",
    "    # make sure these sum to 1\n",
    "    if not torch.sum(spatial_weights)==0.0:\n",
    "        spatial_weights = spatial_weights/torch.sum(spatial_weights)   \n",
    "   \n",
    "    spatial_weights = spatial_weights.view([1,1,spatial_weights.shape[0],-1]).expand([batch_size,n_channels,-1,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "    \n",
    "    # compute autocorrelation of this image patch\n",
    "    if subtract_patch_mean:\n",
    "\n",
    "        wmean = torch.sum(torch.sum(images * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean = wmean.view([batch_size,-1,1,1]).expand([-1,-1,images.shape[2],images.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        \n",
    "        weighted_images = (images - wmean) * torch.sqrt(spatial_weights) # square root of the weights here because they will get squared again in next operation\n",
    "        \n",
    "        auto_corr = torch.fft.fftshift(torch.real(torch.fft.ifft2(torch.abs(torch.fft.fft2(weighted_images, dim=[2,3]))**2, dim=[2,3])), dim=[2,3]);\n",
    "    else:\n",
    "        weighted_images = images * torch.sqrt(spatial_weights)\n",
    "        auto_corr = torch.fft.fftshift(torch.real(torch.fft.ifft2(torch.abs(torch.fft.fft2(weighted_images, dim=[2,3]))**2, dim=[2,3])), dim=[2,3]);\n",
    "\n",
    "    if output_pix is not None:\n",
    "\n",
    "        # crop out just the center region\n",
    "        new_center = int(np.floor(auto_corr.shape[2]/2))\n",
    "        n_pix_out = np.min([int(np.floor(output_pix/2)), np.min([new_center, auto_corr.shape[2]-new_center])])\n",
    "        auto_corr = auto_corr[:,:,new_center-n_pix_out:new_center+n_pix_out+1, new_center-n_pix_out:new_center+n_pix_out+1]        \n",
    "    \n",
    "    if enforce_size and not (np.shape(auto_corr)[2]==output_pix or np.shape(auto_corr)[2]==output_pix+1):\n",
    "        \n",
    "        # just pad w zeros if want same size.\n",
    "        pix_diff = output_pix - np.shape(auto_corr)[2]   \n",
    "        auto_corr = torch.nn.functional.pad(auto_corr, [int(np.floor(pix_diff/2)), int(np.ceil(pix_diff/2)), int(np.floor(pix_diff/2)), int(np.ceil(pix_diff/2))], mode='constant', value=0)\n",
    "        assert(np.shape(auto_corr)[2]==output_pix and np.shape(auto_corr)[3]==output_pix)\n",
    "\n",
    "    if single_image:\n",
    "        auto_corr = torch.squeeze(auto_corr)\n",
    "        \n",
    "    return auto_corr\n",
    "\n",
    "def weighted_cross_corr_2d(images1, images2, spatial_weights, patch_bbox=None, subtract_patch_mean=True, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute cross-correlation of two identically-sized images, weighting the pixels based on the values in spatial_weights (could be for instance a pRF definition for a voxel).\n",
    "    Can optionally specify a square patch of the image to compute over, based on \"patch_bbox\" params. Otherwise use whole image.\n",
    "    Basically a dot product of image values.\n",
    "    Input parameters:\n",
    "        patch_bbox: (optional) bounding box of the patch to use for this calculation. [xmin xmax ymin ymax], see get_bbox_from_prf\n",
    "        subtract_patch_mean: do you want to subtract the weighted mean of image patch before computing?\n",
    "    Returns:\n",
    "        A single value that captures correlation between images (zero spatial shift)\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')  \n",
    "    if isinstance(images1, np.ndarray):\n",
    "        images1 = torch_utils._to_torch(images1, device)\n",
    "    if isinstance(images2, np.ndarray):\n",
    "        images2 = torch_utils._to_torch(images2, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)      \n",
    "    \n",
    "    if len(np.shape(images1))==2:\n",
    "        # pretend the batch and channel dims exist, for 2D input only (3D won't work)\n",
    "        single_image=True\n",
    "        images1 = images1.view([1,1,images1.shape[0],-1])\n",
    "        images2 = images2.view([1,1,images2.shape[0],-1])\n",
    "    else:\n",
    "        single_image=False\n",
    "        \n",
    "    # have to be same size\n",
    "    assert(images1.shape==images2.shape)\n",
    "    assert(images1.shape[2]==spatial_weights.shape[0] and images1.shape[3]==spatial_weights.shape[1])\n",
    "    assert(images2.shape[2]==spatial_weights.shape[0] and images2.shape[3]==spatial_weights.shape[1])\n",
    "    # images is [batch_size x n_channels x nPix x nPix]\n",
    "    batch_size = images1.shape[0]\n",
    "    n_channels = images1.shape[1]\n",
    "    \n",
    "\n",
    "    if patch_bbox is not None:\n",
    "        [xmin, xmax, ymin, ymax] = patch_bbox\n",
    "        # first crop out the region of the image that's currently of interest\n",
    "        images1 = images1[:,:,xmin:xmax, ymin:ymax]\n",
    "        images2 = images2[:,:,xmin:xmax, ymin:ymax]\n",
    "        # crop same region from spatial weights matrix\n",
    "        spatial_weights = spatial_weights[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    # make sure the wts sum to 1\n",
    "    if not torch.sum(spatial_weights)==0.0:\n",
    "        spatial_weights = spatial_weights/torch.sum(spatial_weights)\n",
    "    spatial_weights = spatial_weights.view([1,1,spatial_weights.shape[0],-1]).expand([batch_size,n_channels,-1,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "    \n",
    "    # compute cross-correlation\n",
    "    if subtract_patch_mean:\n",
    "        # subtract mean of each weighted image patch and take their dot product.\n",
    "        # this quantity is equal to weighted covariance (only true if mean-centered)\n",
    "        wmean1 = torch.sum(torch.sum(images1 * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean1 = wmean1.view([batch_size,-1,1,1]).expand([-1,-1,images1.shape[2],images1.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        wmean2 = torch.sum(torch.sum(images2 * spatial_weights, dim=3), dim=2) # size is [batch_size x 1]\n",
    "        wmean2 = wmean2.view([batch_size,-1,1,1]).expand([-1,-1,images2.shape[2],images2.shape[3]]) # [batch_size x n_channels x nPix x nPix]\n",
    "        weighted_images1 = (images1 - wmean1) * torch.sqrt(spatial_weights) # square root of the weights here because they will get squared again in dot product operation.\n",
    "        weighted_images2 = (images2 - wmean2) * torch.sqrt(spatial_weights)\n",
    "\n",
    "        cross_corr = torch.sum(torch.sum(weighted_images1 * weighted_images2, dim=3), dim=2)    \n",
    "\n",
    "    else:\n",
    "        # dot product of raw (weighted) values\n",
    "        # this is closer to what scipy.signal.correlate2d will do (except this is weighted)\n",
    "        weighted_images1 = images1 * torch.sqrt(spatial_weights)\n",
    "        weighted_images2 = images2 * torch.sqrt(spatial_weights)\n",
    "        cross_corr = torch.sum(torch.sum(weighted_images1 * weighted_images2, dim=3), dim=2)      \n",
    "        \n",
    "    if single_image:\n",
    "        cross_corr = torch.squeeze(cross_corr)\n",
    "        \n",
    "    return cross_corr\n",
    "\n",
    "\n",
    "\n",
    "def get_weighted_pixel_features(image_batch, spatial_weights, device=None):\n",
    "    \"\"\"\n",
    "    Compute mean, variance, skewness, kurtosis of luminance values for each of a batch of images.\n",
    "    Input size is [batch_size x n_channels x npix x npix]\n",
    "    Spatial weights describes a weighting function, [npix x npix]\n",
    "    Returns [batch_size x n_channels] size array for each property.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(image_batch, np.ndarray):\n",
    "        image_batch = torch_utils._to_torch(image_batch, device)\n",
    "    if isinstance(spatial_weights, np.ndarray):\n",
    "        spatial_weights = torch_utils._to_torch(spatial_weights, device)\n",
    "     \n",
    "    assert(image_batch.shape[2]==spatial_weights.shape[0] and image_batch.shape[3]==spatial_weights.shape[1])\n",
    "    assert(image_batch.shape[1]==1)\n",
    "    \n",
    "    batch_size = image_batch.shape[0]\n",
    "    n_channels = image_batch.shape[1]\n",
    "    n_pix = image_batch.shape[2]\n",
    "\n",
    "    image_batch = image_batch.view([batch_size, n_channels, n_pix**2])\n",
    "    spatial_weights = spatial_weights/torch.sum(spatial_weights)\n",
    "    spatial_weights = spatial_weights.view([1,1,n_pix**2]).expand([batch_size,n_channels,-1]) # [batch_size x n_channels x nPix x nPix]    \n",
    "   \n",
    "    ims_weighted = image_batch * spatial_weights\n",
    "   \n",
    "    wmean = torch.sum(ims_weighted, axis=2).view([batch_size,-1,1])\n",
    "\n",
    "    wvar = torch.sum(spatial_weights * (image_batch - wmean.expand([-1,-1,n_pix**2]))**2, axis=2).view([batch_size,-1,1])\n",
    "    \n",
    "    wskew = torch.sum(spatial_weights *(image_batch - wmean.expand([-1,-1,n_pix**2]))**3 / (wvar**(3/2)), axis=2).view([batch_size,-1,1])\n",
    "    \n",
    "    wkurt = torch.sum(spatial_weights *(image_batch - wmean.expand([-1,-1,n_pix**2]))**4 / (wvar**(2)), axis=2).view([batch_size,-1,1])\n",
    "    \n",
    "    # correct for nans/inf values which happen when variance is very small (denominator)\n",
    "    wskew[torch.isnan(wskew)] = 0.0\n",
    "    wkurt[torch.isnan(wkurt)] = 0.0\n",
    "    wskew[torch.isinf(wskew)] = 0.0\n",
    "    wkurt[torch.isinf(wkurt)] = 0.0\n",
    "    \n",
    "    return torch.squeeze(wmean, dim=2), torch.squeeze(wvar, dim=2), torch.squeeze(wskew, dim=2), torch.squeeze(wkurt, dim=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
