{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9574d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic modules\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "import argparse\n",
    "import skimage.transform\n",
    "\n",
    "# import custom modules\n",
    "root_dir   = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(root_dir,'code'))\n",
    "from model_src import fwrf_fit as fwrf_fit\n",
    "from model_src import fwrf_predict as fwrf_predict\n",
    "from model_src import texture_statistics_gabor, texture_statistics_pyramid\n",
    "\n",
    "from model_fitting import initialize_fitting\n",
    "\n",
    "fpX = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7fb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=1\n",
    "roi=None\n",
    "\n",
    "ridge=0\n",
    "\n",
    "shuffle_images=0\n",
    "random_images=0\n",
    "random_voxel_data=0\n",
    "\n",
    "sample_batch_size=100\n",
    "voxel_batch_size=100\n",
    "zscore_features=1\n",
    "nonlin_fn=0\n",
    "padding_mode='circular'\n",
    "\n",
    "n_ori=4\n",
    "n_sf=4\n",
    "up_to_sess=1\n",
    "debug=1\n",
    "shuff_rnd_seed=0\n",
    "# shuff_rnd_seed=251709\n",
    "\n",
    "fitting_type='texture'\n",
    "\n",
    "do_fitting=1\n",
    "do_val=1\n",
    "do_partial=1\n",
    "date_str=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeda7778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f49dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce GTX TITAN X\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n",
      "Time Stamp: Aug-23-2021_2032\n",
      "\n",
      "Will save final output file to /user_data/mmhender/model_fits/S01/texture_pyramid_OLS_4ori_4sf/Aug-23-2021_2032_DEBUG/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = initialize_fitting.init_cuda()\n",
    "# device = torch.device('cpu:0')\n",
    "nsd_root, stim_root, beta_root, mask_root = initialize_fitting.get_paths()\n",
    "model_name, feature_types_exclude = initialize_fitting.get_pyramid_model_name(ridge, n_ori, n_sf)\n",
    "\n",
    "if do_fitting==False and date_str is None:\n",
    "    raise ValueError('if you want to start midway through the process (--do_fitting=False), then specify the date when training result was saved (--date_str).')\n",
    "\n",
    "if do_fitting==True and date_str is not None:\n",
    "    raise ValueError('if you want to do fitting from scratch (--do_fitting=True), specify --date_str=None (rather than entering a date)')\n",
    "\n",
    "output_dir, fn2save = initialize_fitting.get_save_path(root_dir, subject, model_name, shuffle_images, random_images, random_voxel_data, debug, date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f902c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(688, 1, 240, 240)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_stim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51b7f033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing steerable pyramid features from memory.\n"
     ]
    }
   ],
   "source": [
    "_texture_fn.clear_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a88aed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "(100, 1, 240, 240)\n",
      "time elapsed = 18.51205\n",
      "Computing higher order correlations...\n",
      "time elapsed = 1.04776\n",
      "Final size of features concatenated is [100 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n"
     ]
    }
   ],
   "source": [
    "feats = _texture_fn(trn_stim_data[0:100], models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ef0db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing higher order correlations...\n",
      "time elapsed = 1.20195\n",
      "Final size of features concatenated is [100 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n"
     ]
    }
   ],
   "source": [
    "feats = _texture_fn(trn_stim_data[0:100], models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8975c779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 30, 60, 120, 240]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npix_each_scale = [_fmaps_fn.pyr.pyr_size[(sc,0)][0] for sc in range(n_sf)]\n",
    "npix_each_scale.append(_fmaps_fn.pyr.pyr_size['residual_lowpass'][0])\n",
    "npix_each_scale.reverse()\n",
    "npix_each_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3022cc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 30, 60, 120, 240]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps_complex_all, fmaps_resid_all, fmaps_lowpass_recon_all, fmaps_coarser_upsampled_all = maps\n",
    "   \n",
    "npix_each_scale = [fmaps_complex_all[sc].shape[2] for sc in np.arange(n_sf-1,-1,-1)]\n",
    "npix_each_scale.append(fmaps_resid_all[0].shape[2])\n",
    "npix_each_scale.reverse()\n",
    "npix_each_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3348d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([688, 641])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "208827f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide what voxels to use  \n",
    "voxel_mask, voxel_index, voxel_roi, voxel_ncsnr, brain_nii_shape = initialize_fitting.get_voxel_info(mask_root, beta_root, subject, roi)\n",
    "\n",
    "# get all data and corresponding images, in two splits. always fixed set that gets left out\n",
    "trn_stim_data, trn_voxel_data, val_stim_single_trial_data, val_voxel_single_trial_data, \\\n",
    "    n_voxels, n_trials_val, image_order = initialize_fitting.get_data_splits(nsd_root, beta_root, stim_root, subject, voxel_mask, up_to_sess, \n",
    "                                                                             shuffle_images=shuffle_images, random_images=random_images, random_voxel_data=random_voxel_data)\n",
    "\n",
    "# Need a multiple of 8\n",
    "process_at_size=240\n",
    "trn_stim_data = skimage.transform.resize(trn_stim_data, output_shape=(trn_stim_data.shape[0],1,process_at_size, process_at_size))\n",
    "val_stim_single_trial_data = skimage.transform.resize(val_stim_single_trial_data, output_shape=(val_stim_single_trial_data.shape[0],1,process_at_size, process_at_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cabfd525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n",
      "[]\n",
      "\n",
      "Possible lambda values are:\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Set up the pyramid\n",
    "_fmaps_fn = steerable_pyramid_extractor(pyr_height=n_sf, n_ori = n_ori)\n",
    "# Params for the spatial aspect of the model (possible pRFs)\n",
    "#     aperture_rf_range=0.8 # using smaller range here because not sure what to do with RFs at edges...\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "n_prf_sd_out=2\n",
    "_texture_fn = texture_feature_extractor(_fmaps_fn,sample_batch_size=sample_batch_size, feature_types_exclude=feature_types_exclude, n_prf_sd_out=n_prf_sd_out, aperture=aperture, device=device)\n",
    "\n",
    "# More params for fitting\n",
    "holdout_size, lambdas = initialize_fitting.get_fitting_pars(trn_voxel_data, zscore_features, ridge=ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b707ce74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_size = 619 (90.0%)\n",
      "dtype = <class 'numpy.float32'>\n",
      "device = cuda:0\n",
      "---------------------------------------\n",
      "Computing a new random seed\n",
      "Seeding random number generator: seed is 242123\n",
      "Clearing steerable pyramid features from memory.\n",
      "\n",
      "\n",
      "Getting features for prf 0: [x,y,sigma] is [-0.55 -0.55 0.0400]\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "(688, 1, 240, 240)\n",
      "time elapsed = 126.02994\n",
      "Computing higher order correlations...\n",
      "time elapsed = 6.94720\n",
      "Final size of features concatenated is [688 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "\n",
      "Fitting version 0 of 15: full_model, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 1 of 15: leave_out_pixel_stats, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 2 of 15: leave_out_mean_magnitudes, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 3 of 15: leave_out_mean_realparts, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 4 of 15: leave_out_marginal_stats_lowpass_recons, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 5 of 15: leave_out_variance_highpass_resid, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 6 of 15: leave_out_magnitude_feature_autocorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 7 of 15: leave_out_lowpass_recon_autocorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 8 of 15: leave_out_highpass_resid_autocorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 9 of 15: leave_out_magnitude_within_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 10 of 15: leave_out_real_within_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 11 of 15: leave_out_magnitude_across_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 12 of 15: leave_out_real_imag_across_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 13 of 15: leave_out_real_spatshift_within_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 14 of 15: leave_out_real_spatshift_across_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Getting features for prf 1: [x,y,sigma] is [-0.49 -0.55 0.0400]\n",
      "Computing higher order correlations...\n",
      "time elapsed = 6.91371\n",
      "Final size of features concatenated is [688 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "\n",
      "Fitting version 0 of 15: full_model, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 1 of 15: leave_out_pixel_stats, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 2 of 15: leave_out_mean_magnitudes, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 3 of 15: leave_out_mean_realparts, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 4 of 15: leave_out_marginal_stats_lowpass_recons, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 5 of 15: leave_out_variance_highpass_resid, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 6 of 15: leave_out_magnitude_feature_autocorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 7 of 15: leave_out_lowpass_recon_autocorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 8 of 15: leave_out_highpass_resid_autocorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 9 of 15: leave_out_magnitude_within_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 10 of 15: leave_out_real_within_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 11 of 15: leave_out_magnitude_across_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 12 of 15: leave_out_real_imag_across_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 13 of 15: leave_out_real_spatshift_within_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 14 of 15: leave_out_real_spatshift_across_scale_crosscorrs, \n",
      "Voxels [ 14900:14912 ] of 14913\n",
      "---------------------------------------\n",
      "total time = 156.149505s\n",
      "total throughput = 0.010471s/voxel\n",
      "voxel throughput = 0.000723s/voxel\n",
      "setup throughput = 0.166142s/model\n",
      "Clearing steerable pyramid features from memory.\n"
     ]
    }
   ],
   "source": [
    "do_varpart=True\n",
    "best_losses, best_lambdas, best_params, feature_info = fit_texture_model_ridge(trn_stim_data, trn_voxel_data, _texture_fn, models, lambdas, \\\n",
    "            zscore=zscore_features, voxel_batch_size=voxel_batch_size, holdout_size=holdout_size, \\\n",
    "                                                                               shuffle=True, add_bias=True, debug=debug, shuff_rnd_seed=shuff_rnd_seed,device=device, do_varpart=do_varpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14ba2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing steerable pyramid features from memory.\n",
      "Getting features for prf 0: [x,y,sigma] is [-0.55 -0.55 0.0400]\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "(62, 1, 240, 240)\n",
      "time elapsed = 11.52642\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.80326\n",
      "Final size of features concatenated is [62 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "Getting features for prf 1: [x,y,sigma] is [-0.49 -0.55 0.0400]\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.69474\n",
      "Final size of features concatenated is [62 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "Clearing steerable pyramid features from memory.\n",
      "Getting predictions for voxels [0-99] of 14913\n",
      "Evaluating version 0 of 15: full_model\n",
      "Evaluating version 1 of 15: leave_out_pixel_stats\n",
      "Evaluating version 2 of 15: leave_out_mean_magnitudes\n",
      "Evaluating version 3 of 15: leave_out_mean_realparts\n",
      "Evaluating version 4 of 15: leave_out_marginal_stats_lowpass_recons\n",
      "Evaluating version 5 of 15: leave_out_variance_highpass_resid\n",
      "Evaluating version 6 of 15: leave_out_magnitude_feature_autocorrs\n",
      "Evaluating version 7 of 15: leave_out_lowpass_recon_autocorrs\n",
      "Evaluating version 8 of 15: leave_out_highpass_resid_autocorrs\n",
      "Evaluating version 9 of 15: leave_out_magnitude_within_scale_crosscorrs\n",
      "Evaluating version 10 of 15: leave_out_real_within_scale_crosscorrs\n",
      "Evaluating version 11 of 15: leave_out_magnitude_across_scale_crosscorrs\n",
      "Evaluating version 12 of 15: leave_out_real_imag_across_scale_crosscorrs\n",
      "Evaluating version 13 of 15: leave_out_real_spatshift_within_scale_crosscorrs\n",
      "Evaluating version 14 of 15: leave_out_real_spatshift_across_scale_crosscorrs\n",
      "Getting predictions for voxels [100-199] of 14913\n",
      "Evaluating version 0 of 15: full_model\n",
      "Evaluating version 1 of 15: leave_out_pixel_stats\n",
      "Evaluating version 2 of 15: leave_out_mean_magnitudes\n",
      "Evaluating version 3 of 15: leave_out_mean_realparts\n",
      "Evaluating version 4 of 15: leave_out_marginal_stats_lowpass_recons\n",
      "Evaluating version 5 of 15: leave_out_variance_highpass_resid\n",
      "Evaluating version 6 of 15: leave_out_magnitude_feature_autocorrs\n",
      "Evaluating version 7 of 15: leave_out_lowpass_recon_autocorrs\n",
      "Evaluating version 8 of 15: leave_out_highpass_resid_autocorrs\n",
      "Evaluating version 9 of 15: leave_out_magnitude_within_scale_crosscorrs\n",
      "Evaluating version 10 of 15: leave_out_real_within_scale_crosscorrs\n",
      "Evaluating version 11 of 15: leave_out_magnitude_across_scale_crosscorrs\n",
      "Evaluating version 12 of 15: leave_out_real_imag_across_scale_crosscorrs\n",
      "Evaluating version 13 of 15: leave_out_real_spatshift_within_scale_crosscorrs\n",
      "Evaluating version 14 of 15: leave_out_real_spatshift_across_scale_crosscorrs\n",
      "Getting predictions for voxels [200-299] of 14913\n"
     ]
    }
   ],
   "source": [
    "val_cc, val_r2 = validate_texture_model_varpart(best_params, models, val_voxel_single_trial_data, \\\n",
    "                                                             val_stim_single_trial_data, _texture_fn, \\\n",
    "                                                             sample_batch_size=sample_batch_size, \\\n",
    "                                                             voxel_batch_size=voxel_batch_size, debug=debug, dtype=fpX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13fe6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import numpy_utility, torch_utils\n",
    "\n",
    "\n",
    "def get_r2(actual,predicted):\n",
    "  \n",
    "    \"\"\"\n",
    "    This computes the coefficient of determination (R2).\n",
    "    For OLS, this is a good measure of variance explained. \n",
    "    Not necessarily true for ridge regression - can use signed correlation coefficient^2 instead.\n",
    "    With OLS & when train/test sets are identical, R2 = correlation coefficient^2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate r2 for this fit.\n",
    "    ssres = np.sum(np.power((predicted - actual),2));\n",
    "    sstot = np.sum(np.power((actual - np.mean(actual)),2));\n",
    "    r2 = 1-(ssres/sstot)\n",
    "    \n",
    "    return r2\n",
    " \n",
    "\n",
    "def validate_texture_model_varpart(best_params, prf_models, val_voxel_single_trial_data, val_stim_single_trial_data, _texture_fn, sample_batch_size=100, voxel_batch_size=100, debug=False, dtype=np.float32):\n",
    "    \n",
    "    \"\"\" \n",
    "    Evaluate trained model, leaving out a subset of features at a time.\n",
    "    \"\"\"\n",
    "    images = val_stim_single_trial_data\n",
    "    params = best_params\n",
    "    dtype = images.dtype.type\n",
    "    device = _texture_fn.device\n",
    "    \n",
    "    n_trials, n_voxels = len(images), len(params[0])\n",
    "    n_prfs = prf_models.shape[0]\n",
    "    n_features = params[1].shape[1]  \n",
    "\n",
    "    best_models, weights, bias, features_mt, features_st, best_model_inds, partial_version_names = params\n",
    "    \n",
    "    # val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "    n_voxels = np.shape(val_voxel_single_trial_data)[1]\n",
    "    n_features_total = _texture_fn.n_features_total\n",
    "    n_feature_types = len(_texture_fn.feature_types_include)\n",
    "    n_partial_versions = len(partial_version_names)\n",
    "    if n_partial_versions>1:\n",
    "        masks = np.concatenate([np.expand_dims(np.array(_texture_fn.feature_column_labels!=ff).astype('int'), axis=0) for ff in np.arange(-1,n_feature_types)], axis=0)\n",
    "    else:\n",
    "        masks = np.ones([1,n_features_total])\n",
    "    # \"partial versions\" will be listed as: [full model, leave out first set of features, leave out second set of features...]\n",
    "\n",
    "    masks = np.transpose(masks)\n",
    "\n",
    "    val_cc  = np.zeros(shape=(n_voxels, n_partial_versions), dtype=dtype)\n",
    "    val_r2 = np.zeros(shape=(n_voxels, n_partial_versions), dtype=dtype)\n",
    "\n",
    "    pred_models = np.full(fill_value=0, shape=(n_trials, n_features, n_prfs), dtype=dtype)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # First gather texture features for all pRFs.\n",
    "        \n",
    "        _texture_fn.clear_maps()\n",
    "        \n",
    "        for mm in range(n_prfs):\n",
    "            if mm>1 and debug:\n",
    "                break\n",
    "            print('Getting features for prf %d: [x,y,sigma] is [%.2f %.2f %.4f]'%(mm, prf_models[mm,0],  prf_models[mm,1],  prf_models[mm,2] ))\n",
    "            all_feat_concat, feature_info = _texture_fn(images,prf_models[mm,:])\n",
    "            \n",
    "            pred_models[:,:,mm] = torch_utils.get_value(all_feat_concat)\n",
    "        \n",
    "        _texture_fn.clear_maps()\n",
    "    \n",
    "    \n",
    "        vv=-1\n",
    "        ## Looping over voxels here in batches, will eventually go through all.\n",
    "        for rv, lv in numpy_utility.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "            vv=vv+1\n",
    "            print('Getting predictions for voxels [%d-%d] of %d'%(rv[0],rv[-1],n_voxels))\n",
    "\n",
    "            if vv>1 and debug:\n",
    "                break\n",
    "            \n",
    "            # Looping over versions of model w different features set to zero (variance partition)\n",
    "            for pp in range(n_partial_versions):\n",
    "                \n",
    "                print('Evaluating version %d of %d: %s'%(pp, n_partial_versions, partial_version_names[pp]))\n",
    "   \n",
    "                # [trials x features x voxels]\n",
    "                features_full = pred_models[:,:,best_model_inds[rv,pp]]\n",
    "                           \n",
    "                nonzero_inds = masks[:,pp]==1\n",
    "                \n",
    "                features = features_full[:,nonzero_inds,:]\n",
    "\n",
    "                # making sure to gather only the columns for features included in this partial model\n",
    "                _weights = torch_utils._to_torch(weights[rv,:,pp][:,nonzero_inds]) \n",
    "                \n",
    "                _bias = torch_utils._to_torch(bias[rv,pp])\n",
    "\n",
    "            \n",
    "                if features_mt is not None:\n",
    "                    _features_m = torch_utils._to_torch(features_mt[rv,:][:,nonzero_inds])\n",
    "                if features_st is not None:\n",
    "                    _features_s = torch_utils._to_torch(features_st[rv,:][:,nonzero_inds])\n",
    "                \n",
    "                pred_block = np.full(fill_value=0, shape=(n_trials, lv), dtype=dtype)\n",
    "                \n",
    "                \n",
    "                # Now looping over validation set trials in batches\n",
    "                for rt, lt in numpy_utility.iterate_range(0, n_trials, sample_batch_size):\n",
    "\n",
    "                    _features = torch_utils._to_torch(features[rt,:,:]) # trials x features x voxels\n",
    "                    if features_mt is not None:    \n",
    "                        # features_m is [nvoxels x nfeatures] - need [trials x features x voxels]\n",
    "                        _features = _features - torch.tile(torch.unsqueeze(_features_m, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "\n",
    "                    if features_st is not None:\n",
    "                        _features = _features/torch.tile(torch.unsqueeze(_features_s, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "                        _features[torch.isnan(_features)] = 0.0 # this applies in the pca case when last few columns of features are missing\n",
    "\n",
    "                    # features is [#samples, #features, #voxels] - swap dims to [#voxels, #samples, features]\n",
    "                    _features = torch.transpose(torch.transpose(_features, 0, 2), 1, 2)\n",
    "                    # weights is [#voxels, #features]\n",
    "                    # _r will be [#voxels, #samples, 1] - then [#samples, #voxels]\n",
    "\n",
    "                    _r = torch.squeeze(torch.bmm(_features, torch.unsqueeze(_weights, 2)), dim=2).t() \n",
    "\n",
    "                    if _bias is not None:\n",
    "                        _r = _r + torch.tile(torch.unsqueeze(_bias, 0), [_r.shape[0],1])\n",
    "\n",
    "                    pred_block[rt] = torch_utils.get_value(_r) \n",
    "\n",
    "                # Now for this batch of voxels and this partial version of the model, measure performance.\n",
    "#                 print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "                for vi in range(lv):   \n",
    "                    val_cc[rv[vi],pp] = np.corrcoef(val_voxel_single_trial_data[:,rv[vi]], pred_block[:,vi])[0,1]  \n",
    "                    val_r2[rv[vi],pp] = get_r2(val_voxel_single_trial_data[:,rv[vi]], pred_block[:,vi])\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "        \n",
    "    val_cc = np.nan_to_num(val_cc)\n",
    "    val_r2 = np.nan_to_num(val_r2) \n",
    "    \n",
    "    return val_cc, val_r2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "963964e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_texture_model_ridge(images, voxel_data, _texture_fn, models, lambdas, zscore=False, voxel_batch_size=100, \n",
    "                            holdout_size=100, shuffle=True, add_bias=False, debug=False, shuff_rnd_seed=0, \n",
    "                            device=None, do_varpart=True):\n",
    "   \n",
    "    \"\"\"\n",
    "    Solve for encoding model weights using ridge regression.\n",
    "    Inputs:\n",
    "        images: the training images, [n_trials x 1 x height x width]\n",
    "        voxel_data: the training voxel data, [n_trials x n_voxels]\n",
    "        _texture_fn: module that maps from images to texture model features\n",
    "        models: the list of possible pRFs to test, columns are [x, y, sigma]\n",
    "        lambdas: ridge lambda parameters to test\n",
    "        zscore: want to zscore each column of feature matrix before fitting?\n",
    "        voxel_batch_size: how many voxels to use at a time for model fitting\n",
    "        holdout_size: how many training trials to hold out for computing loss/lambda selection?\n",
    "        shuffle: do we shuffle training data order before holding trials out?\n",
    "        add_bias: add a column of ones to feature matrix, for an additive bias?\n",
    "        debug: want to run a shortened version of this, to test it?\n",
    "        shuff_rnd_seed: if we do shuffle training data (shuffle=True), what random seed to use? if zero, choose a new random seed in this code.\n",
    "    Outputs:\n",
    "        best_losses: loss value for each voxel (with best pRF and best lambda), eval on held out set\n",
    "        best_lambdas: best lambda for each voxel (chosen based on loss w held out set)\n",
    "        best_params: \n",
    "            [0] best pRF for each voxel [x,y,sigma]\n",
    "            [1] best weights for each voxel/feature\n",
    "            [2] if add_bias=True, best bias value for each voxel\n",
    "            [3] if zscore=True, the mean of each feature before z-score\n",
    "            [4] if zscore=True, the std of each feature before z-score\n",
    "            [5] index of the best pRF for each voxel (i.e. index of row in \"models\")\n",
    "        feature_info: describes types of features in texture model, see texture_feature_extractor in texture_statistics.py\n",
    "        \n",
    "    \"\"\"\n",
    "   \n",
    "    dtype = images.dtype.type\n",
    "    if device is None:\n",
    "        device=torch.device('cpu:0')\n",
    "#     device = next(_texture_fn.parameters()).device\n",
    "    trn_size = len(voxel_data) - holdout_size\n",
    "    assert trn_size>0, 'Training size needs to be greater than zero'\n",
    "    \n",
    "    print ('trn_size = %d (%.1f%%)' % (trn_size, float(trn_size)*100/len(voxel_data)))\n",
    "    print ('dtype = %s' % dtype)\n",
    "    print ('device = %s' % device)\n",
    "    print ('---------------------------------------')\n",
    "    \n",
    "    # First do shuffling of data and define set to hold out\n",
    "    n_trials = len(images)\n",
    "    n_prfs = len(models)\n",
    "    n_voxels = voxel_data.shape[1]\n",
    "    order = np.arange(len(voxel_data), dtype=int)\n",
    "    if shuffle:\n",
    "        if shuff_rnd_seed==0:\n",
    "            print('Computing a new random seed')\n",
    "            shuff_rnd_seed = int(time.strftime('%M%H%d', time.localtime()))\n",
    "        print('Seeding random number generator: seed is %d'%shuff_rnd_seed)\n",
    "        np.random.seed(shuff_rnd_seed)\n",
    "        np.random.shuffle(order)\n",
    "    images = images[order]\n",
    "    voxel_data = voxel_data[order]  \n",
    "    trn_data = voxel_data[:trn_size]\n",
    "    out_data = voxel_data[trn_size:]\n",
    "\n",
    "    n_features_total = _texture_fn.n_features_total\n",
    "    n_feature_types = len(_texture_fn.feature_types_include)\n",
    "    if do_varpart:\n",
    "        n_partial_versions = n_feature_types+1\n",
    "        partial_version_names = ['full_model']+['leave_out_%s'%ff for ff in _texture_fn.feature_types_include]\n",
    "        masks = np.concatenate([np.expand_dims(np.array(_texture_fn.feature_column_labels!=ff).astype('int'), axis=0) for ff in np.arange(-1,n_feature_types)], axis=0)\n",
    "    else:\n",
    "        n_partial_versions = 1;  \n",
    "        partial_version_names = ['full_model']\n",
    "        masks = np.ones([1,n_features_total])\n",
    "    # \"partial versions\" will be listed as: [full model, leave out first set of features, leave out second set of features...]\n",
    "\n",
    "    if add_bias:\n",
    "        masks = np.concatenate([masks, np.ones([masks.shape[0],1])], axis=1) # always include intercept \n",
    "    masks = np.transpose(masks)\n",
    "    # masks is [n_features_total (including intercept) x n_partial_versions]\n",
    "\n",
    "    # Create full model value buffers    \n",
    "    best_models = np.full(shape=(n_voxels,n_partial_versions), fill_value=-1, dtype=int)   \n",
    "    best_lambdas = np.full(shape=(n_voxels,n_partial_versions), fill_value=-1, dtype=int)\n",
    "    best_losses = np.full(fill_value=np.inf, shape=(n_voxels,n_partial_versions), dtype=dtype)\n",
    "    # creating a third dim here, listing the \"partial\" versions of the model (setting to zero a subset of features at a time)\n",
    "    best_w_params = np.zeros(shape=(n_voxels, n_features_total,n_partial_versions), dtype=dtype)\n",
    "\n",
    "    if add_bias:\n",
    "        best_w_params = np.concatenate([best_w_params, np.ones(shape=(n_voxels,1,n_partial_versions), dtype=dtype)], axis=1)\n",
    "\n",
    "    features_mean = None\n",
    "    features_std = None\n",
    "    if zscore:\n",
    "        features_mean = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "        features_std  = np.zeros(shape=(n_voxels, n_features_total), dtype=dtype)\n",
    "    \n",
    "    _texture_fn.clear_maps()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    vox_loop_time = 0\n",
    "    print ('')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Looping over models (here models are different spatial RF definitions)\n",
    "        for m,(x,y,sigma) in enumerate(models):\n",
    "            if debug and m>1:\n",
    "                break\n",
    "            print('\\nGetting features for prf %d: [x,y,sigma] is [%.2f %.2f %.4f]'%(m, models[m,0],  models[m,1],  models[m,2] ))\n",
    "            \n",
    "            t = time.time()   \n",
    "            \n",
    "            # Get features for the desired pRF, across all trn set image   \n",
    "        \n",
    "            all_feat_concat, feature_info = _texture_fn(images, [x,y,sigma])\n",
    "            \n",
    "            features = torch_utils.get_value(all_feat_concat)\n",
    "            \n",
    "            elapsed = time.time() - t\n",
    "        \n",
    "            if zscore:  \n",
    "                features_m = np.mean(features, axis=0, keepdims=True) #[:trn_size]\n",
    "                features_s = np.std(features, axis=0, keepdims=True) + 1e-6          \n",
    "                features -= features_m\n",
    "                features /= features_s    \n",
    "                \n",
    "            if add_bias:\n",
    "                features = np.concatenate([features, np.ones(shape=(len(features), 1), dtype=dtype)], axis=1)\n",
    "            \n",
    "            # separate design matrix into training/held out data (for lambda selection)\n",
    "            trn_features = features[:trn_size]\n",
    "            out_features = features[trn_size:]   \n",
    "\n",
    "            zero_columns = np.sum(trn_features[:,0:-1], axis=0)==0\n",
    "            if np.sum(zero_columns)>0:\n",
    "                print('n zero columns: %d'%np.sum(zero_columns))\n",
    "                for ff in range(len(feature_info[1])):\n",
    "                    if np.sum(zero_columns[feature_info[0]==ff])>0:\n",
    "                        print('   %d columns are %s'%(np.sum(zero_columns[feature_info[0]==ff]), feature_info[1][ff]))\n",
    "\n",
    "            # Looping over versions of model w different features set to zero (variance partition)\n",
    "            for pp in range(n_partial_versions):\n",
    "                \n",
    "                print('\\nFitting version %d of %d: %s, '%(pp, n_partial_versions, partial_version_names[pp]))\n",
    "\n",
    "                nonzero_inds = masks[:,pp]==1\n",
    "                best_w_tmp = best_w_params[:,nonzero_inds,pp] # chunk of the full weights matrix to work with for this partial model\n",
    "\n",
    "                # Send matrices to gpu\n",
    "                _xtrn = torch_utils._to_torch(trn_features[:,nonzero_inds], device=device)\n",
    "                _xout = torch_utils._to_torch(out_features[:,nonzero_inds], device=device)   \n",
    "\n",
    "                # Do part of the matrix math involved in ridge regression optimization out of the loop, \n",
    "                # because this part will be same for all the voxels.\n",
    "                _cof = _cofactor_fn_cpu(_xtrn, lambdas)\n",
    "\n",
    "                # Now looping over batches of voxels (only reason is because can't store all in memory at same time)\n",
    "                vox_start = time.time()\n",
    "                for rv,lv in numpy_utility.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "                    sys.stdout.write('\\rVoxels [%6d:%-6d] of %d' % (rv[0], rv[-1], n_voxels))\n",
    "\n",
    "                    # Send matrices to gpu\n",
    "                    _vtrn = torch_utils._to_torch(trn_data[:,rv], device=device)\n",
    "                    _vout = torch_utils._to_torch(out_data[:,rv], device=device)\n",
    "\n",
    "                    # Here is where optimization happens - relatively simple matrix math inside loss fn.\n",
    "                    _betas, _loss = _loss_fn(_cof, _vtrn, _xout, _vout) #   [#lambda, #feature, #voxel, ], [#lambda, #voxel]\n",
    "                    # Now have a set of weights (in betas) and a loss value for every voxel and every lambda. \n",
    "                    # goal is then to choose for each voxel, what is the best lambda and what weights went with that lambda.\n",
    "\n",
    "                    # first choose best lambda value and the loss that went with it.\n",
    "                    _values, _select = torch.min(_loss, dim=0)\n",
    "                    betas = torch_utils.get_value(_betas)\n",
    "                    values, select = torch_utils.get_value(_values), torch_utils.get_value(_select)\n",
    "\n",
    "                    # comparing this loss to the other models for each voxel (e.g. the other RF position/sizes)\n",
    "                    imp = values<best_losses[rv,pp]\n",
    "\n",
    "                    if np.sum(imp)>0:                    \n",
    "                        # for whichever voxels had improvement relative to previous models, save parameters now\n",
    "                        # this means we won't have to save all params for all models, just best.\n",
    "                        arv = np.array(rv)[imp]\n",
    "                        li = select[imp]\n",
    "                        best_lambdas[arv,pp] = li\n",
    "                        best_losses[arv,pp] = values[imp]\n",
    "                        best_models[arv,pp] = m\n",
    "                        if zscore:\n",
    "                            features_mean[arv] = features_m # broadcast over updated voxels\n",
    "                            features_std[arv]  = features_s\n",
    "                        # taking the weights associated with the best lambda value\n",
    "                        best_w_tmp[arv,:] = numpy_utility.select_along_axis(betas[:,:,imp], li, run_axis=2, choice_axis=0).T\n",
    "\n",
    "                best_w_params[:,nonzero_inds,pp] = best_w_tmp\n",
    "\n",
    "                vox_loop_time += (time.time() - vox_start)\n",
    "                elapsed = (time.time() - vox_start)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    # Print information about how fitting went...\n",
    "    total_time = time.time() - start_time\n",
    "    inv_time = total_time - vox_loop_time\n",
    "    return_params = [best_w_params[:,:n_features_total,:]]\n",
    "    if add_bias:\n",
    "        return_params += [best_w_params[:,-1,:]]\n",
    "    else: \n",
    "        return_params += [None,]\n",
    "    print ('\\n---------------------------------------')\n",
    "    print ('total time = %fs' % total_time)\n",
    "    print ('total throughput = %fs/voxel' % (total_time / n_voxels))\n",
    "    print ('voxel throughput = %fs/voxel' % (vox_loop_time / n_voxels))\n",
    "    print ('setup throughput = %fs/model' % (inv_time / n_prfs))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    best_params = [models[best_models],]+return_params+[features_mean, features_std]+[best_models]+[partial_version_names]\n",
    "    \n",
    "    _texture_fn.clear_maps()\n",
    "    \n",
    "    \n",
    "    return best_losses, best_lambdas, best_params, feature_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cde3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import numpy_utility, torch_utils\n",
    "\n",
    "def _cofactor_fn_cpu(_x, lambdas):\n",
    "    '''\n",
    "    Generating a matrix needed to solve ridge regression model for each lambda value.\n",
    "    Ridge regression (Tikhonov) solution is :\n",
    "    w = (X^T*X + I*lambda)^-1 * X^T * Y\n",
    "    This func will return (X^T*X + I*lambda)^-1 * X^T. \n",
    "    So once we have that, can just multiply by training data (Y) to get weights.\n",
    "    returned size is [nLambdas x nFeatures x nTrials]\n",
    "    This version makes sure that the torch inverse operation is done on the cpu, and in floating point-64 precision.\n",
    "    Otherwise get bad results for small lambda values. This seems to be a torch-specific bug.\n",
    "    \n",
    "    '''\n",
    "    device_orig = _x.device\n",
    "    type_orig = _x.dtype\n",
    "    # switch to this specific format which works with inverse\n",
    "    _x = _x.to('cpu').to(torch.float64)\n",
    "    _f = torch.stack([(torch.mm(torch.t(_x), _x) + torch.eye(_x.size()[1], device='cpu', dtype=torch.float64) * l).inverse() for l in lambdas], axis=0) \n",
    "    \n",
    "    # [#lambdas, #feature, #feature] \n",
    "    cof = torch.tensordot(_f, _x, dims=[[2],[1]]) # [#lambdas, #feature, #sample]\n",
    "    \n",
    "    # put back to whatever way it was before, so that we can continue with other operations as usual\n",
    "    return cof.to(device_orig).to(type_orig)\n",
    "\n",
    "\n",
    "\n",
    "def _loss_fn(_cofactor, _vtrn, _xout, _vout):\n",
    "    '''\n",
    "    Calculate loss given \"cofactor\" from cofactor_fn, training data, held-out design matrix, held out data.\n",
    "    returns weights (betas) based on equation\n",
    "    w = (X^T*X + I*lambda)^-1 * X^T * Y\n",
    "    also returns loss for these weights w the held out data. SSE is loss func here.\n",
    "    '''\n",
    "\n",
    "    _beta = torch.tensordot(_cofactor, _vtrn, dims=[[2], [0]]) # [#lambdas, #feature, #voxel]\n",
    "    _pred = torch.tensordot(_xout, _beta, dims=[[1],[1]]) # [#samples, #lambdas, #voxels]\n",
    "    _loss = torch.sum(torch.pow(_vout[:,None,:] - _pred, 2), dim=0) # [#lambdas, #voxels]\n",
    "    return _beta, _loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e64372b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import pyrtools as pt\n",
    "from utils import numpy_utility, torch_utils, texture_utils\n",
    "\n",
    "class texture_feature_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module to compute higher-order texture statistics of input images (e.g. Portilla & Simoncelli 2000, IJCV)\n",
    "    Statistics are computed within a specified region of space (a voxel's pRF)\n",
    "    Can specify different subsets of features to include (i.e. pixel-level stats, simple/complex cells, cross-correlations, auto-correlations)\n",
    "    Inputs to the forward pass are images and pRF parameters of interest [x,y,sigma]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,_fmaps_fn, sample_batch_size=100, feature_types_exclude=None, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "        \n",
    "        super(texture_feature_extractor, self).__init__()\n",
    "        \n",
    "        self.fmaps_fn = _fmaps_fn   \n",
    "        self.fmaps = None\n",
    "        self.n_sf = _fmaps_fn.pyr_height\n",
    "        self.n_ori =  _fmaps_fn.n_ori\n",
    "       \n",
    "        self.sample_batch_size = sample_batch_size       \n",
    "        self.n_prf_sd_out = n_prf_sd_out\n",
    "        self.aperture = aperture\n",
    "        self.device = device       \n",
    "       \n",
    "        self.update_feature_list(feature_types_exclude)\n",
    "       \n",
    "    def update_feature_list(self, feature_types_exclude):\n",
    "        \n",
    "        feature_types_all = ['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', \\\n",
    "            'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', \\\n",
    "            'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', \\\n",
    "            'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
    "        feature_type_dims = [6,16,16,10,1,\\\n",
    "                        272,73,25,\\\n",
    "                        24,24,48,96,\\\n",
    "                       10,20]\n",
    "\n",
    "        if feature_types_exclude is None:\n",
    "            feature_types_exclude = []\n",
    "        # decide which features to ignore, or use all features\n",
    "        self.feature_types_exclude = feature_types_exclude\n",
    "        \n",
    "        print(self.feature_types_exclude)    \n",
    "        # a few shorthands for ignoring sets of features at a time\n",
    "        if 'crosscorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'crosscorrs' in ff])\n",
    "        if 'autocorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'autocorrs' in ff])\n",
    "        if 'pixel' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['pixel_stats'])\n",
    "\n",
    "        self.feature_types_include  = [ff for ff in feature_types_all if not ff in feature_types_exclude]\n",
    "        if len(self.feature_types_include)==0:\n",
    "            raise ValueError('you have specified too many features to exclude, and now you have no features left! aborting.')\n",
    "            \n",
    "        feature_dims_include = [feature_type_dims[fi] for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in feature_types_exclude]\n",
    "        # how many features will be needed, in total?\n",
    "        self.n_features_total = np.sum(feature_dims_include)\n",
    "        \n",
    "        # numbers that define which feature types are in which column\n",
    "        self.feature_column_labels = np.squeeze(np.concatenate([fi*np.ones([1,feature_dims_include[fi]]) for fi in range(len(feature_dims_include))], axis=1).astype('int'))\n",
    "        assert(np.size(self.feature_column_labels)==self.n_features_total)\n",
    "\n",
    "    def get_maps(self, images):\n",
    "    \n",
    "        print('Running steerable pyramid feature extraction...')\n",
    "        print('Images array shape is:')\n",
    "        print(images.shape)\n",
    "        t = time.time()\n",
    "        fmaps = _fmaps_fn(images, to_torch=False, device=device)        \n",
    "        self.fmaps = fmaps\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    def clear_maps(self):\n",
    "        \n",
    "        print('Clearing steerable pyramid features from memory.')\n",
    "        self.fmaps = None\n",
    "        \n",
    "    def forward(self, images, prf_params):\n",
    "        \n",
    "        if self.fmaps is None:\n",
    "            self.get_maps(images)\n",
    "        else:\n",
    "            assert(images.shape[0]==self.fmaps[0][0].shape[0])\n",
    "        \n",
    "        if isinstance(prf_params, torch.Tensor):\n",
    "            prf_params = torch_utils.get_value(prf_params)\n",
    "        assert(np.size(prf_params)==3)\n",
    "        prf_params = np.squeeze(prf_params)\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = torch_utils.get_value(images)\n",
    "\n",
    "        print('Computing higher order correlations...')\n",
    "      \n",
    "        t = time.time()\n",
    "        pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "            magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "            magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "            real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs =  \\\n",
    "                    get_higher_order_features(self.fmaps, images, prf_params, sample_batch_size=self.sample_batch_size, n_prf_sd_out=self.n_prf_sd_out, aperture=self.aperture, device=self.device)\n",
    "        \n",
    "        \n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "        all_feat = OrderedDict({'pixel_stats':pixel_stats, 'mean_magnitudes':mean_magnitudes, 'mean_realparts':mean_realparts, \\\n",
    "                                'marginal_stats_lowpass_recons':marginal_stats_lowpass_recons, 'variance_highpass_resid':variance_highpass_resid, \\\n",
    "            'magnitude_feature_autocorrs':magnitude_feature_autocorrs, 'lowpass_recon_autocorrs':lowpass_recon_autocorrs, 'highpass_resid_autocorrs':highpass_resid_autocorrs, \\\n",
    "            'magnitude_within_scale_crosscorrs':magnitude_within_scale_crosscorrs, 'real_within_scale_crosscorrs':real_within_scale_crosscorrs, \\\n",
    "            'magnitude_across_scale_crosscorrs':magnitude_across_scale_crosscorrs, 'real_imag_across_scale_crosscorrs':real_imag_across_scale_crosscorrs, \\\n",
    "            'real_spatshift_within_scale_crosscorrs':real_spatshift_within_scale_crosscorrs, 'real_spatshift_across_scale_crosscorrs':real_spatshift_across_scale_crosscorrs})\n",
    "\n",
    "        feature_names_full = list(all_feat.keys())\n",
    "        feature_names = [fname for fname in feature_names_full if fname in self.feature_types_include]\n",
    "        assert(feature_names==self.feature_types_include) # double check here that the order is correct\n",
    "        \n",
    "        for ff, feature_name in enumerate(feature_names):   \n",
    "            assert(all_feat[feature_name] is not None)\n",
    "            if ff==0:\n",
    "                all_feat_concat = all_feat[feature_name]\n",
    "            else:               \n",
    "                all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "\n",
    "        assert(all_feat_concat.shape[1]==self.n_features_total)\n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        print('Feature types included are:')\n",
    "        print(feature_names)\n",
    "\n",
    "        if torch.any(torch.isnan(all_feat_concat)):\n",
    "            print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "        if torch.any(torch.sum(all_feat_concat, axis=0)==0):\n",
    "            print('\\nWARNING THERE ARE ZEROS IN FEATURES MATRIX\\n')\n",
    "            print('zeros for columns:')\n",
    "            print(np.where(torch.sum(all_feat_concat, axis=0)==0))\n",
    "        return all_feat_concat, [self.feature_column_labels, feature_names]\n",
    "    \n",
    "\n",
    "class steerable_pyramid_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module that utilizes steerable pyramid ( https://pyrtools.readthedocs.io/en/latest/) to extract features.\n",
    "    For a batch of input images, will return all the pyramid coefficients, as well as additional types of feature maps\n",
    "    (i.e. partially reconstructed lowpass images at several frequency levels, upsampled feature maps).\n",
    "    These are used by 'get_higher_order_features' to extract various textural features of the image.\n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pyr_height=4, n_ori=8):\n",
    "        \n",
    "        super(steerable_pyramid_extractor, self).__init__()       \n",
    "        self.n_ori = n_ori\n",
    "        self.pyr_height = pyr_height # how many spatial frequencies?\n",
    "        self.pyr = None\n",
    "        \n",
    "    def forward(self, image_batch, to_torch=False, device=None):\n",
    "        \n",
    "        batch_size = image_batch.shape[0]\n",
    "        t  = time.time()\n",
    "        for ii in range(batch_size):\n",
    "            \n",
    "            # Call the pyramid generation code here, get all features for this image.\n",
    "            pyr = pt.pyramids.SteerablePyramidFreq(image_batch[ii,0,:,:], is_complex=True, height = self.pyr_height, order = self.n_ori-1)\n",
    "            self.pyr = pyr # storing the most recently generated pyramid, in case we need its properties later\n",
    "            \n",
    "            if ii==0:  \n",
    "                \n",
    "                # Initialize all the feature maps we want to store...\n",
    "                fmaps_complex = []\n",
    "                fmaps_coarser_upsampled = []\n",
    "               \n",
    "                # Will generate several low-pass filtered representations of the image - will use these as additional feature maps for \n",
    "                # computing autocorrelations and marginal statistics.\n",
    "                fmaps_lowpass_recon = []\n",
    "                fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "               \n",
    "                # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "                sf_reverse  = self.pyr_height  # need to go backward because pyr comes out in the opposite order from what i want\n",
    "                for sf in range(self.pyr_height):\n",
    "                    sf_reverse -= 1\n",
    "                    fmaps_complex.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))\n",
    "                    \n",
    "                    # Initialize \"parent\" representations for this level (map from the next coarsest resolution, upsampled to the finer scale)\n",
    "                    # To be used for cross-scale comparisons.\n",
    "                    if sf==0:\n",
    "                        # this will be the lowpass residual (non-oriented).\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, 1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))                     \n",
    "                    else:\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))     \n",
    "                        \n",
    "                    fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                         pyr.pyr_coeffs[sf_reverse,0].shape[1])))\n",
    "\n",
    "                fmaps_resid = []    \n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_highpass'].shape[0],pyr.pyr_coeffs['residual_highpass'].shape[1])))\n",
    "\n",
    "            # First get lowpass filtered representation of the image\n",
    "            lowpass_recon = pyr.recon_pyr(levels='residual_lowpass', bands='all', twidth=1) \n",
    "            scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size['residual_lowpass'][0]\n",
    "            lowpass_recon = texture_utils.shrink(lowpass_recon, scale_by)*scale_by**2\n",
    "            fmaps_lowpass_recon[0][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "            # Get the \"parent\" for lowest SF level (upsample the residual lowpass)\n",
    "#             print(pyr.pyr_coeffs['residual_lowpass'].shape)\n",
    "            upsampled = texture_utils.expand(pyr.pyr_coeffs['residual_lowpass'], factor=2)/2**2\n",
    "#             print(upsampled.shape)\n",
    "            fmaps_coarser_upsampled[0][ii,0,:,:] = upsampled\n",
    "            \n",
    "            # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "            sf_reverse  = self.pyr_height # need to go backward because pyr comes out in the opposite order from what i want\n",
    "            for sf in range(self.pyr_height):\n",
    "                sf_reverse -= 1\n",
    "                for oo in range(self.n_ori):     \n",
    "\n",
    "                    # These are the main feature maps of the pyramid - one feature map per scale per orientation band.\n",
    "                    # Complex number, can take the magnitude or real/imaginary part to simulate complex or simple cell-type responses.\n",
    "                    fmaps_complex[sf][ii,oo,:,:] = pyr.pyr_coeffs[(sf_reverse,oo)]\n",
    "                    \n",
    "                    if sf<self.pyr_height-1:\n",
    "                        # Store this as a \"parent\" representation, will be used for the next most fine SF level (i.e. sf+1)\n",
    "                        upsampled = texture_utils.expand(pyr.pyr_coeffs[(sf_reverse,oo)], factor=2)/2**2\n",
    "                        # Double the phase (angle of the complex number); note this doesn't affect the magnitude. \n",
    "                        phase_doubled = texture_utils.double_phase(upsampled)\n",
    "                        fmaps_coarser_upsampled[sf+1][ii,oo,:,:] = phase_doubled\n",
    "                   \n",
    "                        \n",
    "                # Get the bandpass filtered representation for this scale\n",
    "                bandpass_image = np.real(pyr.recon_pyr(levels=sf_reverse, bands='all', twidth=1))\n",
    "                scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size[(sf_reverse,0)][0]\n",
    "                bandpass_image = texture_utils.shrink(bandpass_image, factor=scale_by)*scale_by**2\n",
    "                \n",
    "                # Add it onto the lowpass_recon (gets modified every loop iteration)\n",
    "                lowpass_recon = texture_utils.expand(lowpass_recon, factor=2)/2**2\n",
    "                lowpass_recon = lowpass_recon + bandpass_image\n",
    "                fmaps_lowpass_recon[sf+1][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "\n",
    "            # Grab residual feature maps, the lowest and highest levels of the pyramid\n",
    "            fmaps_resid[0][ii,0,:,:] = pyr.pyr_coeffs['residual_lowpass']\n",
    "            fmaps_resid[1][ii,0,:,:] = pyr.pyr_coeffs['residual_highpass']\n",
    "            \n",
    "            \n",
    "        elapsed = time.time() - t\n",
    "#         print('time elapsed: %.5f s'%elapsed)\n",
    "\n",
    "        if to_torch:            \n",
    "            fmaps_complex = [torch.from_numpy(fm).to(device) for fm in fmaps_complex]            \n",
    "            fmaps_resid = [torch_utils._to_torch(fm, device=device) for fm in fmaps_resid]\n",
    "            fmaps_lowpass_recon = [torch_utils._to_torch(fm, device=device) for fm in fmaps_lowpass_recon]                      \n",
    "            fmaps_coarser_upsampled = [torch.from_numpy(fm).to(device) for fm in fmaps_coarser_upsampled]\n",
    "\n",
    "        return fmaps_complex, fmaps_resid, fmaps_lowpass_recon, fmaps_coarser_upsampled\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "def get_higher_order_features(fmaps, images, prf_params, sample_batch_size=20, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute higher order texture features for a batch of images.\n",
    "    Input the module that defines steerable pyramid (i.e. 'steerable_pyramid_extractor'), and desired prf parameters.\n",
    "    Returns arrays of each higher order feature.  \n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "\n",
    "    fmaps_complex_all, fmaps_resid_all, fmaps_lowpass_recon_all, fmaps_coarser_upsampled_all = fmaps\n",
    "   \n",
    "    n_trials = fmaps_complex_all[0].shape[0]\n",
    "    x,y,sigma = prf_params\n",
    "\n",
    "    n_sf = _fmaps_fn.pyr_height\n",
    "    n_ori = _fmaps_fn.n_ori\n",
    "        \n",
    "    # all pairs of different orientation channels.\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "\n",
    "    # mean, variance, skew, kurtosis, min, max\n",
    "    pixel_stats = torch.zeros((n_trials,6), device=device)\n",
    "\n",
    "    # Mean magnitude each scale/orientation, within the prf.\n",
    "    mean_magnitudes = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "    mean_realparts = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "\n",
    "    # Store the skew and kurtosis of the lowpass reconstructions at each scale\n",
    "    marginal_stats_lowpass_recons = torch.zeros((n_trials, n_sf+1, 2), device=device)\n",
    "\n",
    "    # Variance of the highpass residual\n",
    "    variance_highpass_resid = torch.zeros((n_trials, 1), device=device)\n",
    "\n",
    "    # how many unique autocorrelation values will we get out for each feature map? These will be pre-defined, same for every pRF.\n",
    "    # but different for different scales of feature maps.\n",
    "    # note also that for bigger prfs, there will potentially be more pixels that contribute to the autocorrelation computation - \n",
    "    # but a fixed portion of the matrix is returned.\n",
    "    autocorr_output_pix=np.array([3,3,5,7,7])\n",
    "    n_autocorr_vals = ((autocorr_output_pix**2+1)/2).astype('int')\n",
    "    max_autocorr_vals = np.max(n_autocorr_vals)\n",
    "    \n",
    "    # Spatial autocorrelation of the magnitude of spectral coefficients, within each scale and orientation.\n",
    "    magnitude_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, max_autocorr_vals], device=device) # this is ace in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the partially-reconstructed lowpass image representation at each scale\n",
    "    lowpass_recon_autocorrs = torch.zeros([n_trials, n_sf+1, max_autocorr_vals], device=device) # this is acr in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the highpass residual\n",
    "    highpass_resid_autocorrs = torch.zeros([n_trials, 1, max_autocorr_vals], device=device)\n",
    "\n",
    "    # Within scale correlations of feature maps: compare feature map magnitudes for different orientations.\n",
    "    magnitude_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is C0 in the matlab code\n",
    "    # Using the real parts.\n",
    "    real_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is Cr0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations of feature maps: always comparing each scale to an up-sampled version of the scale coarser than it.\n",
    "    magnitude_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_ori, n_ori], device=device) # this is Cx0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations, using the real and imaginary parts separately. The phase (angle) of the coarser map is doubled before computing these.\n",
    "    real_imag_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, 2, n_ori, n_ori], device=device) # this is Crx0 in the matlab code\n",
    "\n",
    "    # These are comparisons with spatially shifted versions of the lowpass residual. Not sure we need this...\n",
    "    n_spatshifts = 5;\n",
    "    real_spatshift_within_scale_crosscorrs = torch.zeros([n_trials, 1, n_spatshifts, n_spatshifts], device=device)# this is Cr0 in the matlab code\n",
    "    real_spatshift_across_scale_crosscorrs = torch.zeros([n_trials, 1, n_ori, n_spatshifts], device=device)  # this is Crx0 in the matlab code\n",
    "\n",
    "    # Looping over batches of trials to compute everything of interest.\n",
    "    bb=-1\n",
    "    for batch_inds, batch_size_actual in numpy_utility.iterate_range(0, n_trials, sample_batch_size):\n",
    "        bb=bb+1\n",
    "\n",
    "        fmaps_complex = [torch.from_numpy(fmaps_complex_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_complex_all))]\n",
    "        fmaps_resid = [torch.from_numpy(fmaps_resid_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_resid_all))]\n",
    "        fmaps_lowpass_recon = [torch.from_numpy(fmaps_lowpass_recon_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_lowpass_recon_all))]\n",
    "        fmaps_coarser_upsampled = [torch.from_numpy(fmaps_coarser_upsampled_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_coarser_upsampled_all))]\n",
    "\n",
    "        if bb==0:\n",
    "            npix_each_scale = [_fmaps_fn.pyr.pyr_size[(sc,0)][0] for sc in range(n_sf)]\n",
    "            npix_each_scale.append(_fmaps_fn.pyr.pyr_size['residual_lowpass'][0])\n",
    "            npix_each_scale.reverse()\n",
    "            \n",
    "        # First working with the finest scale (original image)\n",
    "        n_pix = npix_each_scale[-1]      \n",
    "        g = numpy_utility.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[-1])\n",
    "\n",
    "        # Gather pixel-wise statistics here \n",
    "        wmean, wvar, wskew, wkurt = texture_utils.get_weighted_pixel_features(images[batch_inds], spatial_weights, device=device)\n",
    "        pixel_stats[batch_inds,0] = torch.squeeze(wmean)\n",
    "        pixel_stats[batch_inds,1] = torch.squeeze(wvar)\n",
    "        pixel_stats[batch_inds,2] = torch.squeeze(wskew)\n",
    "        pixel_stats[batch_inds,3] = torch.squeeze(wkurt)\n",
    "        pixel_stats[batch_inds,4] = torch_utils._to_torch(np.squeeze(np.min(np.min(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "        pixel_stats[batch_inds,5] = torch_utils._to_torch(np.squeeze(np.max(np.max(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "\n",
    "        # Autocorrs of the highpass residual\n",
    "        highpass_resid = fmaps_resid[1]\n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(highpass_resid, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[-1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        highpass_resid_autocorrs[batch_inds,0,0:n_autocorr_vals[-1]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[-1]])\n",
    "\n",
    "        # Variance of the highpass residual\n",
    "        m, wvar, s, k = texture_utils.get_weighted_pixel_features(highpass_resid, spatial_weights, device=device)\n",
    "        variance_highpass_resid[batch_inds,0] = torch.squeeze(wvar)\n",
    "\n",
    "        # Next work with the low-pass reconstruction (most coarse scale, smallest npix)\n",
    "        n_pix = npix_each_scale[0]       \n",
    "        g = numpy_utility.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[0])\n",
    "\n",
    "        lowpass_rec = fmaps_lowpass_recon[0]\n",
    "\n",
    "        # Marginal stats of low-pass reconstruction\n",
    "        m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_rec, spatial_weights, device=device)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,0] = torch.squeeze(wskew)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,1] = torch.squeeze(wkurt)\n",
    "\n",
    "        # Autocorrs of low-pass reconstruction \n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_rec, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[0], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        lowpass_recon_autocorrs[batch_inds,0,0:n_autocorr_vals[0]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[0]])\n",
    "\n",
    "        # Looping over spatial frequency/scale\n",
    "        # Loop goes low SF (smallest npix) to higher SF (largest npix)\n",
    "        for ff in range(n_sf):\n",
    "         \n",
    "            # Scale specific things - get the prf at this resolution of interest    \n",
    "            n_pix = npix_each_scale[ff+1]           \n",
    "            g = numpy_utility.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "            spatial_weights = g[2][0]\n",
    "            patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[1+ff])\n",
    "\n",
    "            # Get the low-pass reconstruction at this scale\n",
    "            lowpass_summed = fmaps_lowpass_recon[ff+1]  # this is summed over this scale band and those below it\n",
    "            m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_summed, spatial_weights, device=device)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,0] = torch.squeeze(wskew)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,1] = torch.squeeze(wkurt)\n",
    "\n",
    "            # Autocorrelations of low-pass reconstruction (at this scale)\n",
    "            auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_summed, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "            lowpass_recon_autocorrs[batch_inds,ff+1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "            # Loop over orientation channels\n",
    "            xx=-1\n",
    "            for oo1 in range(n_ori):       \n",
    "\n",
    "                # Magnitude of the complex coefficients; complex cell-like responses\n",
    "                mag1 = torch.abs(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                # The mean magnitudes here are basically second-order spectral statistics, within the specified spatial region defined by weights\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(mag1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_magnitudes[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "                \n",
    "                mag1 = mag1 - torch.tile(torch.mean(torch.mean(mag1, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                # Real parts of the complex coefficients; simple cell-like responses\n",
    "                real1 = torch.real(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()    \n",
    "                \n",
    "                # Average of the real parts within the specified spatial region\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(real1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_realparts[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "\n",
    "                # Complex cell autocorrelation (correlation w spatially shifted versions of itself)     \n",
    "                auto_corr = texture_utils.weighted_auto_corr_2d(mag1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "                magnitude_feature_autocorrs[batch_inds,ff,oo1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "                # Within-scale correlations - comparing resp at orient==oo1 to responses at all other orientations, same scale.\n",
    "                for oo2 in np.arange(oo1+1, n_ori):            \n",
    "                    xx = xx+1 \n",
    "                    assert(oo1==ori_pairs[xx,0] and oo2==ori_pairs[xx,1])\n",
    "\n",
    "                    # Magnitude at the other orientation (oo2)\n",
    "                    mag2 = torch.abs(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()      \n",
    "                    mag2 = mag2 - torch.tile(torch.mean(torch.mean(mag2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                    # Correlate the magnitude feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    magnitude_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                    # Real part at the other orientation (oo2)\n",
    "                    real2 = torch.real(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()                     \n",
    "\n",
    "                    # Correlate the real feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(real1, real2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    real_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                # Cross-scale correlations - for these we care about same ori to same ori, so looping over all orientations.\n",
    "                # Going to compare coefficients at the current scale to those at a coarser scale (ff-1)\n",
    "                # If we're at first scale (ff=0), then will use a different method.\n",
    "                if ff>0:\n",
    "\n",
    "                    for oo2 in range(n_ori):\n",
    "\n",
    "                        # Get magnitude of coefficients for neighboring (coarser) scale                        \n",
    "                        mag_coarser2 = torch.abs(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        mag_coarser2 = mag_coarser2 - torch.tile(torch.mean(torch.mean(mag_coarser2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                        # Correlate this with the finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)            \n",
    "                        magnitude_across_scale_crosscorrs[batch_inds,ff-1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        # Get the real and imaginary parts at coarser scale\n",
    "                        real_coarser2 = torch.real(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        imag_coarser2 = torch.imag(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                        # Correlate each of these with real part at finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,0,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, imag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # instead of different orientations for the \"parent\" level here, have spatially shifted versions.\n",
    "                    real_coarser = torch.real(fmaps_coarser_upsampled[ff][:,0,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                    shifts = [[0,0],[1,3],[-1,3],[1,2],[-1,2]]\n",
    "                    for si1, shift1 in enumerate(shifts):\n",
    "\n",
    "                        ss,dd = shift1\n",
    "                        real_coarser_shifted1 = torch.roll(real_coarser, shifts=ss, dims=dd)               \n",
    "                        # Real part at the finer scale compared to spatially shifted at the coarser scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser_shifted1, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_spatshift_across_scale_crosscorrs[batch_inds,ff,oo1,si1] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        for si2 in np.arange(si1+1, n_spatshifts):\n",
    "\n",
    "                            ss,dd = shifts[si2]\n",
    "                            real_coarser_shifted2 = torch.roll(real_coarser, shifts=ss, dims=dd) \n",
    "                            # Real parts at same scale, comparing spatially shifted.\n",
    "                            cross_corr = texture_utils.weighted_cross_corr_2d(real_coarser_shifted1, real_coarser_shifted2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                            real_spatshift_within_scale_crosscorrs[batch_inds,ff,si1,si2] = torch.squeeze(cross_corr)\n",
    "\n",
    "            \n",
    "    # Reshape everything to [ntrials x nfeatures]\n",
    "    \n",
    "    mean_magnitudes = torch.reshape(mean_magnitudes, [n_trials, -1])\n",
    "    mean_realparts = torch.reshape(mean_realparts, [n_trials, -1])\n",
    "    marginal_stats_lowpass_recons = torch.reshape(marginal_stats_lowpass_recons, [n_trials, -1])\n",
    "    variance_highpass_resid =torch.reshape(variance_highpass_resid, [n_trials, -1])\n",
    "\n",
    "    magnitude_feature_autocorrs = torch.reshape(magnitude_feature_autocorrs, [n_trials, -1])\n",
    "    # take out the zero columns, which happen because of different size autocorr outputs.\n",
    "    magnitude_feature_autocorrs = magnitude_feature_autocorrs[:,torch.sum(magnitude_feature_autocorrs, axis=0)!=0]\n",
    "    assert(magnitude_feature_autocorrs.shape[1]==np.sum(n_autocorr_vals[1:]*n_ori))\n",
    "\n",
    "    lowpass_recon_autocorrs = torch.reshape(lowpass_recon_autocorrs, [n_trials, -1])\n",
    "    lowpass_recon_autocorrs = lowpass_recon_autocorrs[:,torch.sum(lowpass_recon_autocorrs, axis=0)!=0]\n",
    "    assert(lowpass_recon_autocorrs.shape[1]==np.sum(n_autocorr_vals))\n",
    "\n",
    "    highpass_resid_autocorrs = torch.reshape(highpass_resid_autocorrs, [n_trials, -1])\n",
    "\n",
    "    magnitude_within_scale_crosscorrs = torch.reshape(magnitude_within_scale_crosscorrs, [n_trials, -1])\n",
    "    real_within_scale_crosscorrs = torch.reshape(real_within_scale_crosscorrs, [n_trials, -1])\n",
    "    magnitude_across_scale_crosscorrs = torch.reshape(magnitude_across_scale_crosscorrs, [n_trials, -1])\n",
    "    real_imag_across_scale_crosscorrs = torch.reshape(real_imag_across_scale_crosscorrs, [n_trials, -1])\n",
    "    real_spatshift_within_scale_crosscorrs = torch.reshape(real_spatshift_within_scale_crosscorrs, [n_trials, -1])\n",
    "    \n",
    "    real_spatshift_within_scale_crosscorrs = real_spatshift_within_scale_crosscorrs[:,torch.sum(real_spatshift_within_scale_crosscorrs, axis=0)!=0]\n",
    "    assert(real_spatshift_within_scale_crosscorrs.shape[1]==np.sum(np.arange(1,n_spatshifts)))\n",
    "\n",
    "    real_spatshift_across_scale_crosscorrs = torch.reshape(real_spatshift_across_scale_crosscorrs, [n_trials, -1])\n",
    "\n",
    "        \n",
    "    return pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "            magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "            magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "            real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
