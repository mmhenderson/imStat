{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7135a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic modules\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "import argparse\n",
    "import skimage.transform\n",
    "\n",
    "# import custom modules\n",
    "root_dir   = '/user_data/mmhender/imStat/'\n",
    "sys.path.append(os.path.join(root_dir, 'code'))\n",
    "from model_src import fwrf_fit as fwrf_fit\n",
    "from model_src import fwrf_predict as fwrf_predict\n",
    "from model_src import texture_statistics_gabor, texture_statistics_pyramid, bdcn_features\n",
    "from utils import nsd_utils, roi_utils\n",
    "from model_fitting import initialize_fitting\n",
    "\n",
    "\n",
    "bdcn_path = '/user_data/mmhender/toolboxes/BDCN/'\n",
    "\n",
    "\n",
    "fpX = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95eb5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a):\n",
    "    \n",
    "    return a+2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341a99ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = fun\n",
    "thing(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36598b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject=1\n",
    "volume_space = True\n",
    "\n",
    "ridge=True\n",
    "\n",
    "shuffle_images=False\n",
    "random_images=False\n",
    "random_voxel_data=False\n",
    "\n",
    "sample_batch_size=100\n",
    "voxel_batch_size=100\n",
    "zscore_features=True\n",
    "\n",
    "up_to_sess=1\n",
    "debug=True\n",
    "shuff_rnd_seed=0\n",
    "\n",
    "n_ori=4\n",
    "n_sf=4\n",
    "nonlin_fn=False\n",
    "\n",
    "# fitting_type='pyramid_texture'\n",
    "\n",
    "do_fitting=True\n",
    "do_val=True\n",
    "do_varpart=True\n",
    "\n",
    "date_str=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0cbbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce RTX 2080 Ti\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = initialize_fitting.init_cuda()\n",
    "model_name, feature_types_exclude = initialize_fitting.get_pyramid_model_name(ridge, n_ori, n_sf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f57db08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Volume space: ROI defs are located at: /lab_data/tarrlab/common/datasets/NSD/nsddata/ppdata/subj01/func1pt8mm/roi\n",
      "\n",
      "3794 voxels of overlap between kastner and prf definitions, using prf defs\n",
      "unique values in retino labels:\n",
      "[-1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.\n",
      " 17. 18. 19. 20. 21. 22. 23. 24. 25.]\n",
      "0 voxels of overlap between face and place definitions, using place defs\n",
      "unique values in categ labels:\n",
      "[-1.  0. 26. 27. 28. 30. 31. 32. 33.]\n",
      "1535 voxels are defined (differently) in both retinotopic areas and category areas\n",
      "\n",
      "14913 voxels are defined across all areas, and will be used for analysis\n",
      "\n",
      "Loading numerical label/name mappings for all ROIs:\n",
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4']\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "['V1v', 'V1d', 'V2v', 'V2d', 'V3v', 'V3d', 'hV4', 'VO1', 'VO2', 'PHC1', 'PHC2', 'TO2', 'TO1', 'LO2', 'LO1', 'V3B', 'V3A', 'IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5', 'SPL1', 'FEF']\n",
      "[1, 2, 3, 4, 5]\n",
      "['OFA', 'FFA-1', 'FFA-2', 'mTL-faces', 'aTL-faces']\n",
      "[1, 2, 3]\n",
      "['OPA', 'PPA', 'RSC']\n",
      "\n",
      "Sizes of all defined ROIs in this subject:\n",
      "Region V1 has 2392 voxels. Includes subregions:\n",
      "['V1v', 'V1d']\n",
      "Region V2 has 2096 voxels. Includes subregions:\n",
      "['V2v', 'V2d']\n",
      "Region V3 has 1674 voxels. Includes subregions:\n",
      "['V3v', 'V3d']\n",
      "Region hV4 has 721 voxels. Includes subregions:\n",
      "['hV4']\n",
      "Region VO1-2 has 482 voxels. Includes subregions:\n",
      "['VO1', 'VO2']\n",
      "Region PHC1-2 has 382 voxels. Includes subregions:\n",
      "['PHC1', 'PHC2']\n",
      "Region LO1-2 has 488 voxels. Includes subregions:\n",
      "['LO2', 'LO1']\n",
      "Region TO1-2 has 339 voxels. Includes subregions:\n",
      "['TO2', 'TO1']\n",
      "Region V3ab has 965 voxels. Includes subregions:\n",
      "['V3B', 'V3A']\n",
      "Region IPS0-5 has 2155 voxels. Includes subregions:\n",
      "['IPS0', 'IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5']\n",
      "Region SPL1 has 164 voxels. Includes subregions:\n",
      "['SPL1']\n",
      "Region FEF has 72 voxels. Includes subregions:\n",
      "['FEF']\n",
      "\n",
      "\n",
      "Region OFA has 355 voxels.\n",
      "Region FFA-1 has 484 voxels.\n",
      "Region FFA-2 has 310 voxels.\n",
      "Region mTL-faces has 0 voxels.\n",
      "Region aTL-faces has 159 voxels.\n",
      "Region OPA has 1611 voxels.\n",
      "Region PPA has 1033 voxels.\n",
      "Region RSC has 566 voxels.\n",
      "\n",
      "Loading images for subject 1\n",
      "\n",
      "image data size: (10000, 3, 227, 227) , dtype: uint8 , value range: 0 255\n",
      "Loading data for sessions:\n",
      "[1]\n",
      "Data is located in: /lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR...\n",
      "Loading from /lab_data/tarrlab/common/datasets/NSD/nsddata_betas/ppdata/subj01/func1pt8mm/betas_fithrf_GLMdenoise_RR/betas_session01.nii.gz...\n",
      "Raw data:\n",
      "float64 -32768.0 32767.0 (750, 81, 104, 83)\n",
      "Adjusted data (divided by 300):\n",
      "float32 -109.22667 109.223335 (750, 699192)\n",
      "z-scoring beta weights within this session...\n",
      "mean = 1.237, sigma = 1.391\n",
      "\n",
      "Size of full data set [nTrials x nVoxels] is:\n",
      "(750, 14913)\n"
     ]
    }
   ],
   "source": [
    "# decide what voxels to use  \n",
    "voxel_mask, voxel_index, voxel_roi, voxel_ncsnr, brain_nii_shape = roi_utils.get_voxel_roi_info(subject, volume_space)\n",
    "\n",
    "sessions = np.arange(0,up_to_sess)\n",
    "zscore_betas_within_sess = True\n",
    "# get all data and corresponding images, in two splits. always fixed set that gets left out\n",
    "trn_stim_data, trn_voxel_data, val_stim_data, val_voxel_data, image_order = nsd_utils.get_data_splits(subject, sessions=sessions, \\\n",
    "                                                                 voxel_mask=voxel_mask, volume_space=volume_space, \\\n",
    "                                                                  zscore_betas_within_sess=zscore_betas_within_sess, \\\n",
    "                                                                  shuffle_images=shuffle_images, random_images=random_images, \\\n",
    "                                                                                     random_voxel_data=random_voxel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048472b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a multiple of 8\n",
    "process_at_size=240\n",
    "trn_stim_data = skimage.transform.resize(trn_stim_data, output_shape=(trn_stim_data.shape[0],1,process_at_size, process_at_size))\n",
    "val_stim_data = skimage.transform.resize(val_stim_data, output_shape=(val_stim_data.shape[0],1,process_at_size, process_at_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d86549d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44db6f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n",
      "\n",
      "Possible lambda values are:\n",
      "[1.0000000e+00 4.2169652e+00 1.7782795e+01 7.4989418e+01 3.1622775e+02\n",
      " 1.3335215e+03 5.6234131e+03 2.3713736e+04 1.0000000e+05]\n"
     ]
    }
   ],
   "source": [
    "# Set up the pRFs to test\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "# More params for fitting\n",
    "holdout_size, lambdas = initialize_fitting.get_fitting_pars(trn_voxel_data, zscore_features, ridge=ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "c69ecc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Set up the pyramid\n",
    "_fmaps_fn = texture_statistics_pyramid.steerable_pyramid_extractor(pyr_height=n_sf, n_ori = n_ori)\n",
    "# Params for the spatial aspect of the model (possible pRFs)\n",
    "#     aperture_rf_range=0.8 # using smaller range here because not sure what to do with RFs at edges...\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "do_varpart=True\n",
    "group_all_hl_feats=True\n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "n_prf_sd_out=2\n",
    "_texture_fn = texture_feature_extractor(_fmaps_fn,sample_batch_size=sample_batch_size, feature_types_exclude=feature_types_exclude, \\\n",
    "                                        n_prf_sd_out=n_prf_sd_out, aperture=aperture, \n",
    "                                        do_varpart = do_varpart, group_all_hl_feats = group_all_hl_feats, device=device)\n",
    "# _texture_fn_old = texture_statistics_pyramid.texture_feature_extractor(_fmaps_fn,sample_batch_size=sample_batch_size, \\\n",
    "#                                                                        feature_types_exclude=feature_types_exclude, \\\n",
    "#                                                                        n_prf_sd_out=n_prf_sd_out, aperture=aperture, device=device)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "bcef309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/mmhender/toolboxes/BDCN/bdcn.py:232: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(param, 0.080)\n"
     ]
    }
   ],
   "source": [
    "# Set up the contour model\n",
    "map_ind = -1\n",
    "n_prf_sd_out=2\n",
    "mult_patch_by_prf=True\n",
    "downsample_factor=1\n",
    "do_nms=False\n",
    "do_pca=True\n",
    "max_pc_to_retain = 400\n",
    "min_pct_var=99\n",
    "# Set up the contour feature extractor\n",
    "pretrained_model_file = os.path.join(bdcn_path,'params','bdcn_pretrained_on_bsds500.pth')\n",
    "_contour_fn = bdcn_feature_extractor(pretrained_model_file, device, aperture, n_prf_sd_out, \\\n",
    "                                           batch_size=10, map_ind=map_ind, mult_patch_by_prf=mult_patch_by_prf,                                     \n",
    "                                            downsample_factor = downsample_factor, do_nms = do_nms, \\\n",
    "                                            do_pca = do_pca, min_pct_var = min_pct_var, max_pc_to_retain = max_pc_to_retain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "725f04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _feature_extractor = combined_feature_extractor(modules=[_texture_fn, _contour_fn], module_names=['bdcn1','texture'], \\\n",
    "#                                                do_varpart = True)\n",
    "_feature_extractor = combined_feature_extractor(modules=[_contour_fn, _texture_fn], module_names=['bdcn', 'texture'], \\\n",
    "                                               do_varpart = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "9d6b2e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing for fitting: finding the size of feature matrix for each candidate prf\n",
      "Initializing arrays for PCA params\n",
      "Clearing BDCN contour features from memory.\n",
      "Clearing steerable pyramid features from memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmhender/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:80: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "images = trn_stim_data\n",
    "image_size = images.shape[2:4]\n",
    "dtype = images.dtype.type\n",
    "_feature_extractor.init_for_fitting(image_size, models, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "7a743b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, names = _feature_extractor.get_partial_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "daeb9272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_magnitudes', 'all_other_texture_feats']"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_texture_fn.feature_group_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "75af7e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BDCN contour feature extraction...\n",
      "Images array shape is:\n",
      "(688, 1, 240, 240)\n",
      "Final array shape is:\n",
      "torch.Size([688, 1, 240, 240])\n",
      "time elapsed = 7.06344\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.55, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 9]\n",
      "[min max] of first image patch is:\n",
      "[tensor(8.4639e-06, device='cuda:0'), tensor(0.2264, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(688, 72)\n",
      "Running PCA...\n",
      "Retaining 9 components to expl 99 pct var\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "(688, 1, 240, 240)\n",
      "time elapsed = 135.12133\n",
      "Computing higher order correlations...\n",
      "time elapsed = 7.16783\n",
      "Final size of features concatenated is [688 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n"
     ]
    }
   ],
   "source": [
    "features, feature_inds_defined = _feature_extractor(images, models[0,:], 0, fitting_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "d46fe3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BDCN contour feature extraction...\n",
      "Images array shape is:\n",
      "(688, 1, 240, 240)\n",
      "Final array shape is:\n",
      "torch.Size([688, 1, 240, 240])\n",
      "time elapsed = 7.07141\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.55, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 9]\n",
      "[min max] of first image patch is:\n",
      "[tensor(8.4639e-06, device='cuda:0'), tensor(0.2264, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(688, 72)\n",
      "Running PCA...\n",
      "Retaining 9 components to expl 99 pct var\n"
     ]
    }
   ],
   "source": [
    "features, feature_inds_defined = _contour_fn(images, models[0,:], 0, fitting_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "60b7d845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([688, 650])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "e087cff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds = np.concatenate((feature_inds_defined, [True]), axis=0)\n",
    "inds[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "63501182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1a7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "df446be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined_feature_extractor(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, modules, module_names, do_varpart=False):\n",
    "        \n",
    "        super(combined_feature_extractor, self).__init__()\n",
    "        self.modules = modules\n",
    "        self.module_names = module_names\n",
    "        self.do_varpart = do_varpart\n",
    "        \n",
    "    def init_for_fitting(self, image_size, models, dtype):\n",
    "        \n",
    "        max_features = 0\n",
    "        for module in self.modules:            \n",
    "            module.init_for_fitting(image_size, models, dtype)\n",
    "            max_features += module.max_features\n",
    "            \n",
    "        self.max_features = max_features\n",
    "        \n",
    "    def clear_maps(self):\n",
    "        \n",
    "        for module in self.modules:\n",
    "            module.clear_maps()\n",
    "            \n",
    "    def get_partial_versions(self):\n",
    "        \n",
    "        if not hasattr(self, 'max_features'):\n",
    "            raise RuntimeError('need to run init_for_fitting first')\n",
    "\n",
    "        n_total_feat = _feature_extractor.max_features\n",
    "        masks = np.ones((1,n_total_feat), dtype=int)\n",
    "        names = ['full_combined_model']\n",
    "            \n",
    "        if self.do_varpart:\n",
    "\n",
    "            # going to define \"masks\" that combine certain sub-sets of models features at a time\n",
    "            # to be uses for variance partitioning\n",
    "            feature_start_ind = 0\n",
    "\n",
    "            for mi, module in enumerate(_feature_extractor.modules):\n",
    "\n",
    "                # first a version that only includes the features in current module\n",
    "                new_mask = np.zeros((1, n_total_feat), dtype=int)\n",
    "                new_mask[0,feature_start_ind:feature_start_ind+module.max_features] = 1\n",
    "                masks = np.concatenate((masks, new_mask), axis=0)\n",
    "                names += ['just_' + _feature_extractor.module_names[mi]]\n",
    "\n",
    "                if len(_feature_extractor.modules)>2:        \n",
    "                    # next a version that only everything but the features in current module\n",
    "                    # (note if there are just 2 modules, this would be redundant)\n",
    "                    new_mask = np.ones((1, n_total_feat), dtype=int)\n",
    "                    new_mask[0,feature_start_ind:feature_start_ind+module.max_features] = 0\n",
    "                    masks = np.concatenate((masks, new_mask), axis=0)\n",
    "                    names += ['leave_out_' + _feature_extractor.module_names[mi]]\n",
    "\n",
    "                # if the module has any subsets of features defined, will also do partial versions with those subsets only\n",
    "                module_partial_masks, module_partial_names = module.get_partial_versions()\n",
    "                if len(module_partial_names)>1:        \n",
    "                    new_masks = np.zeros((len(module_partial_names)-1, n_total_feat), dtype=int)\n",
    "                    new_masks[:,feature_start_ind:feature_start_ind+module.max_features] = module_partial_masks[1:]\n",
    "                    masks = np.concatenate((masks, new_masks), axis=0)\n",
    "                    names += [_feature_extractor.module_names[mi] + '_' + name + '_no_other_modules'for name in module_partial_names[1:]]\n",
    "\n",
    "                    if len(module_partial_names)==3:        \n",
    "                        # for this special case, also adding in some other combinations  \n",
    "                        # if more than two subsets then this will get too complicated...\n",
    "                        new_masks = np.ones((len(module_partial_names)-1, n_total_feat), dtype=int)\n",
    "                        new_masks[:,feature_start_ind:feature_start_ind+module.max_features] = module_partial_masks[1:]\n",
    "                        masks = np.concatenate((masks, new_masks), axis=0)\n",
    "                        names += [_feature_extractor.module_names[mi] + '_' + name + '_plus_other_modules' for name in module_partial_names[1:]]\n",
    "\n",
    "                feature_start_ind += module.max_features\n",
    "\n",
    "        return masks, names\n",
    "        \n",
    "    def forward(self, images, prf_params, prf_model_ind, fitting_mode = True):\n",
    "\n",
    "        for mi, module in enumerate(self.modules):\n",
    "            \n",
    "            features, inds = module(images, prf_params, prf_model_ind, fitting_mode)\n",
    "            \n",
    "            if mi==0:\n",
    "                all_features_concat = features\n",
    "                feature_inds_defined = inds\n",
    "            else:\n",
    "                all_features_concat = torch.cat((all_features_concat, features), axis=1)\n",
    "                feature_inds_defined = np.concatenate((feature_inds_defined, inds), axis=0)\n",
    "  \n",
    "        return all_features_concat, feature_inds_defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "36762ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pixel_stats',\n",
       " 'mean_magnitudes',\n",
       " 'mean_realparts',\n",
       " 'marginal_stats_lowpass_recons',\n",
       " 'variance_highpass_resid',\n",
       " 'magnitude_feature_autocorrs',\n",
       " 'lowpass_recon_autocorrs',\n",
       " 'highpass_resid_autocorrs',\n",
       " 'magnitude_within_scale_crosscorrs',\n",
       " 'real_within_scale_crosscorrs',\n",
       " 'magnitude_across_scale_crosscorrs',\n",
       " 'real_imag_across_scale_crosscorrs',\n",
       " 'real_spatshift_within_scale_crosscorrs',\n",
       " 'real_spatshift_across_scale_crosscorrs']"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.modules[1].feature_types_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "94914a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import pyrtools as pt\n",
    "from utils import numpy_utils, torch_utils, texture_utils, prf_utils\n",
    "\n",
    "class texture_feature_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module to compute higher-order texture statistics of input images (e.g. Portilla & Simoncelli 2000, IJCV)\n",
    "    Statistics are computed within a specified region of space (a voxel's pRF)\n",
    "    Can specify different subsets of features to include (i.e. pixel-level stats, simple/complex cells, cross-correlations, auto-correlations)\n",
    "    Inputs to the forward pass are images and pRF parameters of interest [x,y,sigma]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,_fmaps_fn, sample_batch_size=100, feature_types_exclude=None, n_prf_sd_out=2, \\\n",
    "                 aperture=1.0, do_varpart=False, group_all_hl_feats=False, device=None):\n",
    "        \n",
    "        super(texture_feature_extractor, self).__init__()\n",
    "        \n",
    "        self.fmaps_fn = _fmaps_fn   \n",
    "        self.fmaps = None\n",
    "        self.n_sf = _fmaps_fn.pyr_height\n",
    "        self.n_ori =  _fmaps_fn.n_ori\n",
    "       \n",
    "        self.sample_batch_size = sample_batch_size       \n",
    "        self.n_prf_sd_out = n_prf_sd_out\n",
    "        self.aperture = aperture\n",
    "        self.device = device       \n",
    "        \n",
    "        self.do_varpart = do_varpart\n",
    "        self.group_all_hl_feats = group_all_hl_feats\n",
    "        \n",
    "        self.update_feature_list(feature_types_exclude)\n",
    "        self.do_pca = False\n",
    "       \n",
    "    def init_for_fitting(self, image_size, models, dtype):\n",
    "\n",
    "        \"\"\"\n",
    "        Additional initialization operations.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.max_features = self.n_features_total            \n",
    "        self.clear_maps()\n",
    "        \n",
    "       \n",
    "    def update_feature_list(self, feature_types_exclude):\n",
    "        \n",
    "        feature_types_all = ['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', \\\n",
    "            'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', \\\n",
    "            'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', \\\n",
    "            'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
    "        feature_type_dims = [6,16,16,10,1,\\\n",
    "                        272,73,25,\\\n",
    "                        24,24,48,96,\\\n",
    "                       10,20]\n",
    "\n",
    "        if feature_types_exclude is None:\n",
    "            feature_types_exclude = []\n",
    "        # decide which features to ignore, or use all features\n",
    "        self.feature_types_exclude = feature_types_exclude\n",
    "        \n",
    "        print(self.feature_types_exclude)    \n",
    "        # a few shorthands for ignoring sets of features at a time\n",
    "        if 'crosscorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'crosscorrs' in ff])\n",
    "        if 'autocorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'autocorrs' in ff])\n",
    "        if 'pixel' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['pixel_stats'])\n",
    "\n",
    "        self.feature_types_include  = [ff for ff in feature_types_all if not ff in feature_types_exclude]\n",
    "        if len(self.feature_types_include)==0:\n",
    "            raise ValueError('you have specified too many features to exclude, and now you have no features left! aborting.')\n",
    "            \n",
    "        feature_dims_include = [feature_type_dims[fi] for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in feature_types_exclude]\n",
    "        # how many features will be needed, in total?\n",
    "        self.n_features_total = np.sum(feature_dims_include)\n",
    "        \n",
    "        # numbers that define which feature types are in which column\n",
    "        self.feature_column_labels = np.squeeze(np.concatenate([fi*np.ones([1,feature_dims_include[fi]]) for fi in range(len(feature_dims_include))], axis=1).astype('int'))\n",
    "        assert(np.size(self.feature_column_labels)==self.n_features_total)\n",
    "        \n",
    "        if self.group_all_hl_feats:\n",
    "            # In this case pretend there are just two groups of features - the 'mean_magnitudes' which are first-level gabor-like\n",
    "            # and all other features combined into a second group. Makes it simpler to do variance partition analysis.\n",
    "            # if do_varpart=False, this does nothing.\n",
    "            self.feature_column_labels[self.feature_column_labels != 1] = -1\n",
    "            self.feature_column_labels[self.feature_column_labels==1] = 0\n",
    "            self.feature_column_labels[self.feature_column_labels==-1] = 1\n",
    "            self.feature_group_names = ['mean_magnitudes', 'all_other_texture_feats']\n",
    "        else:\n",
    "            self.feature_group_names = self.feature_types_include\n",
    "            \n",
    "    def get_partial_versions(self):\n",
    "        \n",
    "        if not hasattr(self, 'max_features'):\n",
    "            raise RuntimeError('need to run init_for_fitting first')\n",
    "            \n",
    "        n_feature_types = len(self.feature_group_names)\n",
    "        partial_version_names = ['full_model'] \n",
    "        masks = np.ones([1,self.n_features_total])\n",
    "        \n",
    "        if self.do_varpart:\n",
    "            \n",
    "            # \"Partial versions\" will be listed as: [full model, model w only first set of features, model w only second set, ...             \n",
    "            partial_version_names += ['just_%s'%ff for ff in self.feature_group_names]\n",
    "            masks2 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels==ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "            masks = np.concatenate((masks, masks2), axis=0)\n",
    "            \n",
    "            if n_feature_types > 2:\n",
    "                # if more than two types, also include models where we leave out first set of features, leave out second set of features...]\n",
    "                partial_version_names += ['leave_out_%s'%ff for ff in self.feature_group_names]           \n",
    "                masks3 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels!=ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "                masks = np.concatenate((masks, masks3), axis=0)           \n",
    "        \n",
    "        # masks always goes [n partial versions x n total features]\n",
    "        return masks, partial_version_names\n",
    "\n",
    "    def get_maps(self, images):\n",
    "    \n",
    "        print('Running steerable pyramid feature extraction...')\n",
    "        print('Images array shape is:')\n",
    "        print(images.shape)\n",
    "        t = time.time()\n",
    "        fmaps = self.fmaps_fn(images, to_torch=False, device=self.device)        \n",
    "        self.fmaps = fmaps\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    def clear_maps(self):\n",
    "        \n",
    "        print('Clearing steerable pyramid features from memory.')\n",
    "        self.fmaps = None\n",
    "        \n",
    "    def forward(self, images, prf_params, prf_model_index, fitting_mode=True):\n",
    "        \n",
    "        if self.fmaps is None:\n",
    "            self.get_maps(images)\n",
    "        else:\n",
    "            assert(images.shape[0]==self.fmaps[0][0].shape[0])\n",
    "        \n",
    "        if isinstance(prf_params, torch.Tensor):\n",
    "            prf_params = torch_utils.get_value(prf_params)\n",
    "        assert(np.size(prf_params)==3)\n",
    "        prf_params = np.squeeze(prf_params)\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = torch_utils.get_value(images)\n",
    "\n",
    "        print('Computing higher order correlations...')\n",
    "      \n",
    "        t = time.time()\n",
    "        pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "            magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "            magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "            real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs =  \\\n",
    "                    get_higher_order_features(self.fmaps, images, prf_params, sample_batch_size=self.sample_batch_size, n_prf_sd_out=self.n_prf_sd_out, aperture=self.aperture, device=self.device)\n",
    "        \n",
    "        \n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "        all_feat = OrderedDict({'pixel_stats':pixel_stats, 'mean_magnitudes':mean_magnitudes, 'mean_realparts':mean_realparts, \\\n",
    "                                'marginal_stats_lowpass_recons':marginal_stats_lowpass_recons, 'variance_highpass_resid':variance_highpass_resid, \\\n",
    "            'magnitude_feature_autocorrs':magnitude_feature_autocorrs, 'lowpass_recon_autocorrs':lowpass_recon_autocorrs, 'highpass_resid_autocorrs':highpass_resid_autocorrs, \\\n",
    "            'magnitude_within_scale_crosscorrs':magnitude_within_scale_crosscorrs, 'real_within_scale_crosscorrs':real_within_scale_crosscorrs, \\\n",
    "            'magnitude_across_scale_crosscorrs':magnitude_across_scale_crosscorrs, 'real_imag_across_scale_crosscorrs':real_imag_across_scale_crosscorrs, \\\n",
    "            'real_spatshift_within_scale_crosscorrs':real_spatshift_within_scale_crosscorrs, 'real_spatshift_across_scale_crosscorrs':real_spatshift_across_scale_crosscorrs})\n",
    "\n",
    "        feature_names_full = list(all_feat.keys())\n",
    "        feature_names = [fname for fname in feature_names_full if fname in self.feature_types_include]\n",
    "        self.feature_names = feature_names\n",
    "        assert(feature_names==self.feature_types_include) # double check here that the order is correct\n",
    "        \n",
    "        for ff, feature_name in enumerate(feature_names):   \n",
    "            assert(all_feat[feature_name] is not None)\n",
    "            if ff==0:\n",
    "                all_feat_concat = all_feat[feature_name]\n",
    "            else:               \n",
    "                all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "\n",
    "        assert(all_feat_concat.shape[1]==self.n_features_total)\n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        print('Feature types included are:')\n",
    "        print(feature_names)\n",
    "\n",
    "        if torch.any(torch.isnan(all_feat_concat)):\n",
    "            print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "        if torch.any(torch.sum(all_feat_concat, axis=0)==0):\n",
    "            print('\\nWARNING THERE ARE ZEROS IN FEATURES MATRIX\\n')\n",
    "            print('zeros for columns:')\n",
    "            print(np.where(torch.sum(all_feat_concat, axis=0)==0))\n",
    "            \n",
    "        feature_inds_defined = np.ones((self.n_features_total,), dtype=bool)\n",
    "        \n",
    "        return all_feat_concat, feature_inds_defined\n",
    "    \n",
    "\n",
    "class steerable_pyramid_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module that utilizes steerable pyramid ( https://pyrtools.readthedocs.io/en/latest/) to extract features.\n",
    "    For a batch of input images, will return all the pyramid coefficients, as well as additional types of feature maps\n",
    "    (i.e. partially reconstructed lowpass images at several frequency levels, upsampled feature maps).\n",
    "    These are used by 'get_higher_order_features' to extract various textural features of the image.\n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pyr_height=4, n_ori=8):\n",
    "        \n",
    "        super(steerable_pyramid_extractor, self).__init__()       \n",
    "        self.n_ori = n_ori\n",
    "        self.pyr_height = pyr_height # how many spatial frequencies?\n",
    "        self.pyr = None\n",
    "        \n",
    "    def forward(self, image_batch, to_torch=False, device=None):\n",
    "        \n",
    "        batch_size = image_batch.shape[0]\n",
    "        t  = time.time()\n",
    "        for ii in range(batch_size):\n",
    "            \n",
    "            # Call the pyramid generation code here, get all features for this image.\n",
    "            pyr = pt.pyramids.SteerablePyramidFreq(image_batch[ii,0,:,:], is_complex=True, height = self.pyr_height, order = self.n_ori-1)\n",
    "            self.pyr = pyr # storing the most recently generated pyramid, in case we need its properties later\n",
    "            \n",
    "            if ii==0:  \n",
    "                \n",
    "                # Initialize all the feature maps we want to store...\n",
    "                fmaps_complex = []\n",
    "                fmaps_coarser_upsampled = []\n",
    "               \n",
    "                # Will generate several low-pass filtered representations of the image - will use these as additional feature maps for \n",
    "                # computing autocorrelations and marginal statistics.\n",
    "                fmaps_lowpass_recon = []\n",
    "                fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "               \n",
    "                # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "                sf_reverse  = self.pyr_height  # need to go backward because pyr comes out in the opposite order from what i want\n",
    "                for sf in range(self.pyr_height):\n",
    "                    sf_reverse -= 1\n",
    "                    fmaps_complex.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))\n",
    "                    \n",
    "                    # Initialize \"parent\" representations for this level (map from the next coarsest resolution, upsampled to the finer scale)\n",
    "                    # To be used for cross-scale comparisons.\n",
    "                    if sf==0:\n",
    "                        # this will be the lowpass residual (non-oriented).\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, 1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))                     \n",
    "                    else:\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))     \n",
    "                        \n",
    "                    fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                         pyr.pyr_coeffs[sf_reverse,0].shape[1])))\n",
    "\n",
    "                fmaps_resid = []    \n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_highpass'].shape[0],pyr.pyr_coeffs['residual_highpass'].shape[1])))\n",
    "\n",
    "            # First get lowpass filtered representation of the image\n",
    "            lowpass_recon = pyr.recon_pyr(levels='residual_lowpass', bands='all', twidth=1) \n",
    "            scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size['residual_lowpass'][0]\n",
    "            lowpass_recon = texture_utils.shrink(lowpass_recon, scale_by)*scale_by**2\n",
    "            fmaps_lowpass_recon[0][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "            # Get the \"parent\" for lowest SF level (upsample the residual lowpass)\n",
    "#             print(pyr.pyr_coeffs['residual_lowpass'].shape)\n",
    "            upsampled = texture_utils.expand(pyr.pyr_coeffs['residual_lowpass'], factor=2)/2**2\n",
    "#             print(upsampled.shape)\n",
    "            fmaps_coarser_upsampled[0][ii,0,:,:] = upsampled\n",
    "            \n",
    "            # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "            sf_reverse  = self.pyr_height # need to go backward because pyr comes out in the opposite order from what i want\n",
    "            for sf in range(self.pyr_height):\n",
    "                sf_reverse -= 1\n",
    "                for oo in range(self.n_ori):     \n",
    "\n",
    "                    # These are the main feature maps of the pyramid - one feature map per scale per orientation band.\n",
    "                    # Complex number, can take the magnitude or real/imaginary part to simulate complex or simple cell-type responses.\n",
    "                    fmaps_complex[sf][ii,oo,:,:] = pyr.pyr_coeffs[(sf_reverse,oo)]\n",
    "                    \n",
    "                    if sf<self.pyr_height-1:\n",
    "                        # Store this as a \"parent\" representation, will be used for the next most fine SF level (i.e. sf+1)\n",
    "                        upsampled = texture_utils.expand(pyr.pyr_coeffs[(sf_reverse,oo)], factor=2)/2**2\n",
    "                        # Double the phase (angle of the complex number); note this doesn't affect the magnitude. \n",
    "                        phase_doubled = texture_utils.double_phase(upsampled)\n",
    "                        fmaps_coarser_upsampled[sf+1][ii,oo,:,:] = phase_doubled\n",
    "                   \n",
    "                        \n",
    "                # Get the bandpass filtered representation for this scale\n",
    "                bandpass_image = np.real(pyr.recon_pyr(levels=sf_reverse, bands='all', twidth=1))\n",
    "                scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size[(sf_reverse,0)][0]\n",
    "                bandpass_image = texture_utils.shrink(bandpass_image, factor=scale_by)*scale_by**2\n",
    "                \n",
    "                # Add it onto the lowpass_recon (gets modified every loop iteration)\n",
    "                lowpass_recon = texture_utils.expand(lowpass_recon, factor=2)/2**2\n",
    "                lowpass_recon = lowpass_recon + bandpass_image\n",
    "                fmaps_lowpass_recon[sf+1][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "\n",
    "            # Grab residual feature maps, the lowest and highest levels of the pyramid\n",
    "            fmaps_resid[0][ii,0,:,:] = pyr.pyr_coeffs['residual_lowpass']\n",
    "            fmaps_resid[1][ii,0,:,:] = pyr.pyr_coeffs['residual_highpass']\n",
    "            \n",
    "            \n",
    "        elapsed = time.time() - t\n",
    "#         print('time elapsed: %.5f s'%elapsed)\n",
    "\n",
    "        if to_torch:            \n",
    "            fmaps_complex = [torch.from_numpy(fm).to(device) for fm in fmaps_complex]            \n",
    "            fmaps_resid = [torch_utils._to_torch(fm, device=device) for fm in fmaps_resid]\n",
    "            fmaps_lowpass_recon = [torch_utils._to_torch(fm, device=device) for fm in fmaps_lowpass_recon]                      \n",
    "            fmaps_coarser_upsampled = [torch.from_numpy(fm).to(device) for fm in fmaps_coarser_upsampled]\n",
    "\n",
    "        return fmaps_complex, fmaps_resid, fmaps_lowpass_recon, fmaps_coarser_upsampled\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "def get_higher_order_features(fmaps, images, prf_params, sample_batch_size=20, n_prf_sd_out=2, aperture=1.0, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute higher order texture features for a batch of images.\n",
    "    Input the module that defines steerable pyramid (i.e. 'steerable_pyramid_extractor'), and desired prf parameters.\n",
    "    Returns arrays of each higher order feature.  \n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "\n",
    "    fmaps_complex_all, fmaps_resid_all, fmaps_lowpass_recon_all, fmaps_coarser_upsampled_all = fmaps\n",
    "   \n",
    "    n_trials = fmaps_complex_all[0].shape[0]\n",
    "    x,y,sigma = prf_params\n",
    "\n",
    "    n_sf = len(fmaps_complex_all)\n",
    "    n_ori = fmaps_complex_all[0].shape[1]\n",
    "        \n",
    "    # all pairs of different orientation channels.\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "\n",
    "    # mean, variance, skew, kurtosis, min, max\n",
    "    pixel_stats = torch.zeros((n_trials,6), device=device)\n",
    "\n",
    "    # Mean magnitude each scale/orientation, within the prf.\n",
    "    mean_magnitudes = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "    mean_realparts = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "\n",
    "    # Store the skew and kurtosis of the lowpass reconstructions at each scale\n",
    "    marginal_stats_lowpass_recons = torch.zeros((n_trials, n_sf+1, 2), device=device)\n",
    "\n",
    "    # Variance of the highpass residual\n",
    "    variance_highpass_resid = torch.zeros((n_trials, 1), device=device)\n",
    "\n",
    "    # how many unique autocorrelation values will we get out for each feature map? These will be pre-defined, same for every pRF.\n",
    "    # but different for different scales of feature maps.\n",
    "    # note also that for bigger prfs, there will potentially be more pixels that contribute to the autocorrelation computation - \n",
    "    # but a fixed portion of the matrix is returned.\n",
    "    autocorr_output_pix=np.array([3,3,5,7,7])\n",
    "    n_autocorr_vals = ((autocorr_output_pix**2+1)/2).astype('int')\n",
    "    max_autocorr_vals = np.max(n_autocorr_vals)\n",
    "    \n",
    "    # Spatial autocorrelation of the magnitude of spectral coefficients, within each scale and orientation.\n",
    "    magnitude_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, max_autocorr_vals], device=device) # this is ace in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the partially-reconstructed lowpass image representation at each scale\n",
    "    lowpass_recon_autocorrs = torch.zeros([n_trials, n_sf+1, max_autocorr_vals], device=device) # this is acr in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the highpass residual\n",
    "    highpass_resid_autocorrs = torch.zeros([n_trials, 1, max_autocorr_vals], device=device)\n",
    "\n",
    "    # Within scale correlations of feature maps: compare feature map magnitudes for different orientations.\n",
    "    magnitude_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is C0 in the matlab code\n",
    "    # Using the real parts.\n",
    "    real_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is Cr0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations of feature maps: always comparing each scale to an up-sampled version of the scale coarser than it.\n",
    "    magnitude_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_ori, n_ori], device=device) # this is Cx0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations, using the real and imaginary parts separately. The phase (angle) of the coarser map is doubled before computing these.\n",
    "    real_imag_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, 2, n_ori, n_ori], device=device) # this is Crx0 in the matlab code\n",
    "\n",
    "    # These are comparisons with spatially shifted versions of the lowpass residual. Not sure we need this...\n",
    "    n_spatshifts = 5;\n",
    "    real_spatshift_within_scale_crosscorrs = torch.zeros([n_trials, 1, n_spatshifts, n_spatshifts], device=device)# this is Cr0 in the matlab code\n",
    "    real_spatshift_across_scale_crosscorrs = torch.zeros([n_trials, 1, n_ori, n_spatshifts], device=device)  # this is Crx0 in the matlab code\n",
    "\n",
    "    # Looping over batches of trials to compute everything of interest.\n",
    "    bb=-1\n",
    "    for batch_inds, batch_size_actual in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "        bb=bb+1\n",
    "\n",
    "        fmaps_complex = [torch.from_numpy(fmaps_complex_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_complex_all))]\n",
    "        fmaps_resid = [torch.from_numpy(fmaps_resid_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_resid_all))]\n",
    "        fmaps_lowpass_recon = [torch.from_numpy(fmaps_lowpass_recon_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_lowpass_recon_all))]\n",
    "        fmaps_coarser_upsampled = [torch.from_numpy(fmaps_coarser_upsampled_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_coarser_upsampled_all))]\n",
    "\n",
    "        if bb==0:\n",
    "            npix_each_scale = [fmaps_complex_all[sc].shape[2] for sc in np.arange(n_sf-1,-1,-1)]\n",
    "            npix_each_scale.append(fmaps_resid_all[0].shape[2])\n",
    "            npix_each_scale.reverse()\n",
    "\n",
    "        # First working with the finest scale (original image)\n",
    "        n_pix = npix_each_scale[-1]      \n",
    "        g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[-1])\n",
    "\n",
    "        # Gather pixel-wise statistics here \n",
    "        wmean, wvar, wskew, wkurt = texture_utils.get_weighted_pixel_features(images[batch_inds], spatial_weights, device=device)\n",
    "        pixel_stats[batch_inds,0] = torch.squeeze(wmean)\n",
    "        pixel_stats[batch_inds,1] = torch.squeeze(wvar)\n",
    "        pixel_stats[batch_inds,2] = torch.squeeze(wskew)\n",
    "        pixel_stats[batch_inds,3] = torch.squeeze(wkurt)\n",
    "        pixel_stats[batch_inds,4] = torch_utils._to_torch(np.squeeze(np.min(np.min(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "        pixel_stats[batch_inds,5] = torch_utils._to_torch(np.squeeze(np.max(np.max(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "\n",
    "        # Autocorrs of the highpass residual\n",
    "        highpass_resid = fmaps_resid[1]\n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(highpass_resid, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[-1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        highpass_resid_autocorrs[batch_inds,0,0:n_autocorr_vals[-1]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[-1]])\n",
    "\n",
    "        # Variance of the highpass residual\n",
    "        m, wvar, s, k = texture_utils.get_weighted_pixel_features(highpass_resid, spatial_weights, device=device)\n",
    "        variance_highpass_resid[batch_inds,0] = torch.squeeze(wvar)\n",
    "\n",
    "        # Next work with the low-pass reconstruction (most coarse scale, smallest npix)\n",
    "        n_pix = npix_each_scale[0]       \n",
    "        g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[0])\n",
    "\n",
    "        lowpass_rec = fmaps_lowpass_recon[0]\n",
    "\n",
    "        # Marginal stats of low-pass reconstruction\n",
    "        m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_rec, spatial_weights, device=device)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,0] = torch.squeeze(wskew)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,1] = torch.squeeze(wkurt)\n",
    "\n",
    "        # Autocorrs of low-pass reconstruction \n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_rec, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[0], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        lowpass_recon_autocorrs[batch_inds,0,0:n_autocorr_vals[0]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[0]])\n",
    "\n",
    "        # Looping over spatial frequency/scale\n",
    "        # Loop goes low SF (smallest npix) to higher SF (largest npix)\n",
    "        for ff in range(n_sf):\n",
    "         \n",
    "            # Scale specific things - get the prf at this resolution of interest    \n",
    "            n_pix = npix_each_scale[ff+1]           \n",
    "            g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "            spatial_weights = g[2][0]\n",
    "            patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[1+ff])\n",
    "\n",
    "            # Get the low-pass reconstruction at this scale\n",
    "            lowpass_summed = fmaps_lowpass_recon[ff+1]  # this is summed over this scale band and those below it\n",
    "            m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_summed, spatial_weights, device=device)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,0] = torch.squeeze(wskew)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,1] = torch.squeeze(wkurt)\n",
    "\n",
    "            # Autocorrelations of low-pass reconstruction (at this scale)\n",
    "            auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_summed, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "            lowpass_recon_autocorrs[batch_inds,ff+1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "            # Loop over orientation channels\n",
    "            xx=-1\n",
    "            for oo1 in range(n_ori):       \n",
    "\n",
    "                # Magnitude of the complex coefficients; complex cell-like responses\n",
    "                mag1 = torch.abs(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                # The mean magnitudes here are basically second-order spectral statistics, within the specified spatial region defined by weights\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(mag1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_magnitudes[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "                \n",
    "                mag1 = mag1 - torch.tile(torch.mean(torch.mean(mag1, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                # Real parts of the complex coefficients; simple cell-like responses\n",
    "                real1 = torch.real(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()    \n",
    "                \n",
    "                # Average of the real parts within the specified spatial region\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(real1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_realparts[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "\n",
    "                # Complex cell autocorrelation (correlation w spatially shifted versions of itself)     \n",
    "                auto_corr = texture_utils.weighted_auto_corr_2d(mag1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "                magnitude_feature_autocorrs[batch_inds,ff,oo1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "                # Within-scale correlations - comparing resp at orient==oo1 to responses at all other orientations, same scale.\n",
    "                for oo2 in np.arange(oo1+1, n_ori):            \n",
    "                    xx = xx+1 \n",
    "                    assert(oo1==ori_pairs[xx,0] and oo2==ori_pairs[xx,1])\n",
    "\n",
    "                    # Magnitude at the other orientation (oo2)\n",
    "                    mag2 = torch.abs(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()      \n",
    "                    mag2 = mag2 - torch.tile(torch.mean(torch.mean(mag2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                    # Correlate the magnitude feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    magnitude_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                    # Real part at the other orientation (oo2)\n",
    "                    real2 = torch.real(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()                     \n",
    "\n",
    "                    # Correlate the real feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(real1, real2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    real_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                # Cross-scale correlations - for these we care about same ori to same ori, so looping over all orientations.\n",
    "                # Going to compare coefficients at the current scale to those at a coarser scale (ff-1)\n",
    "                # If we're at first scale (ff=0), then will use a different method.\n",
    "                if ff>0:\n",
    "\n",
    "                    for oo2 in range(n_ori):\n",
    "\n",
    "                        # Get magnitude of coefficients for neighboring (coarser) scale                        \n",
    "                        mag_coarser2 = torch.abs(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        mag_coarser2 = mag_coarser2 - torch.tile(torch.mean(torch.mean(mag_coarser2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                        # Correlate this with the finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)            \n",
    "                        magnitude_across_scale_crosscorrs[batch_inds,ff-1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        # Get the real and imaginary parts at coarser scale\n",
    "                        real_coarser2 = torch.real(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        imag_coarser2 = torch.imag(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                        # Correlate each of these with real part at finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,0,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, imag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # instead of different orientations for the \"parent\" level here, have spatially shifted versions.\n",
    "                    real_coarser = torch.real(fmaps_coarser_upsampled[ff][:,0,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                    shifts = [[0,0],[1,3],[-1,3],[1,2],[-1,2]]\n",
    "                    for si1, shift1 in enumerate(shifts):\n",
    "\n",
    "                        ss,dd = shift1\n",
    "                        real_coarser_shifted1 = torch.roll(real_coarser, shifts=ss, dims=dd)               \n",
    "                        # Real part at the finer scale compared to spatially shifted at the coarser scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser_shifted1, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_spatshift_across_scale_crosscorrs[batch_inds,ff,oo1,si1] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        for si2 in np.arange(si1+1, n_spatshifts):\n",
    "\n",
    "                            ss,dd = shifts[si2]\n",
    "                            real_coarser_shifted2 = torch.roll(real_coarser, shifts=ss, dims=dd) \n",
    "                            # Real parts at same scale, comparing spatially shifted.\n",
    "                            cross_corr = texture_utils.weighted_cross_corr_2d(real_coarser_shifted1, real_coarser_shifted2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                            real_spatshift_within_scale_crosscorrs[batch_inds,ff,si1,si2] = torch.squeeze(cross_corr)\n",
    "\n",
    "            \n",
    "    # Reshape everything to [ntrials x nfeatures]\n",
    "    \n",
    "    mean_magnitudes = torch.reshape(mean_magnitudes, [n_trials, -1])\n",
    "    mean_realparts = torch.reshape(mean_realparts, [n_trials, -1])\n",
    "    marginal_stats_lowpass_recons = torch.reshape(marginal_stats_lowpass_recons, [n_trials, -1])\n",
    "    variance_highpass_resid =torch.reshape(variance_highpass_resid, [n_trials, -1])\n",
    "\n",
    "    magnitude_feature_autocorrs = torch.reshape(magnitude_feature_autocorrs, [n_trials, -1])\n",
    "    # take out the zero columns, which happen because of different size autocorr outputs.\n",
    "    magnitude_feature_autocorrs = magnitude_feature_autocorrs[:,torch.sum(magnitude_feature_autocorrs, axis=0)!=0]\n",
    "    assert(magnitude_feature_autocorrs.shape[1]==np.sum(n_autocorr_vals[1:]*n_ori))\n",
    "\n",
    "    lowpass_recon_autocorrs = torch.reshape(lowpass_recon_autocorrs, [n_trials, -1])\n",
    "    lowpass_recon_autocorrs = lowpass_recon_autocorrs[:,torch.sum(lowpass_recon_autocorrs, axis=0)!=0]\n",
    "    assert(lowpass_recon_autocorrs.shape[1]==np.sum(n_autocorr_vals))\n",
    "\n",
    "    highpass_resid_autocorrs = torch.reshape(highpass_resid_autocorrs, [n_trials, -1])\n",
    "\n",
    "    magnitude_within_scale_crosscorrs = torch.reshape(magnitude_within_scale_crosscorrs, [n_trials, -1])\n",
    "    real_within_scale_crosscorrs = torch.reshape(real_within_scale_crosscorrs, [n_trials, -1])\n",
    "    magnitude_across_scale_crosscorrs = torch.reshape(magnitude_across_scale_crosscorrs, [n_trials, -1])\n",
    "    real_imag_across_scale_crosscorrs = torch.reshape(real_imag_across_scale_crosscorrs, [n_trials, -1])\n",
    "    real_spatshift_within_scale_crosscorrs = torch.reshape(real_spatshift_within_scale_crosscorrs, [n_trials, -1])\n",
    "    \n",
    "    real_spatshift_within_scale_crosscorrs = real_spatshift_within_scale_crosscorrs[:,torch.sum(real_spatshift_within_scale_crosscorrs, axis=0)!=0]\n",
    "    assert(real_spatshift_within_scale_crosscorrs.shape[1]==np.sum(np.arange(1,n_spatshifts)))\n",
    "\n",
    "    real_spatshift_across_scale_crosscorrs = torch.reshape(real_spatshift_across_scale_crosscorrs, [n_trials, -1])\n",
    "\n",
    "        \n",
    "    return pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "            magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "            magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "            real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "47a5f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from utils import prf_utils, torch_utils, texture_utils, nms_utils\n",
    "bdcn_path = '/user_data/mmhender/toolboxes/BDCN/'\n",
    "sys.path.append(bdcn_path)\n",
    "import bdcn\n",
    "\n",
    "class bdcn_feature_extractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model_file, device, aperture = 1.0, n_prf_sd_out = 2, \\\n",
    "                 batch_size=10, map_ind = -1, mult_patch_by_prf=True, downsample_factor = 1, do_nms = False, \\\n",
    "                 do_pca = True, min_pct_var = 99, max_pc_to_retain = 100):\n",
    "        \n",
    "        super(bdcn_feature_extractor, self).__init__()\n",
    "        \n",
    "        self.pretrained_model_file = pretrained_model_file\n",
    "        self.device = device\n",
    "        self.load_model_file()\n",
    "          \n",
    "        self.aperture = aperture\n",
    "        self.n_prf_sd_out = n_prf_sd_out\n",
    "        self.batch_size = batch_size\n",
    "        self.map_ind = map_ind\n",
    "        self.mult_patch_by_prf = mult_patch_by_prf\n",
    "        if downsample_factor<1:\n",
    "            raise ValueError('downsample factor must be >= 1')\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.do_nms = do_nms        \n",
    "        self.fmaps = None\n",
    "    \n",
    "        self.do_pca = do_pca\n",
    "        if self.do_pca:\n",
    "            self.min_pct_var = min_pct_var\n",
    "            self.max_pc_to_retain = max_pc_to_retain\n",
    "        else:\n",
    "            self.min_pct_var = None\n",
    "            self.max_pc_to_retain = None  \n",
    "            \n",
    "        self.do_varpart=False # only one set of features in this model for now, not doing variance partition\n",
    "  \n",
    "    def load_model_file(self):\n",
    "        \n",
    "        model = bdcn.BDCN()\n",
    "        model.load_state_dict(torch.load(self.pretrained_model_file))\n",
    "        model = model.to(self.device)\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "          \n",
    "    def init_for_fitting(self, image_size, models, dtype):\n",
    "\n",
    "        \"\"\"\n",
    "        Additional initialization operations which can only be done once we know image size and\n",
    "        desired set of candidate prfs.\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Initializing for fitting: finding the size of feature matrix for each candidate prf')\n",
    "        n_prfs = len(models)\n",
    "        downsampled_size = np.ceil(np.array(image_size)/self.downsample_factor).astype('int')\n",
    "        n_feat_each_prf = np.zeros(shape=(n_prfs,),dtype=int)\n",
    "\n",
    "        for mm in range(n_prfs):\n",
    "            bbox = texture_utils.get_bbox_from_prf(models[mm], downsampled_size, n_prf_sd_out = self.n_prf_sd_out, \\\n",
    "                                                   min_pix=None, verbose=False, force_square=False)\n",
    "            n_feat_each_prf[mm] =  (bbox[1] - bbox[0]) * (bbox[3] - bbox[2])           \n",
    "        self.n_feat_each_prf = n_feat_each_prf;\n",
    "        \n",
    "        if self.do_pca:\n",
    "            \n",
    "            print('Initializing arrays for PCA params')\n",
    "            # will need to save pca parameters to reproduce it during validation stage\n",
    "            # max pc to retain is just to save space, otherwise the \"pca_wts\" variable becomes huge  \n",
    "            self.max_features = self.max_pc_to_retain\n",
    "            self.pca_wts = [np.zeros(shape=(self.max_pc_to_retain, n_feat_each_prf[mm]), dtype=dtype) for mm in range(n_prfs)] \n",
    "            self.pca_pre_mean = [np.zeros(shape=(n_feat_each_prf[mm],), dtype=dtype) for mm in range(n_prfs)]\n",
    "            self.pct_var_expl = np.zeros(shape=(self.max_pc_to_retain, n_prfs), dtype=dtype)\n",
    "            self.n_comp_needed = np.full(shape=(n_prfs), fill_value=-1, dtype=np.int)\n",
    "\n",
    "        else:\n",
    "            self.max_features = np.max(n_feat_each_prf)\n",
    "            \n",
    "        self.clear_maps()\n",
    " \n",
    "\n",
    "    def get_partial_versions(self):\n",
    "\n",
    "        if not hasattr(self, 'max_features'):\n",
    "            raise RuntimeError('need to run init_for_fitting first')\n",
    "           \n",
    "        partial_version_names = ['full_model']\n",
    "        masks = np.ones([1,self.max_features])\n",
    "\n",
    "        return masks, partial_version_names\n",
    "        \n",
    "    def get_maps(self, images):\n",
    "        \n",
    "        print('Running BDCN contour feature extraction...')\n",
    "        print('Images array shape is:')\n",
    "        print(images.shape)\n",
    "        t = time.time()\n",
    "       \n",
    "        maps_each_scale, names = get_bdcn_maps(self.model, images, self.batch_size, self.map_ind)\n",
    "        maps = torch_utils._to_torch(maps_each_scale[0], device=self.device)\n",
    "\n",
    "        if not self.downsample_factor==1:            \n",
    "            orig_size = np.array(maps.shape[2:4])\n",
    "            downsampled_size = np.ceil(orig_size/self.downsample_factor).astype('int')\n",
    "            resized_maps = torch.nn.functional.interpolate(maps, \\\n",
    "                                                           size=(downsampled_size[0], downsampled_size[1]), \\\n",
    "                                                           mode = 'bilinear')\n",
    "            maps = resized_maps\n",
    "            print('Downsampled by factor of %.2f, new size is:'%self.downsample_factor)\n",
    "            print(maps.shape)\n",
    "                    \n",
    "        maps = torch.sigmoid(maps)\n",
    "        \n",
    "        if self.do_nms:\n",
    "            print('Applying non-maximum suppression to edge maps (can be slow...)')\n",
    "            maps = nms_utils.apply_nms(maps)\n",
    "        \n",
    "        self.fmaps = maps\n",
    "        \n",
    "        print('Final array shape is:')\n",
    "        print(maps.shape)\n",
    "            \n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)        \n",
    "        \n",
    "    def clear_maps(self):\n",
    "        \n",
    "        print('Clearing BDCN contour features from memory.')\n",
    "        self.fmaps = None    \n",
    "    \n",
    "    def forward(self, images, prf_params, prf_model_index, fitting_mode = True):\n",
    "        \n",
    "        if self.fmaps is None:\n",
    "            self.get_maps(images)\n",
    "        else:\n",
    "            assert(images.shape[0]==self.fmaps.shape[0])\n",
    "\n",
    "        maps = self.fmaps  \n",
    "        x,y,sigma = prf_params\n",
    "        print('Getting features for pRF [x,y,sigma]:')\n",
    "        print([x,y,sigma])\n",
    "        n_pix = maps.shape[2]\n",
    "\n",
    "         # Define the RF for this \"model\" version\n",
    "        prf = torch_utils._to_torch(prf_utils.make_gaussian_mass(x, y, sigma, n_pix, size=self.aperture, \\\n",
    "                                  dtype=np.float32)[2], device=self.device)\n",
    "\n",
    "        if self.mult_patch_by_prf:\n",
    "            minval = torch.min(prf)\n",
    "            maxval = torch.max(prf-minval)\n",
    "            prf_scaled = (prf - minval)/maxval\n",
    "            # Multiply the feature map by gaussian pRF weights, before cropping\n",
    "            maps = maps * prf_scaled\n",
    "        \n",
    "        # Crop the patch +/- n SD away from center\n",
    "        bbox = texture_utils.get_bbox_from_prf(prf_params, prf.shape, self.n_prf_sd_out, min_pix=None, verbose=False, force_square=False)\n",
    "        print('bbox to crop is:')\n",
    "        print(bbox)\n",
    "        maps_cropped = maps[:,:,bbox[0]:bbox[1], bbox[2]:bbox[3]]\n",
    "        \n",
    "        print('[min max] of first image patch is:')\n",
    "        print([torch.min(maps_cropped[0,0,:,:]), torch.max(maps_cropped[0,0,:,:])])\n",
    "        \n",
    "        # return [ntrials x nfeatures]\n",
    "        # Note this reshaping goes in \"C\" style order by default\n",
    "        features = torch.reshape(maps_cropped, [maps_cropped.shape[0], np.prod(maps_cropped.shape[1:])])\n",
    "                \n",
    "        if self.do_pca:    \n",
    "            features = self.reduce_pca(features, prf_model_index, fitting_mode)\n",
    "\n",
    "        feature_inds_defined = np.zeros((self.max_features,), dtype=bool)\n",
    "        feature_inds_defined[0:features.shape[1]] = 1\n",
    "            \n",
    "        return features, feature_inds_defined\n",
    "     \n",
    "        \n",
    "    def reduce_pca(self, features, prf_model_index, fitting_mode=True):\n",
    "        \n",
    "        if torch.is_tensor(features):\n",
    "            features = features.detach().cpu().numpy()\n",
    "            was_tensor=True\n",
    "        else:\n",
    "            was_tensor=False\n",
    "            \n",
    "        n_trials = features.shape[0]\n",
    "        n_features_actual = features.shape[1]\n",
    "        assert(n_features_actual == self.n_feat_each_prf[prf_model_index])\n",
    "        print('Preparing for PCA: original dims of features:')\n",
    "        print(features.shape)\n",
    "        \n",
    "        if fitting_mode:\n",
    "            \n",
    "            # Going to perform pca on the raw features\n",
    "            # First make sure it hasn't been done yet!\n",
    "            assert(self.n_comp_needed[prf_model_index]==-1) \n",
    "            print('Running PCA...')\n",
    "            pca = decomposition.PCA(n_components = np.min([np.min([self.max_pc_to_retain, n_features_actual]), n_trials]), copy=False)\n",
    "            # Perform PCA to decorrelate feats and reduce dimensionality\n",
    "            scores = pca.fit_transform(features)           \n",
    "            features = None            \n",
    "            wts = pca.components_\n",
    "            ev = pca.explained_variance_\n",
    "            ev = ev/np.sum(ev)*100\n",
    "            # wts/components goes [ncomponents x nfeatures]. \n",
    "            # nfeatures is always actual number of raw features\n",
    "            # ncomponents is min(ntrials, nfeatures)\n",
    "            # to save space, only going to save up to some max number of components.\n",
    "            n_components_actual = np.min([wts.shape[0], self.max_pc_to_retain])\n",
    "            # save a record of the transformation to be used for validating model\n",
    "            self.pca_wts[prf_model_index][0:n_components_actual,0:n_features_actual] = wts[0:n_components_actual,:] \n",
    "            # mean of each feature, nfeatures long - needed to reproduce transformation\n",
    "            self.pca_pre_mean[prf_model_index][0:n_features_actual] = pca.mean_ \n",
    "            # max len of ev is the number of components\n",
    "            self.pct_var_expl[0:n_components_actual,prf_model_index] = ev[0:n_components_actual]  \n",
    "            n_components_reduced = int(np.where(np.cumsum(ev)>self.min_pct_var)[0][0] if np.any(np.cumsum(ev)>self.min_pct_var) else len(ev))\n",
    "            self.n_comp_needed[prf_model_index] = n_components_reduced\n",
    "            print('Retaining %d components to expl %d pct var'%(n_components_reduced, self.min_pct_var))\n",
    "            assert(n_components_reduced<=self.max_pc_to_retain)            \n",
    "            features_reduced = scores[:,0:n_components_reduced]\n",
    "           \n",
    "        else:\n",
    "            \n",
    "            # This is a validation pass, going to use the pca pars that were computed on training set\n",
    "            # Make sure it has been done already!\n",
    "            assert(self.n_comp_needed[prf_model_index]!=-1)\n",
    "            print('Applying pre-computed PCA matrix...')\n",
    "            # Apply the PCA transformation, just as it was done during training\n",
    "            features_submean = features - np.tile(np.expand_dims(self.pca_pre_mean[prf_model_index][0:n_features_actual], axis=0), [n_trials, 1])\n",
    "            features_reduced = features_submean @ np.transpose(self.pca_wts[prf_model_index][0:self.n_comp_needed[prf_model_index],0:n_features_actual])               \n",
    "                       \n",
    "        features = None\n",
    "        \n",
    "        if was_tensor:\n",
    "            features_reduced = torch.tensor(features_reduced).to(self.device)\n",
    "        \n",
    "        return features_reduced\n",
    "    \n",
    "    \n",
    "\n",
    "def get_bdcn_maps(model, images, batch_size=10, map_inds=None):\n",
    "            \n",
    "    device = list(model.parameters())[0].device\n",
    " \n",
    "    if map_inds is not None:\n",
    "        if np.isscalar(map_inds):\n",
    "            map_inds = [map_inds]\n",
    "    else:\n",
    "        map_inds = np.arange(0,11)\n",
    "        \n",
    "    n_images = images.shape[0]\n",
    "    n_batches = int(np.ceil(n_images/batch_size))\n",
    "\n",
    "    for bb in range(n_batches):\n",
    "\n",
    "        batch_inds = np.arange(batch_size * bb, np.min([batch_size * (bb+1), n_images]))\n",
    "        image_batch = images[batch_inds,:,:,:]\n",
    "        \n",
    "        out = model(prep_for_bdcn(image_batch, device))    \n",
    "        out = [oo.detach().cpu().numpy() for oo in out]\n",
    "\n",
    "        p1_1, p2_1, p3_1, p4_1, p5_1, p1_2, p2_2, p3_2, p4_2, p5_2, fuse = out\n",
    "        \n",
    "        # undoing the sums they do at end of forward pass, to get out the raw feature maps at each scale\n",
    "        # p1_1 = s1\n",
    "        # p2_1 = s2 + o1\n",
    "        # p3_1 = s3 + o2 + o1\n",
    "        # p4_1 = s4 + o3 + o2 + o1\n",
    "        # p5_1 = s5 + o4 + o3 + o2 + o1\n",
    "\n",
    "        # p1_2 = s11 + o21 + o31 + o41 + o51\n",
    "        # p2_2 = s21 + o31 + o41 + o51\n",
    "        # p3_2 = s31 + o41 + o51\n",
    "        # p4_2 = s41 + o51\n",
    "        # p5_2 = s51\n",
    "\n",
    "        s1_1 = p1_1\n",
    "        s2_1 = p2_1 - s1_1\n",
    "        s3_1 = p3_1 - s1_1 - s2_1\n",
    "        s4_1 = p4_1 - s1_1 - s2_1 - s3_1\n",
    "        s5_1 = p5_1 - s1_1 - s2_1 - s3_1 - s4_1\n",
    "\n",
    "        s5_2 = p5_2\n",
    "        s4_2 = p4_2 - s5_2\n",
    "        s3_2 = p3_2 - s5_2 - s4_2\n",
    "        s2_2 = p2_2 - s5_2 - s4_2 - s3_2\n",
    "        s1_2 = p1_2 - s5_2 - s4_2 - s3_2 - s2_2\n",
    "\n",
    "        maps_each_scale_this_batch = [s1_1, s2_1, s3_1, s4_1, s5_1, s1_2, s2_2, s3_2, s4_2, s5_2, fuse]\n",
    "\n",
    "        if bb==0:\n",
    "            maps_each_scale = [maps_each_scale_this_batch[mi] for mi in map_inds]\n",
    "        else:\n",
    "            for ii, mi in enumerate(map_inds):                    \n",
    "                maps_each_scale[ii] = np.concatenate((maps_each_scale[ii], maps_each_scale_this_batch[mi]), axis=0)\n",
    "            \n",
    "\n",
    "    names1 = ['s2d_%d'%(ii+1) for ii in range(5)]\n",
    "    names2 = ['d2s_%d'%(ii+1) for ii in range(5)]\n",
    "    names = list(np.concatenate([names1, names2, ['BDCN: fused']]))\n",
    "\n",
    "    return maps_each_scale, names\n",
    "\n",
    "\n",
    "def prep_for_bdcn(image_data, device):\n",
    "    \n",
    "    if image_data.shape[1]==1:\n",
    "        image_data = np.tile(image_data, [1,3,1,1])\n",
    "    else:\n",
    "        # RGB to BGR\n",
    "        image_data = image_data[:,-1::,:,:]   \n",
    "    mean_bgr = np.expand_dims(np.array([104.00699, 116.66877, 122.67892]), [0,2,3])\n",
    "    dat = image_data * 255 - mean_bgr\n",
    "    dat = torch.tensor(dat, dtype=torch.float32).to(device)\n",
    "\n",
    "    return dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "87cac39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as I\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import numpy_utils, torch_utils, texture_utils\n",
    "\n",
    "def _cofactor_fn_cpu(_x, lambdas):\n",
    "    '''\n",
    "    Generating a matrix needed to solve ridge regression model for each lambda value.\n",
    "    Ridge regression (Tikhonov) solution is :\n",
    "    w = (X^T*X + I*lambda)^-1 * X^T * Y\n",
    "    This func will return (X^T*X + I*lambda)^-1 * X^T. \n",
    "    So once we have that, can just multiply by training data (Y) to get weights.\n",
    "    returned size is [nLambdas x nFeatures x nTrials]\n",
    "    This version makes sure that the torch inverse operation is done on the cpu, and in floating point-64 precision.\n",
    "    Otherwise get bad results for small lambda values. This seems to be a torch-specific bug.\n",
    "    \n",
    "    '''\n",
    "    device_orig = _x.device\n",
    "    type_orig = _x.dtype\n",
    "    # switch to this specific format which works with inverse\n",
    "    _x = _x.to('cpu').to(torch.float64)\n",
    "    _f = torch.stack([(torch.mm(torch.t(_x), _x) + torch.eye(_x.size()[1], device='cpu', dtype=torch.float64) * l).inverse() for l in lambdas], axis=0) \n",
    "    \n",
    "    # [#lambdas, #feature, #feature] \n",
    "    cof = torch.tensordot(_f, _x, dims=[[2],[1]]) # [#lambdas, #feature, #sample]\n",
    "    \n",
    "    # put back to whatever way it was before, so that we can continue with other operations as usual\n",
    "    return cof.to(device_orig).to(type_orig)\n",
    "\n",
    "\n",
    "\n",
    "def _loss_fn(_cofactor, _vtrn, _xout, _vout):\n",
    "    '''\n",
    "    Calculate loss given \"cofactor\" from cofactor_fn, training data, held-out design matrix, held out data.\n",
    "    returns weights (betas) based on equation\n",
    "    w = (X^T*X + I*lambda)^-1 * X^T * Y\n",
    "    also returns loss for these weights w the held out data. SSE is loss func here.\n",
    "    '''\n",
    "\n",
    "    _beta = torch.tensordot(_cofactor, _vtrn, dims=[[2], [0]]) # [#lambdas, #feature, #voxel]\n",
    "    _pred = torch.tensordot(_xout, _beta, dims=[[1],[1]]) # [#samples, #lambdas, #voxels]\n",
    "    _loss = torch.sum(torch.pow(_vout[:,None,:] - _pred, 2), dim=0) # [#lambdas, #voxels]\n",
    "    return _beta, _loss\n",
    "\n",
    "\n",
    "def fit_fwrf_model(images, voxel_data, _feature_extractor, prf_models, lambdas, \\\n",
    "                   zscore=False, add_bias=False, voxel_batch_size=100, holdout_size=100, \\\n",
    "                       shuffle=True, shuff_rnd_seed=0, device=None, do_varpart=False, debug=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Solve for encoding model weights using ridge regression.\n",
    "    Inputs:\n",
    "        images: the training images, [n_trials x 1 x height x width]\n",
    "        voxel_data: the training voxel data, [n_trials x n_voxels]\n",
    "        _feature_extractor_fn: module that maps from images to model features\n",
    "        prf_models: the list of possible pRFs to test, columns are [x, y, sigma]\n",
    "        lambdas: ridge lambda parameters to test\n",
    "        zscore: want to zscore each column of feature matrix before fitting?\n",
    "        add_bias: add a column of ones to feature matrix, for an additive bias?\n",
    "        voxel_batch_size: how many voxels to use at a time for model fitting\n",
    "        holdout_size: how many training trials to hold out for computing loss/lambda selection?\n",
    "        shuffle: do we shuffle training data order before holding trials out?      \n",
    "        shuff_rnd_seed: if we do shuffle training data (shuffle=True), what random seed to use? if zero, choose a new random seed in this code.\n",
    "        device: what device to use? cpu/cuda\n",
    "        do_varpart: perform variance partition by leaving out subsets of features at a time?\n",
    "        debug: want to run a shortened version of this, to test it?\n",
    "    Outputs:\n",
    "        best_losses: loss value for each voxel (with best pRF and best lambda), eval on held out set\n",
    "        best_lambdas: best lambda for each voxel (chosen based on loss w held out set)\n",
    "        best_params: \n",
    "            [0] best pRF for each voxel [x,y,sigma]\n",
    "            [1] best weights for each voxel/feature\n",
    "            [2] if add_bias=True, best bias value for each voxel\n",
    "            [3] if zscore=True, the mean of each feature before z-score\n",
    "            [4] if zscore=True, the std of each feature before z-score\n",
    "            [5] index of the best pRF for each voxel (i.e. index of row in \"prf_models\")\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    dtype = images.dtype.type\n",
    "    if device is None:\n",
    "        device=torch.device('cpu:0')\n",
    "\n",
    "    print ('dtype = %s' % dtype)\n",
    "    print ('device = %s' % device)\n",
    "\n",
    "    n_trials = len(images)\n",
    "    n_prfs = len(prf_models)\n",
    "    n_voxels = voxel_data.shape[1]   \n",
    "\n",
    "    # Get train/holdout splits.\n",
    "    # Held-out data here is used for lamdba selection.\n",
    "    # This is the inner part of nested cross-validation; there is another portion of data ('val') which never enters this function.\n",
    "    trn_size = n_trials - holdout_size\n",
    "    assert trn_size>0, 'Training size needs to be greater than zero'\n",
    "    print ('trn_size = %d (%.1f%%)' % (trn_size, float(trn_size)*100/len(voxel_data)))\n",
    "    order = np.arange(len(voxel_data), dtype=int)\n",
    "    if shuffle:\n",
    "        if shuff_rnd_seed==0:\n",
    "            print('Computing a new random seed')\n",
    "            shuff_rnd_seed = int(time.strftime('%M%H%d', time.localtime()))\n",
    "        print('Seeding random number generator: seed is %d'%shuff_rnd_seed)\n",
    "        np.random.seed(shuff_rnd_seed)\n",
    "        np.random.shuffle(order)\n",
    "    images = images[order]\n",
    "    voxel_data = voxel_data[order]  \n",
    "    trn_data = voxel_data[:trn_size]\n",
    "    out_data = voxel_data[trn_size:]\n",
    "\n",
    "    \n",
    "    # Here is where any model-specific additional initialization steps are done\n",
    "    # Includes initializing pca params arrays, if doing pca\n",
    "    _feature_extractor.init_for_fitting(images.shape[2:4], prf_models, dtype)\n",
    "    max_features = _feature_extractor.max_features\n",
    "\n",
    "    # Decide whether to do any \"partial\" versions of the models (leaving out subsets of features)\n",
    "    # Purpose is for variance partition\n",
    "    masks, partial_version_names = _feature_extractor.get_partial_versions()\n",
    "    n_partial_versions = len(partial_version_names) # will be one if skipping varpart\n",
    "    if add_bias:\n",
    "        masks = np.concatenate([masks, np.ones([masks.shape[0],1])], axis=1) # always include intercept \n",
    "    masks = np.transpose(masks)\n",
    "    # masks is [n_features_total (including intercept) x n_partial_versions]\n",
    "\n",
    "    # Initialize arrays to store model fitting params\n",
    "    best_w_params = np.zeros(shape=(n_voxels, max_features ,n_partial_versions), dtype=dtype)\n",
    "    best_prf_models = np.full(shape=(n_voxels,n_partial_versions), fill_value=-1, dtype=int)   \n",
    "    best_lambdas = np.full(shape=(n_voxels,n_partial_versions), fill_value=-1, dtype=int)\n",
    "    best_losses = np.full(fill_value=np.inf, shape=(n_voxels,n_partial_versions), dtype=dtype)\n",
    "\n",
    "    # Additional params that are optional\n",
    "    if add_bias:\n",
    "        best_w_params = np.concatenate([best_w_params, np.ones(shape=(n_voxels,1,n_partial_versions), dtype=dtype)], axis=1)\n",
    "\n",
    "    if zscore:\n",
    "        features_mean = np.zeros(shape=(n_voxels, max_features), dtype=dtype)\n",
    "        features_std  = np.zeros(shape=(n_voxels, max_features), dtype=dtype)\n",
    "    else:\n",
    "        features_mean = None\n",
    "        features_std = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    vox_loop_time = 0\n",
    "\n",
    "    print ('---------------------------------------\\n')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Looping over prf_models (here prf_models are different spatial RF definitions)\n",
    "        for m,(x,y,sigma) in enumerate(prf_models):\n",
    "            if debug and m>1:\n",
    "                break\n",
    "                \n",
    "            print('\\nGetting features for prf %d: [x,y,sigma] is [%.2f %.2f %.4f]'%(m, prf_models[m,0],  prf_models[m,1],  prf_models[m,2]))\n",
    "\n",
    "            t = time.time()            \n",
    "\n",
    "            # Get features for the desired pRF, across all trn set image            \n",
    "            features, feature_inds_defined = _feature_extractor(images, (x,y,sigma), m, fitting_mode=True)\n",
    "            features = features.detach().cpu().numpy() \n",
    "            \n",
    "            elapsed = time.time() - t\n",
    "\n",
    "            n_features_actual = features.shape[1]\n",
    "            \n",
    "            if zscore:  \n",
    "                features_m = np.mean(features, axis=0, keepdims=True) #[:trn_size]\n",
    "                features_s = np.std(features, axis=0, keepdims=True) + 1e-6          \n",
    "                features -= features_m\n",
    "                features /= features_s    \n",
    "\n",
    "            if add_bias:\n",
    "                features = np.concatenate([features, np.ones(shape=(len(features), 1), dtype=dtype)], axis=1)\n",
    "                feature_inds_defined = np.concatenate((feature_inds_defined, [True]), axis=0)\n",
    "                \n",
    "            trn_features = features[:trn_size,:]\n",
    "            out_features = features[trn_size:,:]\n",
    "            \n",
    "            \n",
    "            # Going to keep track of whether current prf is better than running best, for each voxel.\n",
    "            # This is for the full model only.\n",
    "            # Will use this to make sure for each partial model, we end up saving the params for the prf that was best w full model.\n",
    "            full_model_improved = np.zeros((n_voxels,),dtype=bool)\n",
    "\n",
    "            # Looping over versions of model w different features set to zero (variance partition)\n",
    "            for pp in range(n_partial_versions):\n",
    "\n",
    "                print('\\nFitting version %d of %d: %s, '%(pp, n_partial_versions, partial_version_names[pp]))\n",
    "\n",
    "                nonzero_inds_full = np.logical_and(masks[:,pp], feature_inds_defined)             \n",
    "               \n",
    "                # Send matrices to gpu\n",
    "                nonzero_inds_short = masks[feature_inds_defined,pp]==1\n",
    "                _xtrn = torch_utils._to_torch(trn_features[:, nonzero_inds_short], device=device)\n",
    "                _xout = torch_utils._to_torch(out_features[:, nonzero_inds_short], device=device)   \n",
    "\n",
    "                # Do part of the matrix math involved in ridge regression optimization out of the loop, \n",
    "                # because this part will be same for all the voxels.\n",
    "                _cof = _cofactor_fn_cpu(_xtrn, lambdas = lambdas) \n",
    "\n",
    "                # Now looping over batches of voxels (only reason is because can't store all in memory at same time)\n",
    "                vox_start = time.time()\n",
    "                vi=-1\n",
    "                for rv,lv in numpy_utils.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "                    vi=vi+1\n",
    "                    sys.stdout.write('\\rfitting model %4d of %-4d, voxels [%6d:%-6d] of %d' % (m, n_prfs, rv[0], rv[-1], n_voxels))\n",
    "\n",
    "                    # Send matrices to gpu\n",
    "                    _vtrn = torch_utils._to_torch(trn_data[:,rv], device=device)\n",
    "                    _vout = torch_utils._to_torch(out_data[:,rv], device=device)\n",
    "\n",
    "                    # Here is where optimization happens - relatively simple matrix math inside loss fn.\n",
    "                    _betas, _loss = _loss_fn(_cof, _vtrn, _xout, _vout) #   [#lambda, #feature, #voxel, ], [#lambda, #voxel]\n",
    "                    # Now have a set of weights (in betas) and a loss value for every voxel and every lambda. \n",
    "                    # goal is then to choose for each voxel, what is the best lambda and what weights went with that lambda.\n",
    "\n",
    "                    # choose best lambda value and the loss that went with it.\n",
    "                    _loss_values, _lambda_index = torch.min(_loss, dim=0)\n",
    "                    loss_values, lambda_index = torch_utils.get_value(_loss_values), torch_utils.get_value(_lambda_index)\n",
    "                    betas = torch_utils.get_value(_betas)\n",
    "\n",
    "\n",
    "                    if pp==0:\n",
    "\n",
    "                        # comparing this loss to the other prf_models for each voxel (e.g. the other RF position/sizes)\n",
    "                        assert(partial_version_names[pp]=='full_model' or partial_version_names[pp]=='full_combined_model')               \n",
    "                        imp = loss_values<best_losses[rv,pp]\n",
    "                        full_model_improved[rv] = imp\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # for the partial models we don't actually care which was best for the partial model itself,\n",
    "                        # just care what was best for the full model\n",
    "                        imp = full_model_improved[rv]\n",
    "\n",
    "\n",
    "                    if np.sum(imp)>0:\n",
    "\n",
    "                        # for whichever voxels had improvement relative to previous prf_models, save parameters now\n",
    "                        # this means we won't have to save all params for all prf_models, just best.\n",
    "                        arv = np.array(rv)[imp]\n",
    "\n",
    "                        lambda_inds = lambda_index[imp]\n",
    "                        best_lambdas[arv,pp] = lambda_inds\n",
    "                        best_losses[arv,pp] = loss_values[imp]\n",
    "                        best_prf_models[arv,pp] = m\n",
    "                        if zscore:\n",
    "                            \n",
    "                            fmean_tmp = copy.deepcopy(features_mean[arv,:])\n",
    "                            fstd_tmp = copy.deepcopy(features_std[arv,:])\n",
    "                            fmean_tmp[:,nonzero_inds_full[0:-1]] = features_m[0,nonzero_inds_short[0:-1]] # broadcast over updated voxels\n",
    "                            fmean_tmp[:,~nonzero_inds_full[0:-1]] = 0.0\n",
    "                            fstd_tmp[:,nonzero_inds_full[0:-1]] = features_s[0,nonzero_inds_short[0:-1]] # broadcast over updated voxels\n",
    "                            fstd_tmp[:,~nonzero_inds_full[0:-1]] = 0.0\n",
    "                            features_mean[arv,:] = fmean_tmp\n",
    "                            features_std[arv,:] = fstd_tmp\n",
    "                            \n",
    "                        # taking the weights associated with the best lambda value\n",
    "                        # remember that they won't fill entire matrix, rest of values stay at zero\n",
    "                        best_w_tmp = copy.deepcopy(best_w_params[arv,:,pp])\n",
    "                        best_w_tmp[:,nonzero_inds_full] = numpy_utils.select_along_axis(betas[:,:,imp], lambda_inds, run_axis=2, choice_axis=0).T\n",
    "                        best_w_tmp[:,~nonzero_inds_full] = 0.0 # make sure to fill zeros here\n",
    "\n",
    "#                         # bias is always last value, even if zeros for some other features\n",
    "#                         if add_bias:\n",
    "#                             best_w_tmp[:,-1] = numpy_utils.select_along_axis(betas[:,-1,imp], lambda_inds, run_axis=1, choice_axis=0).T\n",
    "\n",
    "                        best_w_params[arv,:,pp] = best_w_tmp\n",
    "                \n",
    "                vox_loop_time += (time.time() - vox_start)\n",
    "                elapsed = (time.time() - vox_start)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    # Print information about how fitting went...\n",
    "    total_time = time.time() - start_time\n",
    "    inv_time = total_time - vox_loop_time\n",
    "    return_params = [best_w_params[:,0:max_features,:],]\n",
    "    if add_bias:\n",
    "        return_params += [best_w_params[:,-1,:],]\n",
    "    else: \n",
    "        return_params += [None,]\n",
    "    print ('\\n---------------------------------------')\n",
    "    print ('total time = %fs' % total_time)\n",
    "    print ('total throughput = %fs/voxel' % (total_time / n_voxels))\n",
    "    print ('voxel throughput = %fs/voxel' % (vox_loop_time / n_voxels))\n",
    "    print ('setup throughput = %fs/model' % (inv_time / n_prfs))\n",
    "    \n",
    "    # This step clears the big feature maps for training data from feature extractor (no longer needed)\n",
    "    _feature_extractor.clear_maps()\n",
    "    \n",
    "    best_params = [prf_models[best_prf_models],]+return_params+[features_mean, features_std]+[best_prf_models]\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    return best_losses, best_lambdas, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "9f8a4d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype = <class 'numpy.float32'>\n",
      "device = cuda:0\n",
      "trn_size = 588 (85.5%)\n",
      "Seeding random number generator: seed is 842348\n",
      "Initializing for fitting: finding the size of feature matrix for each candidate prf\n",
      "Initializing arrays for PCA params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmhender/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:80: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing BDCN contour features from memory.\n",
      "Clearing steerable pyramid features from memory.\n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "Getting features for prf 0: [x,y,sigma] is [-0.55 -0.55 0.0400]\n",
      "Running BDCN contour feature extraction...\n",
      "Images array shape is:\n",
      "(688, 1, 240, 240)\n",
      "Final array shape is:\n",
      "torch.Size([688, 1, 240, 240])\n",
      "time elapsed = 7.65913\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.55, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 9]\n",
      "[min max] of first image patch is:\n",
      "[tensor(0.0173, device='cuda:0'), tensor(0.3324, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(688, 72)\n",
      "Running PCA...\n",
      "Retaining 9 components to expl 99 pct var\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "(688, 1, 240, 240)\n",
      "time elapsed = 136.86514\n",
      "Computing higher order correlations...\n",
      "time elapsed = 7.37654\n",
      "Final size of features concatenated is [688 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "\n",
      "Fitting version 0 of 7: full_combined_model, \n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 1 of 7: just_bdcn, \n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 2 of 7: just_texture, \n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 3 of 7: texture_just_mean_magnitudes_no_other_modules, \n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 4 of 7: texture_just_all_other_texture_feats_no_other_modules, \n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 5 of 7: texture_just_mean_magnitudes_plus_other_modules, \n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 6 of 7: texture_just_all_other_texture_feats_plus_other_modules, \n",
      "fitting model    0 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Getting features for prf 1: [x,y,sigma] is [-0.49 -0.55 0.0400]\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.49210526315789477, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 22]\n",
      "[min max] of first image patch is:\n",
      "[tensor(0.0103, device='cuda:0'), tensor(0.3292, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(688, 176)\n",
      "Running PCA...\n",
      "Retaining 14 components to expl 99 pct var\n",
      "Computing higher order correlations...\n",
      "time elapsed = 6.53542\n",
      "Final size of features concatenated is [688 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "\n",
      "Fitting version 0 of 7: full_combined_model, \n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 1 of 7: just_bdcn, \n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 2 of 7: just_texture, \n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 3 of 7: texture_just_mean_magnitudes_no_other_modules, \n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 4 of 7: texture_just_all_other_texture_feats_no_other_modules, \n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 5 of 7: texture_just_mean_magnitudes_plus_other_modules, \n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "Fitting version 6 of 7: texture_just_all_other_texture_feats_plus_other_modules, \n",
      "fitting model    1 of 875 , voxels [ 14900:14912 ] of 14913\n",
      "---------------------------------------\n",
      "total time = 171.939200s\n",
      "total throughput = 0.011529s/voxel\n",
      "voxel throughput = 0.000640s/voxel\n",
      "setup throughput = 0.185594s/model\n",
      "Clearing BDCN contour features from memory.\n",
      "Clearing steerable pyramid features from memory.\n"
     ]
    }
   ],
   "source": [
    "images = trn_stim_data\n",
    "voxel_data = trn_voxel_data\n",
    "prf_models = models\n",
    "zscore_features = True\n",
    "add_bias = True\n",
    "voxel_batch_size=100\n",
    "holdout_size=100\n",
    "shuffle=True\n",
    "shuff_rnd_seed = 842348\n",
    "do_varpart=True\n",
    "debug=True\n",
    "\n",
    "best_losses, best_lambdas, best_params = fit_fwrf_model(images, voxel_data, _feature_extractor, prf_models, lambdas, \\\n",
    "                   zscore=zscore_features, add_bias=add_bias, voxel_batch_size=voxel_batch_size, holdout_size=holdout_size, \\\n",
    "                       shuffle=shuffle, shuff_rnd_seed=shuff_rnd_seed, device=device, do_varpart=do_varpart, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "aa8d8f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14913, 1041)"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "21410554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5106623 , 0.17976591, 0.13310748, 0.08246855, 0.06214282,\n",
       "       0.0578819 , 0.05178565, 0.03472991, 0.0284207 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params[4][0,0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "0e5329be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.9986999e-04,  3.4265858e-04, -6.4414664e-05,  2.8409972e-04,\n",
       "       -4.5182815e-04,  7.2751415e-04,  3.7275028e-04,  2.1712780e-03,\n",
       "        7.2326441e-04,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params[1][0,0:20,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "c1e9257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import numpy_utils, torch_utils\n",
    "\n",
    "\n",
    "def get_r2(actual,predicted):\n",
    "  \n",
    "    \"\"\"\n",
    "    This computes the coefficient of determination (R2).\n",
    "    For OLS, this is a good measure of variance explained. \n",
    "    Not necessarily true for ridge regression - can use signed correlation coefficient^2 instead.\n",
    "    With OLS & when train/test sets are identical, R2 = correlation coefficient^2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate r2 for this fit.\n",
    "    ssres = np.sum(np.power((predicted - actual),2));\n",
    "    sstot = np.sum(np.power((actual - np.mean(actual)),2));\n",
    "    r2 = 1-(ssres/sstot)\n",
    "    \n",
    "    return r2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "ad557ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = val_stim_data\n",
    "voxel_data = val_voxel_data\n",
    "\n",
    "params = best_params\n",
    "dtype = images.dtype.type\n",
    "device = _texture_fn.device\n",
    "\n",
    "n_trials, n_voxels = len(images), len(params[0])\n",
    "n_prfs = prf_models.shape[0]\n",
    "n_features = params[1].shape[1]  \n",
    "n_voxels = np.shape(voxel_data)[1]\n",
    "\n",
    "best_models, weights, bias, features_mt, features_st, best_model_inds = params\n",
    "masks, partial_version_names = _feature_extractor.get_partial_versions()\n",
    "masks = np.transpose(masks)    \n",
    "n_features_max = _feature_extractor.max_features\n",
    "n_partial_versions = len(partial_version_names)\n",
    "\n",
    "# val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "val_cc  = np.zeros(shape=(n_voxels, n_partial_versions), dtype=dtype)\n",
    "val_r2 = np.zeros(shape=(n_voxels, n_partial_versions), dtype=dtype)\n",
    "\n",
    "pred_models = np.full(fill_value=0, shape=(n_trials, n_features_max, n_prfs), dtype=dtype)\n",
    "feature_inds_defined_each_prf = np.full(fill_value=0, shape=(n_features_max, n_prfs), dtype=bool)\n",
    "\n",
    "start_time = time.time()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "0cd9bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing BDCN contour features from memory.\n",
      "Clearing steerable pyramid features from memory.\n",
      "Getting features for prf 0: [x,y,sigma] is [-0.55 -0.55 0.0400]\n",
      "Running BDCN contour feature extraction...\n",
      "Images array shape is:\n",
      "(62, 1, 240, 240)\n",
      "Final array shape is:\n",
      "torch.Size([62, 1, 240, 240])\n",
      "time elapsed = 0.64549\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.55, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 9]\n",
      "[min max] of first image patch is:\n",
      "[tensor(0.0058, device='cuda:0'), tensor(0.3294, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(62, 72)\n",
      "Applying pre-computed PCA matrix...\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "(62, 1, 240, 240)\n",
      "time elapsed = 10.96169\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.48873\n",
      "Final size of features concatenated is [62 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "Getting features for prf 1: [x,y,sigma] is [-0.49 -0.55 0.0400]\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.49210526315789477, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 22]\n",
      "[min max] of first image patch is:\n",
      "[tensor(0.0082, device='cuda:0'), tensor(0.3262, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(62, 176)\n",
      "Applying pre-computed PCA matrix...\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.47852\n",
      "Final size of features concatenated is [62 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "Clearing BDCN contour features from memory.\n",
      "Clearing steerable pyramid features from memory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_feature_extractor.clear_maps()\n",
    "\n",
    "for mm in range(n_prfs):\n",
    "    if mm>1 and debug:\n",
    "        break\n",
    "    print('Getting features for prf %d: [x,y,sigma] is [%.2f %.2f %.4f]'%(mm, prf_models[mm,0],  prf_models[mm,1],  prf_models[mm,2] ))\n",
    "    all_feat_concat, feature_inds_defined = _feature_extractor(images, prf_models[mm,:], mm, fitting_mode=False)\n",
    "\n",
    "    pred_models[:,feature_inds_defined,mm] = torch_utils.get_value(all_feat_concat)\n",
    "    feature_inds_defined_each_prf[:,mm] = feature_inds_defined\n",
    "\n",
    "_feature_extractor.clear_maps()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "id": "d04ffea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "8162dba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1041,)"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "43b47d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(features_to_use & feature_inds_defined_each_prf[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "id": "ce58a388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_defined = feature_inds_defined_each_prf[features_to_use,0]\n",
    "np.sum(is_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "ba835879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1041, 875)"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_inds_defined_each_prf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "ef51592e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feature_inds_defined_each_prf[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "1e225b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_full = pred_models[:,:,best_model_inds[rv,pp]]\n",
    "# Note there may be some zeros in this matrix, if we used fewer than the max number of features.\n",
    "# But they are zero in weight matrix too, so turns out ok.\n",
    "\n",
    "_weights = torch_utils._to_torch(weights[rv,:,pp], device=device)   \n",
    "\n",
    "_bias = torch_utils._to_torch(bias[rv,pp], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "549d68e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 1041, 100)"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "3e1c711d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1041])"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "dd6c4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if features_mt is not None:\n",
    "    _features_m = torch_utils._to_torch(features_mt[rv,:], device=device)\n",
    "    _features_m = _features_m[:,feature_inds_defined_each_prf[:,pp]]\n",
    "if features_st is not None:\n",
    "    _features_s = torch_utils._to_torch(features_st[rv,:], device=device)\n",
    "    _features_s = _features_s[:,feature_inds_defined_each_prf[:,pp]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "f3fc8a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating version 3 of 7: texture_just_mean_magnitudes_no_other_modules\n",
      "Includes 16 features\n",
      "number of zeros:\n",
      "0\n",
      "size of weights is:\n",
      "torch.Size([100, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rv = range(100)\n",
    "lv=100\n",
    "pp=3\n",
    "# vv=-1\n",
    "# ## Looping over voxels here in batches, will eventually go through all.\n",
    "# for rv, lv in numpy_utils.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "#     vv=vv+1\n",
    "#     print('Getting predictions for voxels [%d-%d] of %d'%(rv[0],rv[-1],n_voxels))\n",
    "\n",
    "#     if vv>1 and debug:\n",
    "#         break\n",
    "\n",
    "#     # Looping over versions of model w different features set to zero (variance partition)\n",
    "#     for pp in range(n_partial_versions):\n",
    "\n",
    "#         nonzero_inds_full = np.logical_and(masks[:,pp], feature_inds_defined[:,mm])             \n",
    "is_defined = feature_inds_defined_each_prf[features_to_use,mm]\n",
    "\n",
    "print('\\nEvaluating version %d of %d: %s'%(pp, n_partial_versions, partial_version_names[pp]))\n",
    "\n",
    "features_to_use = masks[:,pp]==1\n",
    "print('Includes %d features'%np.sum(features_to_use))\n",
    "\n",
    "# [trials x features x voxels]\n",
    "features_full = pred_models[:,:,best_model_inds[rv,pp]]\n",
    "# Take out the relevant features now\n",
    "features_full = features_full[:,features_to_use,:]\n",
    "# Note there may be some zeros in this matrix, if we used fewer than the max number of features.\n",
    "# But they are zero in weight matrix too, so turns out ok.\n",
    "\n",
    "_weights = torch_utils._to_torch(weights[rv,:,pp], device=device)   \n",
    "_weights = _weights[:, features_to_use]\n",
    "_bias = torch_utils._to_torch(bias[rv,pp], device=device)\n",
    "\n",
    "print('number of zeros:')\n",
    "print(np.sum(features_full[0,:,0]==0))\n",
    "\n",
    "print('size of weights is:')\n",
    "print(_weights.shape)\n",
    "\n",
    "if features_mt is not None:\n",
    "    _features_m = torch_utils._to_torch(features_mt[rv,:], device=device)\n",
    "    _features_m = _features_m[:,features_to_use]\n",
    "if features_st is not None:\n",
    "    _features_s = torch_utils._to_torch(features_st[rv,:], device=device)\n",
    "    _features_s = _features_s[:,features_to_use]\n",
    "\n",
    "pred_block = np.full(fill_value=0, shape=(n_trials, lv), dtype=dtype)\n",
    "\n",
    "# Now looping over validation set trials in batches\n",
    "for rt, lt in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "\n",
    "    _features = torch_utils._to_torch(features_full[rt,:], device=device) # trials x features\n",
    "    if features_mt is not None:    \n",
    "        # features_m is [nvoxels x nfeatures] - need [trials x features x voxels]\n",
    "        _features = _features - torch.tile(torch.unsqueeze(_features_m, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "\n",
    "    if features_st is not None:\n",
    "        _features = _features/torch.tile(torch.unsqueeze(_features_s, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "        _features[torch.isnan(_features)] = 0.0 # this applies in the pca case when last few columns of features are missing\n",
    "        _features[torch.isinf(_features)] = 0.0\n",
    "        \n",
    "    # features is [#samples, #features, #voxels] - swap dims to [#voxels, #samples, features]\n",
    "    _features = torch.transpose(torch.transpose(_features, 0, 2), 1, 2)\n",
    "    # weights is [#voxels, #features]\n",
    "    # _r will be [#voxels, #samples, 1] - then [#samples, #voxels]\n",
    "\n",
    "    _r = torch.squeeze(torch.bmm(_features, torch.unsqueeze(_weights, 2)), dim=2).t() \n",
    "\n",
    "    if _bias is not None:\n",
    "        _r = _r + torch.tile(torch.unsqueeze(_bias, 0), [_r.shape[0],1])\n",
    "\n",
    "    pred_block[rt] = torch_utils.get_value(_r) \n",
    "\n",
    "# Now for this batch of voxels and this partial version of the model, measure performance.\n",
    "#                 print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "for vi in range(lv):   \n",
    "    val_cc[rv[vi],pp] = np.corrcoef(voxel_data[:,rv[vi]], pred_block[:,vi])[0,1]  \n",
    "    val_r2[rv[vi],pp] = get_r2(voxel_data[:,rv[vi]], pred_block[:,vi])\n",
    "\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "id": "1670c781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0001, -0.0002,  0.0127,  ..., -0.0003,  0.0088,  0.0046],\n",
       "        [-0.0001, -0.0002,  0.0127,  ..., -0.0003,  0.0088,  0.0046],\n",
       "        [-0.0001, -0.0002,  0.0127,  ..., -0.0003,  0.0088,  0.0046],\n",
       "        ...,\n",
       "        [-0.0001, -0.0002,  0.0127,  ..., -0.0003,  0.0088,  0.0046],\n",
       "        [-0.0001, -0.0002,  0.0127,  ..., -0.0003,  0.0088,  0.0046],\n",
       "        [-0.0001, -0.0002,  0.0127,  ..., -0.0003,  0.0088,  0.0046]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "bd119705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 1002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "6842f109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., nan],\n",
       "       [nan, nan]])"
      ]
     },
     "execution_count": 992,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(voxel_data[:,rv[vi]], pred_block[:,vi])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "45d6b52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699, 0.00455699, 0.00455699, 0.00455699,\n",
       "       0.00455699, 0.00455699], dtype=float32)"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_block[:,vi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "4417f95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57709819, -0.68539327,  1.24508476,  1.55866957, -0.78992164,\n",
       "        0.55576277,  0.2402944 , -0.44996917,  0.69701743, -0.45656109,\n",
       "        0.49361086,  0.09621488,  0.13764949, -0.5610894 , -0.14203432,\n",
       "        1.0124855 ,  1.45037448,  0.21863545,  1.28651929,  0.41921684,\n",
       "        0.90701562, -0.24373759,  0.71867621,  0.40226632, -0.27198848,\n",
       "       -0.22207853,  0.05383851,  0.48513556,  1.1433816 , -0.91516733,\n",
       "       -0.21737012, -1.19579279,  0.63580698,  0.25347826,  0.23935276,\n",
       "        1.8590709 , -0.28046378, -0.44243559,  0.8797065 , -0.28328881,\n",
       "       -0.46786144, -0.47162822,  1.00118518,  0.3485896 ,  1.34302104,\n",
       "       -0.06293189,  0.98705995, -0.20041959, -1.86439741,  0.33163908,\n",
       "        0.36554012, -0.14674288, -0.08082404, -2.81551099, -1.78435326,\n",
       "       -0.4979957 ,  2.39583826,  0.12823254,  1.42494869, -0.52812999,\n",
       "       -1.26736188,  0.39849955])"
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxel_data[:,rv[vi]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "c446a973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([407, 407, 407, 407, 407, 402, 407, 407, 407, 407, 407, 402, 402, 402,\n",
       "        402, 402, 402, 407, 407, 402, 407, 407, 402, 402, 407, 402, 402, 407,\n",
       "        407, 402, 402, 402, 402, 407, 407, 407, 407, 407, 407, 407, 407, 407,\n",
       "        407, 407, 407, 402, 407, 402, 407, 402, 402, 402, 407, 407, 407, 407,\n",
       "        402, 402, 407, 402, 402, 402, 402, 407, 402, 407, 402, 402, 407, 407,\n",
       "        407, 407, 402, 407, 407, 402, 407, 407, 407, 402, 407, 407, 407, 407,\n",
       "        407, 407, 407, 407, 402, 402, 407, 402, 407, 407, 407, 407, 407, 407,\n",
       "        402, 402], device='cuda:0')"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(_features_s==0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "d7ac01d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(is_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "d5a169be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "6bc033f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2392195e-01, -2.2392195e-01, -2.2392195e-01, ...,\n",
       "        -2.2392195e-01, -3.9323947e-01, -3.9323947e-01],\n",
       "       [ 1.4081092e-01,  1.4081092e-01,  1.4081092e-01, ...,\n",
       "         1.4081092e-01,  7.1858428e-02,  7.1858428e-02],\n",
       "       [ 2.5447050e-02,  2.5447050e-02,  2.5447050e-02, ...,\n",
       "         2.5447050e-02, -8.5845679e-02, -8.5845679e-02],\n",
       "       ...,\n",
       "       [-4.5780726e-10, -4.5780726e-10, -4.5780726e-10, ...,\n",
       "        -4.5780726e-10, -1.7213719e-10, -1.7213719e-10],\n",
       "       [ 7.7769069e-10,  7.7769069e-10,  7.7769069e-10, ...,\n",
       "         7.7769069e-10,  1.5812712e-11,  1.5812712e-11],\n",
       "       [-9.0202068e-10, -9.0202068e-10, -9.0202068e-10, ...,\n",
       "        -9.0202068e-10, -5.1483917e-10, -5.1483917e-10]], dtype=float32)"
      ]
     },
     "execution_count": 941,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_full[0,0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "36bfca57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_defined = feature_inds_defined_each_prf[masks[:,pp],pp]==1\n",
    "is_defined.shape\n",
    "~is_defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "85a437aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_features = torch_utils._to_torch(features_full[rt,:], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "ecf7dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = _features[0,0,:]\n",
    "w = _weights[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "f9f2672b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.3932, -0.2239, -0.2239,\n",
       "        -0.2239, -0.2239, -0.2239, -0.3932, -0.3932, -0.3932, -0.3932, -0.3932,\n",
       "        -0.3932, -0.2239, -0.2239, -0.3932, -0.2239, -0.2239, -0.3932, -0.3932,\n",
       "        -0.2239, -0.3932, -0.3932, -0.2239, -0.2239, -0.3932, -0.3932, -0.3932,\n",
       "        -0.3932, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239,\n",
       "        -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.3932, -0.2239, -0.3932,\n",
       "        -0.2239, -0.3932, -0.3932, -0.3932, -0.2239, -0.2239, -0.2239, -0.2239,\n",
       "        -0.3932, -0.3932, -0.2239, -0.3932, -0.3932, -0.3932, -0.3932, -0.2239,\n",
       "        -0.3932, -0.2239, -0.3932, -0.3932, -0.2239, -0.2239, -0.2239, -0.2239,\n",
       "        -0.3932, -0.2239, -0.2239, -0.3932, -0.2239, -0.2239, -0.2239, -0.3932,\n",
       "        -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239,\n",
       "        -0.3932, -0.3932, -0.2239, -0.3932, -0.2239, -0.2239, -0.2239, -0.2239,\n",
       "        -0.2239, -0.2239, -0.3932, -0.3932], device='cuda:0')"
      ]
     },
     "execution_count": 939,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "6d463fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.9987e-04,  3.4266e-04, -6.4415e-05,  ...,  7.1773e-04,\n",
       "         3.4391e-04,  5.9923e-04], device='cuda:0')"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "6635d0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = f*w\n",
    "torch.any(torch.isnan(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "e7d95a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isinf(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "20727c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0')"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "ecf83ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 62, 1041])"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "e808a077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1041, 1])"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(_weights,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "0e55eb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1396755 ,  0.0279291 , -0.14319251,  0.        , -0.14238772,\n",
       "         0.0430938 , -0.13888815],\n",
       "       [-0.03079309,  0.13824493, -0.05539858,  0.        , -0.0558647 ,\n",
       "         0.15264574, -0.03141689],\n",
       "       [ 0.33008984,  0.130216  ,  0.32965907,  0.        ,  0.33155468,\n",
       "         0.14273554,  0.33203235],\n",
       "       [ 0.15855937, -0.1713802 ,  0.16552094,  0.        ,  0.16560157,\n",
       "        -0.17134535,  0.15864831],\n",
       "       [ 0.0140794 ,  0.16797902,  0.00353686,  0.        ,  0.00367823,\n",
       "         0.16828535,  0.01411854],\n",
       "       [ 0.09249812,  0.00386856,  0.09289657,  0.        ,  0.09301239,\n",
       "         0.00457587,  0.09261369],\n",
       "       [ 0.15115827,  0.00928749,  0.15275912,  0.        ,  0.1525444 ,\n",
       "         0.00997545,  0.15098521],\n",
       "       [ 0.21069852, -0.06625794,  0.21573968,  0.        ,  0.21705034,\n",
       "        -0.06592955,  0.21206613],\n",
       "       [ 0.05650845,  0.08847162,  0.05080779,  0.        ,  0.05120433,\n",
       "         0.06917796,  0.05689257],\n",
       "       [ 0.09817851, -0.15583782,  0.10934305,  0.        ,  0.11956733,\n",
       "        -0.14462583,  0.09761912]], dtype=float32)"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_cc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "8e7fa421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing BDCN contour features from memory.\n",
      "Clearing steerable pyramid features from memory.\n",
      "Getting features for prf 0: [x,y,sigma] is [-0.55 -0.55 0.0400]\n",
      "Running BDCN contour feature extraction...\n",
      "Images array shape is:\n",
      "(62, 1, 240, 240)\n",
      "Final array shape is:\n",
      "torch.Size([62, 1, 240, 240])\n",
      "time elapsed = 0.64745\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.55, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 9]\n",
      "[min max] of first image patch is:\n",
      "[tensor(0.0058, device='cuda:0'), tensor(0.3294, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(62, 72)\n",
      "Applying pre-computed PCA matrix...\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "(62, 1, 240, 240)\n",
      "time elapsed = 11.12420\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.76572\n",
      "Final size of features concatenated is [62 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "Getting features for prf 1: [x,y,sigma] is [-0.49 -0.55 0.0400]\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.49210526315789477, -0.55, 0.03999999910593033]\n",
      "bbox to crop is:\n",
      "[232, 240, 0, 22]\n",
      "[min max] of first image patch is:\n",
      "[tensor(0.0082, device='cuda:0'), tensor(0.3262, device='cuda:0')]\n",
      "Preparing for PCA: original dims of features:\n",
      "(62, 176)\n",
      "Applying pre-computed PCA matrix...\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.47667\n",
      "Final size of features concatenated is [62 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "Clearing BDCN contour features from memory.\n",
      "Clearing steerable pyramid features from memory.\n",
      "Getting predictions for voxels [0-99] of 14913\n",
      "\n",
      "Evaluating version 0 of 7: full_combined_model\n",
      "Includes 1041 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 1041])\n",
      "\n",
      "Evaluating version 1 of 7: just_bdcn\n",
      "Includes 400 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 400])\n",
      "\n",
      "Evaluating version 2 of 7: just_texture\n",
      "Includes 641 features\n",
      "number of zeros:\n",
      "0\n",
      "size of weights is:\n",
      "torch.Size([100, 641])\n",
      "\n",
      "Evaluating version 3 of 7: texture_just_mean_magnitudes_no_other_modules\n",
      "Includes 16 features\n",
      "number of zeros:\n",
      "0\n",
      "size of weights is:\n",
      "torch.Size([100, 16])\n",
      "\n",
      "Evaluating version 4 of 7: texture_just_all_other_texture_feats_no_other_modules\n",
      "Includes 625 features\n",
      "number of zeros:\n",
      "0\n",
      "size of weights is:\n",
      "torch.Size([100, 625])\n",
      "\n",
      "Evaluating version 5 of 7: texture_just_mean_magnitudes_plus_other_modules\n",
      "Includes 416 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 416])\n",
      "\n",
      "Evaluating version 6 of 7: texture_just_all_other_texture_feats_plus_other_modules\n",
      "Includes 1025 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 1025])\n",
      "Getting predictions for voxels [100-199] of 14913\n",
      "\n",
      "Evaluating version 0 of 7: full_combined_model\n",
      "Includes 1041 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 1041])\n",
      "\n",
      "Evaluating version 1 of 7: just_bdcn\n",
      "Includes 400 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 400])\n",
      "\n",
      "Evaluating version 2 of 7: just_texture\n",
      "Includes 641 features\n",
      "number of zeros:\n",
      "0\n",
      "size of weights is:\n",
      "torch.Size([100, 641])\n",
      "\n",
      "Evaluating version 3 of 7: texture_just_mean_magnitudes_no_other_modules\n",
      "Includes 16 features\n",
      "number of zeros:\n",
      "0\n",
      "size of weights is:\n",
      "torch.Size([100, 16])\n",
      "\n",
      "Evaluating version 4 of 7: texture_just_all_other_texture_feats_no_other_modules\n",
      "Includes 625 features\n",
      "number of zeros:\n",
      "0\n",
      "size of weights is:\n",
      "torch.Size([100, 625])\n",
      "\n",
      "Evaluating version 5 of 7: texture_just_mean_magnitudes_plus_other_modules\n",
      "Includes 416 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 416])\n",
      "\n",
      "Evaluating version 6 of 7: texture_just_all_other_texture_feats_plus_other_modules\n",
      "Includes 1025 features\n",
      "number of zeros:\n",
      "391\n",
      "size of weights is:\n",
      "torch.Size([100, 1025])\n",
      "Getting predictions for voxels [200-299] of 14913\n"
     ]
    }
   ],
   "source": [
    "val_cc, val_r2 = validate_fwrf_model(best_params, models, val_voxel_data, val_stim_data, _feature_extractor, \\\n",
    "                                   sample_batch_size=100, voxel_batch_size=100, debug=True, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "fbb84c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fwrf_model(best_params, prf_models, voxel_data, images, _feature_extractor, \\\n",
    "                                   sample_batch_size=100, voxel_batch_size=100, debug=False, dtype=np.float32):\n",
    "    \n",
    "    \"\"\" \n",
    "    Evaluate trained model, leaving out a subset of features at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    params = best_params\n",
    "    dtype = images.dtype.type\n",
    "    device = _texture_fn.device\n",
    "    \n",
    "    n_trials, n_voxels = len(images), len(params[0])\n",
    "    n_prfs = prf_models.shape[0]\n",
    "    n_features = params[1].shape[1]  \n",
    "    n_voxels = np.shape(voxel_data)[1]\n",
    "\n",
    "    best_models, weights, bias, features_mt, features_st, best_model_inds = params\n",
    "    masks, partial_version_names = _feature_extractor.get_partial_versions()\n",
    "    masks = np.transpose(masks)    \n",
    "    n_features_max = _feature_extractor.max_features\n",
    "    n_partial_versions = len(partial_version_names)\n",
    "    \n",
    "    # val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "    val_cc  = np.zeros(shape=(n_voxels, n_partial_versions), dtype=dtype)\n",
    "    val_r2 = np.zeros(shape=(n_voxels, n_partial_versions), dtype=dtype)\n",
    "\n",
    "    pred_models = np.full(fill_value=0, shape=(n_trials, n_features_max, n_prfs), dtype=dtype)\n",
    "    feature_inds_defined_each_prf = np.full(fill_value=0, shape=(n_features_max, n_prfs), dtype=bool)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # First gather texture features for all pRFs.\n",
    "        \n",
    "        _feature_extractor.clear_maps()\n",
    "        \n",
    "        for mm in range(n_prfs):\n",
    "            if mm>1 and debug:\n",
    "                break\n",
    "            print('Getting features for prf %d: [x,y,sigma] is [%.2f %.2f %.4f]'%(mm, prf_models[mm,0],  prf_models[mm,1],  prf_models[mm,2] ))\n",
    "            all_feat_concat, feature_inds_defined = _feature_extractor(images, prf_models[mm,:], mm, fitting_mode=False)\n",
    "            \n",
    "            pred_models[:,feature_inds_defined,mm] = torch_utils.get_value(all_feat_concat)\n",
    "            feature_inds_defined_each_prf[:,mm] = feature_inds_defined\n",
    "            \n",
    "        _feature_extractor.clear_maps()\n",
    "        \n",
    "        vv=-1\n",
    "        ## Looping over voxels here in batches, will eventually go through all.\n",
    "        for rv, lv in numpy_utils.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "            vv=vv+1\n",
    "            print('Getting predictions for voxels [%d-%d] of %d'%(rv[0],rv[-1],n_voxels))\n",
    "\n",
    "            if vv>1 and debug:\n",
    "                break\n",
    "\n",
    "            # Looping over versions of model w different features set to zero (variance partition)\n",
    "            for pp in range(n_partial_versions):\n",
    "\n",
    "        #         nonzero_inds_full = np.logical_and(masks[:,pp], feature_inds_defined[:,mm])             \n",
    "        #         nonzero_inds_short = masks[feature_inds_defined,pp]==1\n",
    "\n",
    "                print('\\nEvaluating version %d of %d: %s'%(pp, n_partial_versions, partial_version_names[pp]))\n",
    "\n",
    "                features_to_use = masks[:,pp]==1\n",
    "                print('Includes %d features'%np.sum(features_to_use))\n",
    "\n",
    "                # [trials x features x voxels]\n",
    "                features_full = pred_models[:,:,best_model_inds[rv,pp]]\n",
    "                # Take out the relevant features now\n",
    "                features_full = features_full[:,features_to_use,:]\n",
    "                # Note there may be some zeros in this matrix, if we used fewer than the max number of features.\n",
    "                # But they are zero in weight matrix too, so turns out ok.\n",
    "\n",
    "                _weights = torch_utils._to_torch(weights[rv,:,pp], device=device)   \n",
    "                _weights = _weights[:, features_to_use]\n",
    "                _bias = torch_utils._to_torch(bias[rv,pp], device=device)\n",
    "\n",
    "                print('number of zeros:')\n",
    "                print(np.sum(features_full[0,:,0]==0))\n",
    "\n",
    "                print('size of weights is:')\n",
    "                print(_weights.shape)\n",
    "\n",
    "                if features_mt is not None:\n",
    "                    _features_m = torch_utils._to_torch(features_mt[rv,:], device=device)\n",
    "                    _features_m = _features_m[:,features_to_use]\n",
    "                if features_st is not None:\n",
    "                    _features_s = torch_utils._to_torch(features_st[rv,:], device=device)\n",
    "                    _features_s = _features_s[:,features_to_use]\n",
    "\n",
    "                pred_block = np.full(fill_value=0, shape=(n_trials, lv), dtype=dtype)\n",
    "\n",
    "                # Now looping over validation set trials in batches\n",
    "                for rt, lt in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "\n",
    "                    _features = torch_utils._to_torch(features_full[rt,:], device=device) # trials x features\n",
    "                    if features_mt is not None:    \n",
    "                        # features_m is [nvoxels x nfeatures] - need [trials x features x voxels]\n",
    "                        _features = _features - torch.tile(torch.unsqueeze(_features_m, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "\n",
    "                    if features_st is not None:\n",
    "                        _features = _features/torch.tile(torch.unsqueeze(_features_s, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "                        # if any entries in std are zero or nan, this gives bad result - fix these now.\n",
    "                        # these bad entries will also be zero in weights, so doesn't matter. just want to avoid nans.\n",
    "                        _features[torch.isnan(_features)] = 0.0 \n",
    "                        _features[torch.isinf(_features)] = 0.0\n",
    "                        \n",
    "                    # features is [#samples, #features, #voxels] - swap dims to [#voxels, #samples, features]\n",
    "                    _features = torch.transpose(torch.transpose(_features, 0, 2), 1, 2)\n",
    "                    # weights is [#voxels, #features]\n",
    "                    # _r will be [#voxels, #samples, 1] - then [#samples, #voxels]\n",
    "\n",
    "                    _r = torch.squeeze(torch.bmm(_features, torch.unsqueeze(_weights, 2)), dim=2).t() \n",
    "\n",
    "                    if _bias is not None:\n",
    "                        _r = _r + torch.tile(torch.unsqueeze(_bias, 0), [_r.shape[0],1])\n",
    "\n",
    "                    pred_block[rt] = torch_utils.get_value(_r) \n",
    "\n",
    "                # Now for this batch of voxels and this partial version of the model, measure performance.\n",
    "        #                 print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "                for vi in range(lv):   \n",
    "                    val_cc[rv[vi],pp] = np.corrcoef(voxel_data[:,rv[vi]], pred_block[:,vi])[0,1]  \n",
    "                    val_r2[rv[vi],pp] = get_r2(voxel_data[:,rv[vi]], pred_block[:,vi])\n",
    "\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    val_cc = np.nan_to_num(val_cc)\n",
    "    val_r2 = np.nan_to_num(val_r2) \n",
    "    \n",
    "    return val_cc, val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_texture_model(best_params, prf_models, val_voxel_single_trial_data, val_stim_single_trial_data, _texture_fn, sample_batch_size=100, voxel_batch_size=100, debug=False, dtype=np.float32):\n",
    "    \n",
    "    # EVALUATE PERFORMANCE ON VALIDATION SET\n",
    "\n",
    "    print('\\nGetting model predictions on validation set...\\n')\n",
    "    val_voxel_pred = get_predictions_texture_model(val_stim_single_trial_data, _texture_fn, best_params, prf_models, sample_batch_size=sample_batch_size, voxel_batch_size=voxel_batch_size, debug=debug)\n",
    "\n",
    "    # val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "    n_voxels = np.shape(val_voxel_single_trial_data)[1]\n",
    "    val_cc  = np.zeros(shape=(n_voxels), dtype=dtype)\n",
    "    val_r2 = np.zeros(shape=(n_voxels), dtype=dtype)\n",
    "    \n",
    "    print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "    for v in tqdm(range(n_voxels)):    \n",
    "        val_cc[v] = np.corrcoef(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])[0,1]  \n",
    "        val_r2[v] = get_r2(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])\n",
    "        \n",
    "    val_cc = np.nan_to_num(val_cc)\n",
    "    val_r2 = np.nan_to_num(val_r2)    \n",
    "    \n",
    "    return val_cc, val_r2\n",
    "\n",
    "\n",
    "def validate_texture_model_partial(best_params, prf_models, val_voxel_single_trial_data, val_stim_single_trial_data, _texture_fn, sample_batch_size=100, voxel_batch_size=100, debug=False, dtype=np.float32):\n",
    "    \n",
    "    \"\"\" \n",
    "    Evaluate trained model, leaving out a subset of features at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # val_cc is the correlation coefficient bw real and predicted responses across trials, for each voxel.\n",
    "    n_voxels = np.shape(val_voxel_single_trial_data)[1]\n",
    "    n_feature_types = len(_texture_fn.feature_types_include)\n",
    "    val_cc  = np.zeros(shape=(n_voxels, n_feature_types), dtype=dtype)\n",
    "    val_r2 = np.zeros(shape=(n_voxels, n_feature_types), dtype=dtype)\n",
    "\n",
    "    orig_feature_column_labels = _texture_fn.feature_column_labels\n",
    "    orig_excluded_features = _texture_fn.feature_types_exclude\n",
    "\n",
    "    for ff, feat_name in enumerate(_texture_fn.feature_types_include):\n",
    "\n",
    "        print('\\nVariance partition, leaving out: %s'%feat_name)\n",
    "        _texture_fn.update_feature_list(orig_excluded_features+[feat_name])\n",
    "        print('Remaining features are:')\n",
    "        print(_texture_fn.feature_types_include)\n",
    "\n",
    "        # Choose columns of interest here, leaving out weights for one feature at a time\n",
    "        params_to_use = copy.deepcopy(best_params)\n",
    "        columns_to_use = np.where(orig_feature_column_labels!=ff)[0]\n",
    "        print(columns_to_use)\n",
    "        params_to_use[1] = params_to_use[1][:,columns_to_use]\n",
    "        if best_params[3] is not None:\n",
    "            params_to_use[3] = params_to_use[3][:,columns_to_use]\n",
    "            params_to_use[4] = params_to_use[4][:,columns_to_use]\n",
    "\n",
    "        print(best_params[1].shape)\n",
    "        print(params_to_use[1].shape)\n",
    "        print('\\nGetting model predictions on validation set...\\n')\n",
    "#         val_voxel_pred = get_predictions_texture_model(val_stim_single_trial_data, _texture_fn, best_params, prf_models, sample_batch_size=sample_batch_size, voxel_batch_size=voxel_batch_size, debug=debug)\n",
    "        val_voxel_pred = get_predictions_texture_model(val_stim_single_trial_data, _texture_fn, params_to_use, prf_models, sample_batch_size=sample_batch_size, voxel_batch_size=voxel_batch_size, debug=debug)\n",
    "        print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "        for v in range(n_voxels):    \n",
    "            val_cc[v,ff] = np.corrcoef(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])[0,1]  \n",
    "            val_r2[v,ff] = get_r2(val_voxel_single_trial_data[:,v], val_voxel_pred[:,v])\n",
    "\n",
    "    val_cc = np.nan_to_num(val_cc)\n",
    "    val_r2 = np.nan_to_num(val_r2) \n",
    "    \n",
    "    return val_cc, val_r2\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions_texture_model(images, _texture_fn, params, prf_models, sample_batch_size=100, voxel_batch_size=100, debug=False):\n",
    "   \n",
    "    dtype = images.dtype.type\n",
    "    device = _texture_fn.device\n",
    "\n",
    "    best_models, weights, bias, features_mt, features_st, best_model_inds = params\n",
    "        \n",
    "    n_trials, n_voxels = len(images), len(params[0])\n",
    "    n_prfs = prf_models.shape[0]\n",
    "    n_features = params[1].shape[1]    \n",
    "    \n",
    "    pred = np.full(fill_value=0, shape=(n_trials, n_voxels), dtype=dtype)\n",
    "    pred_models = np.full(fill_value=0, shape=(n_trials, n_features, n_prfs), dtype=dtype)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # First gather texture features for all pRFs.\n",
    "        for mm in range(n_prfs):\n",
    "            if mm>1 and debug:\n",
    "                break\n",
    "            print('Getting features for prf %d: [x,y,sigma] is [%.2f %.2f %.4f]'%(mm, prf_models[mm,0],  prf_models[mm,1],  prf_models[mm,2] ))\n",
    "            all_feat_concat, feature_info = _texture_fn(images,prf_models[mm,:])\n",
    "            \n",
    "            pred_models[:,:,mm] = torch_utils.get_value(all_feat_concat)\n",
    "        \n",
    "        vv=-1\n",
    "        ## Looping over voxels here in batches, will eventually go through all.\n",
    "        for rv, lv in numpy_utils.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "            vv=vv+1\n",
    "            print('Getting predictions for voxels [%d-%d] of %d'%(rv[0],rv[-1],n_voxels))\n",
    "\n",
    "            if vv>1 and debug:\n",
    "                break\n",
    "                \n",
    "            # [trials x features x voxels]\n",
    "            features = pred_models[:,:,best_model_inds[rv]]\n",
    "\n",
    "            pred_block = np.full(fill_value=0, shape=(n_trials, lv), dtype=dtype)\n",
    "            if features_mt is not None:\n",
    "                _features_m = torch_utils._to_torch(features_mt[rv,:])\n",
    "            if features_st is not None:\n",
    "                _features_s = torch_utils._to_torch(features_st[rv,:])\n",
    "            _weights = torch_utils._to_torch(weights[rv,:])\n",
    "            _bias = torch_utils._to_torch(bias[rv])\n",
    "                \n",
    "            # Now looping over validation set trials in batches\n",
    "            for rt, lt in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "\n",
    "                _features = torch_utils._to_torch(features[rt,:,:]) # trials x features x voxels\n",
    "                if features_mt is not None:    \n",
    "                    # features_m is [nvoxels x nfeatures] - need [trials x features x voxels]\n",
    "                    _features = _features - torch.tile(torch.unsqueeze(_features_m, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "\n",
    "                if features_st is not None:\n",
    "                    _features = _features/torch.tile(torch.unsqueeze(_features_s, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "                    _features[torch.isnan(_features)] = 0.0 # this applies in the pca case when last few columns of features are missing\n",
    "\n",
    "                # features is [#samples, #features, #voxels] - swap dims to [#voxels, #samples, features]\n",
    "                _features = torch.transpose(torch.transpose(_features, 0, 2), 1, 2)\n",
    "                # weights is [#voxels, #features]\n",
    "                # _r will be [#voxels, #samples, 1] - then [#samples, #voxels]\n",
    "\n",
    "                _r = torch.squeeze(torch.bmm(_features, torch.unsqueeze(_weights, 2)), dim=2).t() \n",
    "\n",
    "                if _bias is not None:\n",
    "                    _r = _r + torch.tile(torch.unsqueeze(_bias, 0), [_r.shape[0],1])\n",
    "\n",
    "                pred_block[rt] = torch_utils.get_value(_r) \n",
    "                \n",
    "            pred[:,rv] = pred_block\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    total_time = time.time() - start_time\n",
    "    print ('\\n---------------------------------------')\n",
    "    print ('total time = %fs' % total_time)\n",
    "    print ('sample throughput = %fs/sample' % (total_time / n_trials))\n",
    "    print ('voxel throughput = %fs/voxel' % (total_time / n_voxels))\n",
    "    sys.stdout.flush()\n",
    "    return pred\n",
    "\n",
    "   \n",
    "def validate_bdcn_model(best_params, prf_models, val_voxel_single_trial_data, \\\n",
    "                            val_stim_single_trial_data, _feature_extractor, pc=None, sample_batch_size=100, \\\n",
    "                            voxel_batch_size=100, debug=False, dtype=np.float32):\n",
    "    \n",
    "    \"\"\" \n",
    "    Evaluate trained model, leaving out a subset of features at a time.\n",
    "    \"\"\"\n",
    "    print('starting validation function')\n",
    "    sys.stdout.flush()\n",
    "        \n",
    "    images = val_stim_single_trial_data\n",
    "    params = best_params\n",
    "    dtype = images.dtype.type\n",
    "    device = _feature_extractor.device\n",
    "    \n",
    "    n_trials, n_voxels = len(images), len(params[0])\n",
    "    n_prfs = prf_models.shape[0]\n",
    "    n_features = params[1].shape[1]  \n",
    "\n",
    "    best_models, weights, bias, features_mt, features_st, best_model_inds = params\n",
    "    if pc is not None:\n",
    "        pca_wts, pct_var_expl, min_pct_var, n_comp_needed, pca_pre_mean = pc\n",
    "    \n",
    "    n_voxels = np.shape(val_voxel_single_trial_data)[1]\n",
    "    print('starting to initialize big arrays')\n",
    "    sys.stdout.flush()\n",
    "        \n",
    "    val_cc  = np.zeros(shape=(n_voxels, 1), dtype=dtype)\n",
    "    val_r2 = np.zeros(shape=(n_voxels, 1), dtype=dtype)\n",
    "    n_features_actual = np.zeros(shape=(n_prfs,), dtype=int)\n",
    "\n",
    "    pred_models = np.full(fill_value=0, shape=(n_trials, n_features, n_prfs), dtype=dtype)\n",
    "    \n",
    "    print('about to start loop')\n",
    "    sys.stdout.flush()\n",
    "        \n",
    "    start_time = time.time()    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # First gather features for all pRFs.\n",
    "        \n",
    "        _feature_extractor.clear_maps()\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        \n",
    "        for mm in range(n_prfs):\n",
    "            if mm>1 and debug:\n",
    "                break\n",
    "            print('Getting features for prf %d: [x,y,sigma] is [%.2f %.2f %.4f]'%(mm, prf_models[mm,0],  prf_models[mm,1],  prf_models[mm,2] ))\n",
    "            \n",
    "            features = _feature_extractor(images, prf_models[mm,:]).detach().cpu().numpy()   \n",
    "\n",
    "            if pc is not None:\n",
    "                print('Applying pre-computed PCA matrix')\n",
    "                # Apply the PCA transformation, just as it was done during training\n",
    "                nfeat = features.shape[1]\n",
    "                features_submean = features - np.tile(np.expand_dims(pca_pre_mean[mm][0:nfeat], axis=0), [n_trials, 1])\n",
    "                features_reduced = features_submean @ np.transpose(pca_wts[mm][0:n_comp_needed[mm],0:nfeat])                                               \n",
    "                features = features_reduced\n",
    "\n",
    "            n_features_actual[mm] = features.shape[1]\n",
    "\n",
    "            pred_models[:,0:n_features_actual[mm],mm] = features\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "        \n",
    "                \n",
    "        _feature_extractor.clear_maps()\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    \n",
    "        vv=-1\n",
    "        ## Looping over voxels here in batches, will eventually go through all.\n",
    "        for rv, lv in numpy_utils.iterate_range(0, n_voxels, voxel_batch_size):\n",
    "            vv=vv+1\n",
    "            print('Getting predictions for voxels [%d-%d] of %d'%(rv[0],rv[-1],n_voxels))\n",
    "\n",
    "            if vv>1 and debug:\n",
    "                break\n",
    "            \n",
    "            # [trials x features x voxels]\n",
    "            # to keep this from being huge, just keep the maximum number of features needed for any voxel\n",
    "            # there will be some zeros still, but they are also zero in the weights so not a problem.\n",
    "            feat2use = np.max(n_features_actual[best_model_inds[rv]])\n",
    "            features = pred_models[:,0:feat2use,best_model_inds[rv]]\n",
    "\n",
    "            print('size of feature matrix to use is:')\n",
    "            print(features.shape)\n",
    "            \n",
    "            _weights = torch_utils._to_torch(weights[rv,0:feat2use]) \n",
    "            _bias = torch_utils._to_torch(bias[rv])\n",
    "\n",
    "            if features_mt is not None:\n",
    "                _features_m = torch_utils._to_torch(features_mt[rv,0:feat2use])\n",
    "            if features_st is not None:\n",
    "                _features_s = torch_utils._to_torch(features_st[rv,0:feat2use])\n",
    "\n",
    "            pred_block = np.full(fill_value=0, shape=(n_trials, lv), dtype=dtype)\n",
    "\n",
    "            # Now looping over validation set trials in batches\n",
    "            for rt, lt in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "\n",
    "                _features = torch_utils._to_torch(features[rt,:,:]) # trials x features x voxels\n",
    "                if features_mt is not None:    \n",
    "                    # features_m is [nvoxels x nfeatures] - need [trials x features x voxels]\n",
    "                    _features = _features - torch.tile(torch.unsqueeze(_features_m, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "\n",
    "                if features_st is not None:\n",
    "                    _features = _features/torch.tile(torch.unsqueeze(_features_s, dim=0), [_features.shape[0], 1, 1]).moveaxis([1],[2])\n",
    "                    _features[torch.isnan(_features)] = 0.0 # this applies in the pca case when last few columns of features are missing\n",
    "\n",
    "                # features is [#samples, #features, #voxels] - swap dims to [#voxels, #samples, features]\n",
    "                _features = torch.transpose(torch.transpose(_features, 0, 2), 1, 2)\n",
    "                # weights is [#voxels, #features]\n",
    "                # _r will be [#voxels, #samples, 1] - then [#samples, #voxels]\n",
    "\n",
    "                _r = torch.squeeze(torch.bmm(_features, torch.unsqueeze(_weights, 2)), dim=2).t() \n",
    "\n",
    "                if _bias is not None:\n",
    "                    _r = _r + torch.tile(torch.unsqueeze(_bias, 0), [_r.shape[0],1])\n",
    "\n",
    "                pred_block[rt] = torch_utils.get_value(_r) \n",
    "\n",
    "            # Now for this batch of voxels and this partial version of the model, measure performance.\n",
    "#                 print('\\nEvaluating correlation coefficient on validation set...\\n')\n",
    "            for vi in range(lv):   \n",
    "                val_cc[rv[vi],0] = np.corrcoef(val_voxel_single_trial_data[:,rv[vi]], pred_block[:,vi])[0,1]  \n",
    "                val_r2[rv[vi],0] = get_r2(val_voxel_single_trial_data[:,rv[vi]], pred_block[:,vi])\n",
    "\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "    val_cc = np.nan_to_num(val_cc)\n",
    "    val_r2 = np.nan_to_num(val_r2) \n",
    "    \n",
    "    return val_cc, val_r2\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
