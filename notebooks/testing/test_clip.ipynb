{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1afdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import argparse\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "import h5py\n",
    "import copy\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "#import custom modules\n",
    "code_dir = '/user_data/mmhender/imStat/code/'\n",
    "sys.path.append(code_dir)\n",
    "from utils import prf_utils, torch_utils, texture_utils, default_paths, nsd_utils\n",
    "from model_fitting import initialize_fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ccbd7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 332.24646 seconds to load file\n"
     ]
    }
   ],
   "source": [
    "features_file = '/user_data/mmhender/features/CLIP/S1_RN50_block1_features_each_prf_grid5.h5py'\n",
    "t = time.time()\n",
    "with h5py.File(features_file, 'r') as data_set:\n",
    "    values = np.copy(data_set['/features'][:,:,0:2])\n",
    "    data_set.close() \n",
    "elapsed = time.time() - t\n",
    "print('Took %.5f seconds to load file'%elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d64ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "badvals = values[:,np.sum(np.isnan(scipy.stats.zscore(values[:,:,0])), axis=0)>0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a5084bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_var = np.var(values[:,:,0], axis=0)==0\n",
    "np.sum(zero_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c5af53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_in_prf = values[:,:,0]\n",
    "trninds = np.arange(10000)<10\n",
    "n_features = values.shape[1]\n",
    "zgroup_labels = np.ones(shape=(1,n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acdf428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import numpy_utils\n",
    "features_in_prf_z = np.zeros_like(features_in_prf)\n",
    "features_in_prf_z[trninds,:] = numpy_utils.zscore_in_groups(features_in_prf[trninds,:], zgroup_labels)\n",
    "features_in_prf_z[~trninds,:] = numpy_utils.zscore_in_groups(features_in_prf[~trninds,:], zgroup_labels)\n",
    "# if any feature channels had no variance, fix them now\n",
    "zero_var = (np.var(features_in_prf[trninds,:], axis=0)==0) | \\\n",
    "            (np.var(features_in_prf[~trninds,:], axis=0)==0)\n",
    "# features_in_prf_z[:,zero_var] = features_in_prf[0,zero_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e053a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92410981])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_in_prf_z[0,zero_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe3b5458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0454a5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53097fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model, preprocess = clip.load(\"RN50\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3fad09a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading images for subject 1\n",
      "\n",
      "image data size: (10000, 3, 240, 240) , dtype: uint8 , value range: 0 255\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the image set to work with (all images for the current subject, 10,000 ims)\n",
    "subject=1\n",
    "stim_root = default_paths.stim_root\n",
    "image_data = nsd_utils.get_image_data(subject)  \n",
    "image_data = nsd_utils.image_uncolorize_fn(image_data)\n",
    "\n",
    "n_images = image_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5850eb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 240, 240])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42d567d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = len(model.visual.layer1) + len(model.visual.layer2) + len(model.visual.layer3) + len(model.visual.layer4)\n",
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8aeccd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first making this subfunction that is needed to get the activation on a forward pass\n",
    "def get_activ_fwd_hook(ii,ll):\n",
    "    def hook(self, input, output):\n",
    "        # the relu operation is used multiple times per block, but we only \n",
    "        # want to save its output when it has this specific size.\n",
    "        if output.shape[1]==exp_size[ll]:\n",
    "            print('executing hook for %s'%block_names[ll])  \n",
    "            activ[ii] = output\n",
    "            print(output.shape)\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d166859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 16 residual blocks are segmented into 4 groups here, which have different numbers of features.\n",
    "blocks_each= [len(model.visual.layer1), len(model.visual.layer2), len(model.visual.layer3),len(model.visual.layer4)]\n",
    "which_group = np.repeat(np.arange(4), blocks_each)\n",
    "n_blocks = sum(blocks_each)\n",
    "block_names = ['block_%d'%bb for bb in range(n_blocks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e617abaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing hook for block_0\n",
      "torch.Size([5, 256, 60, 60])\n",
      "executing hook for block_1\n",
      "torch.Size([5, 256, 60, 60])\n",
      "executing hook for block_2\n",
      "torch.Size([5, 256, 60, 60])\n",
      "executing hook for block_3\n",
      "torch.Size([5, 512, 30, 30])\n",
      "executing hook for block_4\n",
      "torch.Size([5, 512, 30, 30])\n",
      "executing hook for block_5\n",
      "torch.Size([5, 512, 30, 30])\n",
      "executing hook for block_6\n",
      "torch.Size([5, 512, 30, 30])\n",
      "executing hook for block_7\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "executing hook for block_8\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "executing hook for block_9\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "executing hook for block_10\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "executing hook for block_11\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "executing hook for block_12\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "executing hook for block_13\n",
      "torch.Size([5, 2048, 7, 7])\n",
      "executing hook for block_14\n",
      "torch.Size([5, 2048, 7, 7])\n",
      "executing hook for block_15\n",
      "torch.Size([5, 2048, 7, 7])\n",
      "torch.Size([5, 256, 60, 60])\n",
      "torch.Size([5, 256, 60, 60])\n",
      "torch.Size([5, 256, 60, 60])\n",
      "torch.Size([5, 512, 30, 30])\n",
      "torch.Size([5, 512, 30, 30])\n",
      "torch.Size([5, 512, 30, 30])\n",
      "torch.Size([5, 512, 30, 30])\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "torch.Size([5, 1024, 15, 15])\n",
      "torch.Size([5, 2048, 7, 7])\n",
      "torch.Size([5, 2048, 7, 7])\n",
      "torch.Size([5, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"RN50\", device=device)\n",
    "model.eval()\n",
    "layer_inds = list(np.arange(16))\n",
    "activ = [[] for ll in layer_inds]\n",
    "hooks = [[] for ll in layer_inds]\n",
    "\n",
    "image_tensors = torch.Tensor(np.tile(image_data[0:5,:,:,:],[1,3,1,1])).to(device)\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # adding a \"hook\" to the module corresponding to each layer, so we'll save activations at each layer.\n",
    "    # For resnet, going to save output of each residual block following last relu operation.\n",
    "    for ii, ll in enumerate(layer_inds):\n",
    "        if which_group[ll]==0:            \n",
    "            h = model.visual.layer1[ll].relu.register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        elif which_group[ll]==1:            \n",
    "            h = model.visual.layer2[ll-blocks_each[0]].relu.register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        elif which_group[ll]==2:            \n",
    "            h = model.visual.layer3[ll-sum(blocks_each[0:2])].relu.register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        elif which_group[ll]==3:            \n",
    "            h = model.visual.layer4[ll-sum(blocks_each[0:3])].relu.register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        else:\n",
    "            h=None\n",
    "        hooks[ii] = h\n",
    "    \n",
    "    # Pass images though the model (hooks get run now)\n",
    "    image_features = model.encode_image(image_tensors)\n",
    "    \n",
    "    # Now remove all the hooks\n",
    "    for ii, ll in enumerate(layer_inds):\n",
    "        print(activ[ii].shape)\n",
    "        hooks[ii].remove\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6d909623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x7f0c2d5e37f0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure();\n",
    "plt.pcolormesh(activ[14][0,1000,:,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c9c4f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that we grabbed the right activations - check their sizes against expected\n",
    "# output size of each block\n",
    "exp_size = [model.visual.layer1[ii].conv3.weight.shape[0] for ii in range(blocks_each[0])] + \\\n",
    "            [model.visual.layer2[ii].conv3.weight.shape[0] for ii in range(blocks_each[1])] + \\\n",
    "            [model.visual.layer3[ii].conv3.weight.shape[0] for ii in range(blocks_each[2])] + \\\n",
    "            [model.visual.layer4[ii].conv3.weight.shape[0] for ii in range(blocks_each[3])]\n",
    "actual_size = [activ[bb].shape[1] for bb in range(n_blocks)]\n",
    "assert(np.all(np.array(actual_size)==np.array(exp_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a222ffa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256,\n",
       " 256,\n",
       " 256,\n",
       " 512,\n",
       " 512,\n",
       " 512,\n",
       " 512,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 2048,\n",
       " 2048,\n",
       " 2048]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "854c2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_each_block = [256,256,256, 512,512,512,512, 1024,1024,1024,1024,1024,1024, 2048,2048,2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "56cc9486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_size==n_features_each_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac860c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_activations_batch(image_batch, layer_inds, device=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Get activations for images in NSD, passed through pretrained CLIP model.\n",
    "    Specify which NSD images to look at, and which layers to return.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')\n",
    "       \n",
    "    # The 16 residual blocks are segmented into 4 groups here, which have different numbers of features.\n",
    "    blocks_each= [len(model.visual.layer1), len(model.visual.layer2), len(model.visual.layer3),len(model.visual.layer4)]\n",
    "    which_group = np.repeat(np.arange(4), blocks_each)\n",
    "    n_blocks = sum(blocks_each)\n",
    "    block_names = ['block_%d'%bb for bb in range(n_blocks)]\n",
    "    # compute how many feature maps will be in each \n",
    "    exp_size = [model.visual.layer1[ii].conv3.weight.shape[0] for ii in range(blocks_each[0])] + \\\n",
    "            [model.visual.layer2[ii].conv3.weight.shape[0] for ii in range(blocks_each[1])] + \\\n",
    "            [model.visual.layer3[ii].conv3.weight.shape[0] for ii in range(blocks_each[2])] + \\\n",
    "            [model.visual.layer4[ii].conv3.weight.shape[0] for ii in range(blocks_each[3])]\n",
    "    \n",
    "    # first loading pre-trained model from torch model zoo\n",
    "    model = models.alexnet(pretrained=True).float().to(device)\n",
    "    if padding_mode is not None:\n",
    "        # change padding type for all convolutional layers, \"reflect\" is a\n",
    "        # good way to minimize edge artifacts.\n",
    "        for ff in model.features:\n",
    "            if hasattr(ff, 'padding_mode'):\n",
    "                ff.padding_mode=padding_mode\n",
    "                print('changing padding mode to %s'%padding_mode)\n",
    "                print(ff)\n",
    "                \n",
    "                \n",
    "    model.eval()\n",
    "    model_name='AlexNet'\n",
    "\n",
    "    is_fc = [('FC' in alexnet_layer_names[ll] or 'fc' in alexnet_layer_names[ll]) for ll in layer_inds]\n",
    "    \n",
    "    if len(layer_inds)==0:\n",
    "        raise ValueError('your layer names do not match any of those specified in alexnet_features.py')\n",
    "\n",
    "    \n",
    "    # first making this subfunction that is needed to get the activation on a forward pass\n",
    "    def get_activ_fwd_hook(ii,ll):\n",
    "        def hook(self, input, output):            \n",
    "            print('hook for %s'%alexnet_layer_names[ll])           \n",
    "            activ[ii] = output\n",
    "            print(output.shape)\n",
    "        return hook\n",
    "   \n",
    "    # get image and labels for this batch\n",
    "    # image_tensors is [batch_size x 3 x 224 x 224]\n",
    "    image_tensors =  torch_utils._to_torch(image_batch, device=device).float()\n",
    "    activ = [[] for ll in layer_inds]\n",
    "    hook = [[] for ll in layer_inds]\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # adding this \"hook\" to the module corresponding to each layer, so we'll save activations at each layer\n",
    "    # this only modifies the \"graph\" e.g. what the model code does when run, but doesn't actually run it yet.\n",
    "    for ii, ll in enumerate(layer_inds):\n",
    "        if not is_fc[ii]:\n",
    "            h = model.features[ll].register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        else:\n",
    "            h = model.classifier[ll-n_feature_layers].register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        hook[ii] = h\n",
    "\n",
    "    # do the forward pass of model, which now includes the forward hooks\n",
    "    # now the \"activ\" variable will get modified, because it gets altered during the hook function\n",
    "    model(image_tensors)\n",
    "    \n",
    "    # Now remove all the hooks\n",
    "    for ii, ll in enumerate(layer_inds):\n",
    "        print(activ[ii].shape)\n",
    "        hook[ii].remove\n",
    "\n",
    "    return activ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype=np.float32\n",
    "\n",
    "# Define sets of alexnet layers\n",
    "alexnet_conv_layer_names = ['Conv1','Conv1_ReLU','Conv1_MaxPool', \\\n",
    "                       'Conv2','Conv2_ReLU','Conv2_MaxPool', \\\n",
    "                       'Conv3','Conv3_ReLU', \\\n",
    "                       'Conv4','Conv4_ReLU', \\\n",
    "                       'Conv5','Conv5_ReLU','Conv5_MaxPool']\n",
    "\n",
    "alexnet_fc_layer_names = ['Dropout6','FC6','FC6_ReLU','Dropout7','FC7','FC7_ReLU','FC8']\n",
    "\n",
    "n_feature_layers = len(alexnet_conv_layer_names)\n",
    "n_classif_layers = len(alexnet_fc_layer_names)\n",
    "n_total_layers = n_feature_layers + n_classif_layers\n",
    "alexnet_layer_names = copy.deepcopy(alexnet_conv_layer_names)\n",
    "alexnet_layer_names.extend(alexnet_fc_layer_names)\n",
    "\n",
    "n_features_each_layer = [64,64,64, 192,192,192, 384,384, 256,256, 256,256]\n",
    "\n",
    "def get_features_each_prf(subject, use_node_storage=False, debug=False, \\\n",
    "                          which_prf_grid=1, padding_mode=None):\n",
    "    \"\"\"\n",
    "    Extract the portion of CNN feature maps corresponding to pRF defined in \"models\"\n",
    "    Return list of the features in each pRF, for each layer of interest.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = initialize_fitting.init_cuda()\n",
    "    if padding_mode=='':\n",
    "        padding_mode = None\n",
    "        \n",
    "    if use_node_storage:\n",
    "        alexnet_feat_path = default_paths.alexnet_feat_path_localnode\n",
    "    else:\n",
    "        alexnet_feat_path = default_paths.alexnet_feat_path\n",
    "\n",
    "    # Load and prepare the image set to work with (all images for the current subject, 10,000 ims)\n",
    "    stim_root = default_paths.stim_root\n",
    "    image_data = nsd_utils.get_image_data(subject)  \n",
    "    image_data = nsd_utils.image_uncolorize_fn(image_data)\n",
    "   \n",
    "    n_images = image_data.shape[0]\n",
    "    \n",
    "    # Params for the spatial aspect of the model (possible pRFs)\n",
    "    prf_models = initialize_fitting.get_prf_models(which_grid=which_prf_grid)    \n",
    "\n",
    "    # Fix these params\n",
    "    n_prf_sd_out = 2\n",
    "    batch_size = 50\n",
    "    mult_patch_by_prf = True\n",
    "    do_avg_pool = True\n",
    "    \n",
    "    layers_to_return = ['Conv1_ReLU', 'Conv2_ReLU','Conv3_ReLU','Conv4_ReLU','Conv5_ReLU']\n",
    "    n_layers = len(layers_to_return)\n",
    "    layer_inds = [ll for ll in range(len(alexnet_layer_names)) \\\n",
    "                      if alexnet_layer_names[ll] in layers_to_return]\n",
    "\n",
    "    n_prfs = len(prf_models)\n",
    "    features_each_prf = [np.zeros((n_images, n_features_each_layer[ll], n_prfs),dtype=dtype) for ll in layer_inds]\n",
    "\n",
    "    n_batches = int(np.ceil(n_images/batch_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for bb in range(n_batches):\n",
    "\n",
    "            if debug and bb>1:\n",
    "                continue\n",
    "            print('Processing images for batch %d of %d'%(bb, n_batches))\n",
    "\n",
    "            batch_inds = np.arange(batch_size * bb, np.min([batch_size * (bb+1), n_images]))\n",
    "\n",
    "            # using grayscale images for better comparison w my other models.\n",
    "            # need to tile to 3 so alexnet weights will be right size\n",
    "            image_batch = np.tile(image_data[batch_inds,:,:,:], [1,3,1,1])\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            activ_batch = get_alexnet_activations_batch(image_batch, layer_inds, \\\n",
    "                                                        device=device, padding_mode=padding_mode)\n",
    "\n",
    "            for ll in range(n_layers):\n",
    "\n",
    "                print('Getting prf-specific activations for layer %s'%alexnet_layer_names[layer_inds[ll]])\n",
    "\n",
    "                maps_full_field = torch.moveaxis(activ_batch[ll], [0,1,2,3], [0,3,1,2])\n",
    "\n",
    "                for mm in range(n_prfs):\n",
    "\n",
    "                    if debug and mm>1:\n",
    "                        continue\n",
    "\n",
    "                    prf_params = prf_models[mm,:]\n",
    "                    x,y,sigma = prf_params\n",
    "                    print('Getting features for pRF [x,y,sigma]:')\n",
    "                    print([x,y,sigma])\n",
    "                    n_pix = maps_full_field.shape[1]\n",
    "\n",
    "                    # Define the RF for this \"model\" version\n",
    "                    prf = torch_utils._to_torch(prf_utils.gauss_2d(center=[x,y], sd=sigma, \\\n",
    "                               patch_size=n_pix, aperture=1.0, dtype=np.float32), device=device)\n",
    "                    minval = torch.min(prf)\n",
    "                    maxval = torch.max(prf-minval)\n",
    "                    prf_scaled = (prf - minval)/maxval\n",
    "\n",
    "                    if sigma==10:\n",
    "                        # creating a \"flat\" pRF here which will average across entire feature map.\n",
    "                        prf_scaled = torch.ones(prf_scaled.shape)\n",
    "                        prf_scaled = prf_scaled/torch.sum(prf_scaled)\n",
    "                        prf_scaled = prf_scaled.to(device)\n",
    "\n",
    "                    if mult_patch_by_prf:\n",
    "                        # This effectively restricts the spatial location, so no need to crop\n",
    "                        maps = maps_full_field * prf_scaled.view([1,n_pix, n_pix,1])\n",
    "                    else:\n",
    "                        # This is a coarser way of choosing which spatial region to look at\n",
    "                        # Crop the patch +/- n SD away from center\n",
    "                        bbox = texture_utils.get_bbox_from_prf(prf_params, prf.shape, n_prf_sd_out, \\\n",
    "                                                       min_pix=None, verbose=False, force_square=False)\n",
    "                        print('bbox to crop is:')\n",
    "                        print(bbox)\n",
    "                        maps = maps_full_field[:,bbox[0]:bbox[1], bbox[2]:bbox[3],:]\n",
    "\n",
    "                    if do_avg_pool:\n",
    "                        features_batch = torch.mean(maps, dim=(1,2))\n",
    "                    else:\n",
    "                        features_batch = torch.max(maps, dim=(1,2))\n",
    "\n",
    "                    print('model %d, min/max of features in batch: [%s, %s]'%(mm, \\\n",
    "                                                  torch.min(features_batch), torch.max(features_batch))) \n",
    "\n",
    "                    features_each_prf[ll][batch_inds,:,mm] = torch_utils.get_value(features_batch)\n",
    "\n",
    "        # Now save the results, one file for each alexnet layer \n",
    "        for ii, ll in enumerate(layer_inds):\n",
    "            if padding_mode is not None:\n",
    "                fn2save = os.path.join(alexnet_feat_path, \\\n",
    "                   'S%d_%s_%s_features_each_prf_grid%d.h5py'%(subject, \\\n",
    "                                       alexnet_layer_names[ll], padding_mode, which_prf_grid))\n",
    "            else:    \n",
    "                fn2save = os.path.join(alexnet_feat_path, \\\n",
    "                   'S%d_%s_features_each_prf_grid%d.h5py'%(subject, \\\n",
    "                                           alexnet_layer_names[ll], which_prf_grid))\n",
    "            print('Writing prf features to %s\\n'%fn2save)\n",
    "\n",
    "            t = time.time()\n",
    "            with h5py.File(fn2save, 'w') as data_set:\n",
    "                dset = data_set.create_dataset(\"features\", np.shape(features_each_prf[ii]), dtype=np.float64)\n",
    "                data_set['/features'][:,:,:] = features_each_prf[ii]\n",
    "                data_set.close()  \n",
    "            elapsed = time.time() - t\n",
    "\n",
    "            print('Took %.5f sec to write file'%elapsed)\n",
    "\n",
    "\n",
    "def get_alexnet_activations_batch(image_batch, layer_inds, device=None, padding_mode=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Get activations for images in NSD, passed through pretrained AlexNet.\n",
    "    Specify which NSD images to look at, and which layers to return.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cpu:0')\n",
    "       \n",
    "    # first loading pre-trained model from torch model zoo\n",
    "    model = models.alexnet(pretrained=True).float().to(device)\n",
    "    if padding_mode is not None:\n",
    "        # change padding type for all convolutional layers, \"reflect\" is a\n",
    "        # good way to minimize edge artifacts.\n",
    "        for ff in model.features:\n",
    "            if hasattr(ff, 'padding_mode'):\n",
    "                ff.padding_mode=padding_mode\n",
    "                print('changing padding mode to %s'%padding_mode)\n",
    "                print(ff)\n",
    "                \n",
    "                \n",
    "    model.eval()\n",
    "    model_name='AlexNet'\n",
    "\n",
    "    is_fc = [('FC' in alexnet_layer_names[ll] or 'fc' in alexnet_layer_names[ll]) for ll in layer_inds]\n",
    "    \n",
    "    if len(layer_inds)==0:\n",
    "        raise ValueError('your layer names do not match any of those specified in alexnet_features.py')\n",
    "\n",
    "    \n",
    "    # first making this subfunction that is needed to get the activation on a forward pass\n",
    "    def get_activ_fwd_hook(ii,ll):\n",
    "        def hook(self, input, output):            \n",
    "            print('hook for %s'%alexnet_layer_names[ll])           \n",
    "            activ[ii] = output\n",
    "            print(output.shape)\n",
    "        return hook\n",
    "   \n",
    "    # get image and labels for this batch\n",
    "    # image_tensors is [batch_size x 3 x 224 x 224]\n",
    "    image_tensors =  torch_utils._to_torch(image_batch, device=device).float()\n",
    "    activ = [[] for ll in layer_inds]\n",
    "    hook = [[] for ll in layer_inds]\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # adding this \"hook\" to the module corresponding to each layer, so we'll save activations at each layer\n",
    "    # this only modifies the \"graph\" e.g. what the model code does when run, but doesn't actually run it yet.\n",
    "    for ii, ll in enumerate(layer_inds):\n",
    "        if not is_fc[ii]:\n",
    "            h = model.features[ll].register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        else:\n",
    "            h = model.classifier[ll-n_feature_layers].register_forward_hook(get_activ_fwd_hook(ii,ll))\n",
    "        hook[ii] = h\n",
    "\n",
    "    # do the forward pass of model, which now includes the forward hooks\n",
    "    # now the \"activ\" variable will get modified, because it gets altered during the hook function\n",
    "    model(image_tensors)\n",
    "    \n",
    "    # Now remove all the hooks\n",
    "    for ii, ll in enumerate(layer_inds):\n",
    "        print(activ[ii].shape)\n",
    "        hook[ii].remove\n",
    "\n",
    "    return activ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
