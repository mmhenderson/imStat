{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "26414fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/user_data/mmhender/imStat/code/')\n",
    "from feature_extraction import texture_statistics_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "645363be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n"
     ]
    }
   ],
   "source": [
    "from model_fitting import initialize_fitting\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f6586be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the pyramid\n",
    "compute_features = False\n",
    "n_sf=4\n",
    "n_ori=4\n",
    "subject=1\n",
    "sample_batch_size=100;\n",
    "feature_types_exclude=None;\n",
    "n_prf_sd_out=2;\n",
    "aperture=1.0;\n",
    "do_varpart=True;\n",
    "zscore_in_groups=True;\n",
    "group_all_hl_feats=True;\n",
    "compute_features=False;\n",
    "use_pca_feats_hl=False\n",
    "use_pca_feats_ll=False\n",
    "min_pct_var = 99;\n",
    "max_pc_to_retain_ll = 100;\n",
    "max_pc_to_retain_hl = 100\n",
    "device=None\n",
    "\n",
    "_fmaps_fn = texture_statistics_pyramid.steerable_pyramid_extractor(pyr_height = n_sf, n_ori = n_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "aefe4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types to exclude from the model:\n",
      "[]\n",
      "Grouping lower level features:\n",
      "['pixel_stats' 'mean_magnitudes' 'mean_realparts'\n",
      " 'marginal_stats_lowpass_recons' 'variance_highpass_resid']\n",
      "Grouping higher level features:\n",
      "['magnitude_feature_autocorrs' 'lowpass_recon_autocorrs'\n",
      " 'highpass_resid_autocorrs' 'magnitude_within_scale_crosscorrs'\n",
      " 'real_within_scale_crosscorrs' 'magnitude_across_scale_crosscorrs'\n",
      " 'real_imag_across_scale_crosscorrs'\n",
      " 'real_spatshift_within_scale_crosscorrs'\n",
      " 'real_spatshift_across_scale_crosscorrs']\n"
     ]
    }
   ],
   "source": [
    "_feature_extractor = texture_feature_extractor(_fmaps_fn,\\\n",
    "                                      sample_batch_size=sample_batch_size, \\\n",
    "                                      subject=subject, feature_types_exclude=feature_types_exclude, \\\n",
    "                                      n_prf_sd_out=n_prf_sd_out,\\\n",
    "                                      aperture=aperture, do_varpart = do_varpart, zscore_in_groups = zscore_in_groups,\\\n",
    "                                      group_all_hl_feats = group_all_hl_feats, compute_features = compute_features, \\\n",
    "                                      use_pca_feats_hl=use_pca_feats_hl, use_pca_feats_ll=use_pca_feats_ll, \\\n",
    "                                      min_pct_var = min_pct_var, \\\n",
    "                                      max_pc_to_retain_ll = max_pc_to_retain_ll, max_pc_to_retain_hl = max_pc_to_retain_hl,\\\n",
    "                                               device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "31e1d8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(_feature_extractor,'zgroup_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5bb4b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing for fitting\n",
      "Clearing precomputed features from memory.\n"
     ]
    }
   ],
   "source": [
    "_feature_extractor.init_for_fitting(image_size=(224,224), models=models, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e55aaa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed features for models [0 - 49] from /user_data/mmhender/features/pyramid_texture/S1_features_each_prf_4ori_4sf.h5py\n",
      "Took 94.61157 seconds to load file\n",
      "Size of features array for this batch is:\n",
      "(10, 641, 50)\n",
      "Index into batch for prf 0: 0\n",
      "Size of features array for this image set and prf is:\n",
      "(10, 641)\n",
      "Final size of features concatenated is [10 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "Final size of features concatenated is [10 x 641]\n",
      "Final size of features concatenated is [10 x 641]\n"
     ]
    }
   ],
   "source": [
    "images =np.arange(0,10);\n",
    "features, defined = _feature_extractor(images=images, prf_params=models[0], prf_model_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fc21b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(_feature_extractor.features_each_prf_batch[0,:,0]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b4de8c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4]),)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(_feature_extractor.features_each_prf_batch[0,:,40]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e29bf0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.n_ll_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cfccb3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9f1d5e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "         6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  7.,  7.,  7.,  7.,\n",
       "         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  8.,\n",
       "         9.,  8.,  9.,  8.,  9.,  8.,  9.,  8.,  9., 10., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 13., 13., 13., 13., 13., 13., 13., 13., 13.,\n",
       "        13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13.,\n",
       "        13., 13., 13., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14.,\n",
       "        14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14.,\n",
       "        14., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15.,\n",
       "        15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 19., 19., 19.,\n",
       "        19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19.,\n",
       "        19., 19., 19., 19.]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.zgroup_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c6e89c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 19]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.n_ll_actual_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "918c5cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks, names = _feature_extractor.get_partial_versions()\n",
    "masks[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3c2103ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import h5py\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import pyrtools as pt\n",
    "from utils import numpy_utils, torch_utils, texture_utils, prf_utils, default_paths\n",
    "pyramid_texture_feat_path = default_paths.pyramid_texture_feat_path\n",
    "from sklearn import decomposition\n",
    "class texture_feature_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module to compute higher-order texture statistics of input images (e.g. Portilla & Simoncelli 2000, IJCV)\n",
    "    Statistics are computed within a specified region of space (a voxel's pRF)\n",
    "    Can specify different subsets of features to include (i.e. pixel-level stats, simple/complex cells, \n",
    "    cross-correlations, auto-correlations)\n",
    "    Inputs to the forward pass are images and pRF parameters of interest [x,y,sigma]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,_fmaps_fn, subject=None, sample_batch_size=100, feature_types_exclude=None, n_prf_sd_out=2, \\\n",
    "                 aperture=1.0, do_varpart=False, zscore_in_groups=False, group_all_hl_feats=False, compute_features=True, \\\n",
    "                use_pca_feats_ll=False, use_pca_feats_hl=False, min_pct_var = 99, max_pc_to_retain_ll = 100, \\\n",
    "                 max_pc_to_retain_hl=100, device=None):\n",
    "        \n",
    "        super(texture_feature_extractor, self).__init__()\n",
    "        \n",
    "        self.subject = subject\n",
    "        self.fmaps_fn = _fmaps_fn   \n",
    "        self.n_sf = _fmaps_fn.pyr_height\n",
    "        self.n_ori =  _fmaps_fn.n_ori\n",
    "       \n",
    "        self.sample_batch_size = sample_batch_size       \n",
    "        self.n_prf_sd_out = n_prf_sd_out\n",
    "        self.aperture = aperture\n",
    "        self.device = device       \n",
    "        \n",
    "        self.do_varpart = do_varpart\n",
    "        self.group_all_hl_feats = group_all_hl_feats\n",
    "   \n",
    "        self.update_feature_list(feature_types_exclude)\n",
    "        self.use_pca_feats_hl = use_pca_feats_hl\n",
    "        self.use_pca_feats_ll = use_pca_feats_ll\n",
    "        self.n_ll_feats = np.sum(np.array(self.feature_type_dims_include)[self.feature_is_ll])\n",
    "        self.n_hl_feats = np.sum(np.array(self.feature_type_dims_include)[~self.feature_is_ll])\n",
    "        \n",
    "        self.any_pca=False\n",
    "        self.min_pct_var = min_pct_var\n",
    "        if self.use_pca_feats_hl:\n",
    "            self.any_pca=True\n",
    "            assert(self.group_all_hl_feats==True)\n",
    "            assert(len(self.feature_types_exclude)==0)\n",
    "            self.features_file_hl = os.path.join(pyramid_texture_feat_path, 'PCA', \\\n",
    "                     'S%d_%dori_%dsf_PCA_higher-level_only.npy'%(subject,self.n_ori, self.n_sf))   \n",
    "            if not os.path.exists(self.features_file_hl):\n",
    "                raise RuntimeError('Looking at %s for precomputed pca features, not found.'%self.features_file_hl)   \n",
    "            self.max_pc_to_retain_hl = np.min([self.n_hl_feats, max_pc_to_retain])\n",
    "            \n",
    "        if self.use_pca_feats_ll:\n",
    "            self.any_pca=True\n",
    "            assert(self.group_all_hl_feats==True)\n",
    "            assert(len(self.feature_types_exclude)==0)\n",
    "            self.features_file_ll = os.path.join(pyramid_texture_feat_path, 'PCA', \\\n",
    "                     'S%d_%dori_%dsf_PCA_lower-level_only.npy'%(subject,self.n_ori, self.n_sf)) \n",
    "            if not os.path.exists(self.features_file_ll):\n",
    "                raise RuntimeError('Looking at %s for precomputed pca features, not found.'%self.features_file_ll)   \n",
    "            self.max_pc_to_retain_ll = np.min([self.n_ll_feats, max_pc_to_retain])\n",
    "        \n",
    "        self.zscore_in_groups = zscore_in_groups\n",
    "        if self.zscore_in_groups:\n",
    "            # Define groups of columns to zscore within.           \n",
    "            assert(len(self.feature_types_exclude)==0)\n",
    "            assert(self.any_pca==False)      \n",
    "            dims = np.array(_feature_extractor.feature_type_dims_all)\n",
    "            # Treat each pixelwise stat as a separate group since diff scales\n",
    "            zgroup_sizes = [1,1,1,1,1,1] + list(dims[1:])          \n",
    "            zgroup_labels = np.concatenate([np.ones(shape=(1, zgroup_sizes[ff]))*ff \\\n",
    "                                                   for ff in range(len(zgroup_sizes))], axis=1)\n",
    "            # For the marginal stats of lowpass recons, separating skew/kurtosis here\n",
    "            zgroup_labels[zgroup_labels>8] = zgroup_labels[zgroup_labels>8]+1\n",
    "            zgroup_labels[0,np.where(zgroup_labels==8)[1][np.arange(1,10,2)]] = 9\n",
    "            self.zgroup_labels = zgroup_labels\n",
    "            \n",
    "        # if compute features is false, this means the features are already generated, so will be looking for a \n",
    "        # saved h5py file of pre-computed features. If true, will run the extraction step now.\n",
    "        self.compute_features = compute_features\n",
    "        \n",
    "        if not self.compute_features:\n",
    "            self.features_file = os.path.join(pyramid_texture_feat_path, 'S%d_features_each_prf_%dori_%dsf.h5py'%(self.subject, self.n_ori, self.n_sf))\n",
    "            if not os.path.exists(self.features_file):\n",
    "                raise RuntimeError('Looking at %s for precomputed features, not found.'%self.features_file)                \n",
    "            self.prf_batch_size=50\n",
    "            self.features_each_prf_batch = None\n",
    "            self.n_ll_actual_batch = None\n",
    "            self.n_hl_actual_batch = None\n",
    "            self.prf_inds_loaded = []\n",
    "        else:\n",
    "            self.fmaps = None\n",
    "    \n",
    "    def init_for_fitting(self, image_size, models, dtype):\n",
    "\n",
    "        \"\"\"\n",
    "        Additional initialization operations.\n",
    "        \"\"\"\n",
    "       \n",
    "        print('Initializing for fitting')\n",
    "        self.max_features = self.n_ll_feats+self.n_hl_feats\n",
    "        self.clear_big_features()\n",
    "        \n",
    "        if not self.compute_features:\n",
    "            # Prepare for loading the pre-computed features: as a compromise between speed and ram usage, will load them in\n",
    "            # batches of multiple prfs at a time. \n",
    "            n_prfs = models.shape[0]\n",
    "            n_prf_batches = int(np.ceil(n_prfs/self.prf_batch_size))          \n",
    "            self.prf_batch_inds = [np.arange(self.prf_batch_size*bb, np.min([self.prf_batch_size*(bb+1), n_prfs])) for bb in range(n_prf_batches)]\n",
    "        \n",
    "    def update_feature_list(self, feature_types_exclude):\n",
    "        \n",
    "        # First defining all the possible features and their dimensionality (fixed)\n",
    "        feature_types_all = ['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', \\\n",
    "            'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', \\\n",
    "            'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', \\\n",
    "            'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
    "        self.feature_types_all = feature_types_all\n",
    "        feature_type_dims = [6,16,16,10,1,\\\n",
    "                        272,73,25,\\\n",
    "                        24,24,48,96,\\\n",
    "                       10,20]\n",
    "        self.feature_is_ll = list(np.ones((5,))==1) + list(np.zeros((9,))==1)\n",
    "        self.feature_type_dims_all = feature_type_dims        \n",
    "         \n",
    "        # Decide which features to ignore, or use all features      \n",
    "        if feature_types_exclude is None:\n",
    "            feature_types_exclude = []\n",
    "        # a few shorthands for ignoring sets of features at a time\n",
    "        if 'crosscorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'crosscorrs' in ff])\n",
    "        if 'autocorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'autocorrs' in ff])\n",
    "        if 'pixel' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['pixel_stats'])\n",
    "        self.feature_types_exclude = feature_types_exclude\n",
    "        print('Feature types to exclude from the model:')\n",
    "        print(self.feature_types_exclude)    \n",
    "\n",
    "        # Now list all the features that we do want to use\n",
    "        self.feature_types_include  = [ff for ff in feature_types_all if not ff in self.feature_types_exclude]\n",
    "        if len(self.feature_types_include)==0:\n",
    "            raise ValueError('you have specified too many features to exclude, and now you have no features left! aborting.')\n",
    "            \n",
    "        self.feature_type_dims_include = [feature_type_dims[fi] for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in self.feature_types_exclude]\n",
    "        # how many features will be needed, in total?\n",
    "        self.n_features_total = np.sum(self.feature_type_dims_include)\n",
    "        self.feature_is_ll = np.array([self.feature_is_ll[fi] for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in self.feature_types_exclude])\n",
    "\n",
    "        # Numbers that define which feature types are in which columns of final output matrix\n",
    "        self.feature_column_labels = np.squeeze(np.concatenate([fi*np.ones([1,self.feature_type_dims_include[fi]]) for fi in range(len(self.feature_type_dims_include))], axis=1).astype('int'))\n",
    "        assert(np.size(self.feature_column_labels)==self.n_features_total)\n",
    "        \n",
    "        if self.group_all_hl_feats:\n",
    "            # In this case pretend there are just two groups of features:\n",
    "            # Lower-level which includes pixel, gabor-like, and marginal stats of lowpass/highpass recons.\n",
    "            # Higher-level which includes all autocorrelations and cross-correlations. \n",
    "            # This makes it simpler to do variance partition analysis.\n",
    "            # if do_varpart=False, this does nothing.\n",
    "            self.feature_column_labels[self.feature_is_ll[self.feature_column_labels]] = 0\n",
    "            self.feature_column_labels[~self.feature_is_ll[self.feature_column_labels]] = 1\n",
    "            self.feature_group_names = ['lower-level', 'higher-level']\n",
    "            \n",
    "            print('Grouping lower level features:')\n",
    "            print(np.array(self.feature_types_include)[self.feature_is_ll])\n",
    "            print('Grouping higher level features:')\n",
    "            print(np.array(self.feature_types_include)[~self.feature_is_ll])\n",
    "        else:\n",
    "            self.feature_group_names = self.feature_types_include\n",
    "            \n",
    "    def get_partial_versions(self):\n",
    "        \n",
    "        if not hasattr(self, 'max_features'):\n",
    "            raise RuntimeError('need to run init_for_fitting first')\n",
    "            \n",
    "        n_feature_types = len(self.feature_group_names)\n",
    "        partial_version_names = ['full_model'] \n",
    "        masks = np.ones([1,self.max_features])\n",
    "        \n",
    "        if self.do_varpart and n_feature_types>1:\n",
    "            \n",
    "            # \"Partial versions\" will be listed as: [full model, model w only first set of features, model w only second set, ...             \n",
    "            partial_version_names += ['just_%s'%ff for ff in self.feature_group_names]\n",
    "            masks2 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels==ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "            masks = np.concatenate((masks, masks2), axis=0)\n",
    "            \n",
    "            if n_feature_types > 2:\n",
    "                # if more than two types, also include models where we leave out first set of features, leave out second set of features...]\n",
    "                partial_version_names += ['leave_out_%s'%ff for ff in self.feature_group_names]           \n",
    "                masks3 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels!=ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "                masks = np.concatenate((masks, masks3), axis=0)           \n",
    "        \n",
    "        # masks always goes [n partial versions x n total features]\n",
    "        return masks, partial_version_names\n",
    "\n",
    "    \n",
    "    def get_maps(self, images):\n",
    "    \n",
    "        print('Running steerable pyramid feature extraction...')\n",
    "        print('Images array shape is:')\n",
    "        print(images.shape)\n",
    "        t = time.time()\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = torch_utils.get_value(images)\n",
    "        fmaps = self.fmaps_fn(images, to_torch=False, device=self.device)        \n",
    "        self.fmaps = fmaps\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    def load_precomputed_features(self, image_inds, prf_model_index):\n",
    "    \n",
    "        if prf_model_index not in self.prf_inds_loaded:\n",
    "            \n",
    "            batch_to_use = np.where([prf_model_index in self.prf_batch_inds[bb] for \\\n",
    "                                         bb in range(len(self.prf_batch_inds))])[0][0]\n",
    "            assert(prf_model_index in self.prf_batch_inds[batch_to_use])\n",
    "            self.prf_inds_loaded = self.prf_batch_inds[batch_to_use]\n",
    "            print('Loading pre-computed features for models [%d - %d] from %s'%(self.prf_batch_inds[batch_to_use][0], \\\n",
    "                                                                              self.prf_batch_inds[batch_to_use][-1], self.features_file))\n",
    "            self.features_each_prf_batch = None\n",
    "            self.n_ll_actual_batch = None\n",
    "            self.n_hl_actual_batch = None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if self.use_pca_feats_ll==False or self.use_pca_feats_hl==False:\n",
    "                \n",
    "                t = time.time()\n",
    "                with h5py.File(self.features_file, 'r') as data_set:\n",
    "                    values = np.copy(data_set['/features'][:,:,self.prf_batch_inds[batch_to_use]])\n",
    "                    data_set.close() \n",
    "                elapsed = time.time() - t\n",
    "                print('Took %.5f seconds to load file'%elapsed)\n",
    "\n",
    "                self.features_each_prf_batch = values[image_inds,:,:]\n",
    "                values=None\n",
    "\n",
    "            if self.any_pca:\n",
    "                \n",
    "                if self.use_pca_feats_ll:\n",
    "                    # loading pre-computed pca features, and deciding here how many features to include in model.\n",
    "                    pc_result = np.load(self.features_file_ll, allow_pickle=True).item()\n",
    "                    scores_each_prf = [pc_result['scores'][mm] for mm in self.prf_batch_inds[batch_to_use]]\n",
    "                    ev_each_prf = [pc_result['ev'][mm] for mm in self.prf_batch_inds[batch_to_use]]\n",
    "                    pc_result = None\n",
    "                    n_pcs_avail = scores_each_prf[0].shape[1]\n",
    "                    n_feat_each_prf = [np.where(np.cumsum(ev)>self.min_pct_var)[0][0] \\\n",
    "                                       if np.size(np.where(np.cumsum(ev)>self.min_pct_var))>0 \\\n",
    "                                       else n_pcs_avail for ev in ev_each_prf]\n",
    "                    n_feat_each_prf = [np.min([nf, self.max_pc_to_retain_ll]) for nf in n_feat_each_prf]\n",
    "                    self.n_ll_actual_batch = n_feat_each_prf\n",
    "                    # putting these all into a 3d array - different sizes originally, so padding w zeros here\n",
    "                    features_each_prf_ll = np.zeros((len(image_inds), self.n_ll_feats, len(n_feat_each_prf)))\n",
    "                    for mm in range(len(n_feat_each_prf)):\n",
    "                        features_each_prf_ll[:,0:n_feat_each_prf[mm],mm] = scores_each_prf[mm][image_inds,0:n_feat_each_prf[mm]]\n",
    "                    \n",
    "                else:                    \n",
    "                    features_each_prf_ll = self.features_each_prf_batch[:,0:self.n_ll_feats,:]\n",
    "                    self.n_ll_actual_batch = [self.n_ll_feats for bb in self.prf_batch_inds[batch_to_use]]\n",
    "                    \n",
    "                if self.use_pca_feats_hl:\n",
    "                    # loading pre-computed pca features, and deciding here how many features to include in model.\n",
    "                    pc_result = np.load(self.features_file_hl, allow_pickle=True).item()\n",
    "                    scores_each_prf = [pc_result['scores'][mm] for mm in self.prf_batch_inds[batch_to_use]]\n",
    "                    ev_each_prf = [pc_result['ev'][mm] for mm in self.prf_batch_inds[batch_to_use]]\n",
    "                    pc_result=None\n",
    "                    n_pcs_avail = scores_each_prf[0].shape[1]\n",
    "                    n_feat_each_prf = [np.where(np.cumsum(ev)>self.min_pct_var)[0][0] \\\n",
    "                                       if np.size(np.where(np.cumsum(ev)>self.min_pct_var))>0 \\\n",
    "                                       else n_pcs_avail for ev in ev_each_prf]\n",
    "                    n_feat_each_prf = [np.min([nf, self.max_pc_to_retain_hl]) for nf in n_feat_each_prf]\n",
    "                    self.n_hl_actual_batch = n_feat_each_prf\n",
    "                    features_each_prf_hl = np.zeros((len(image_inds), self.n_hl_feats, len(n_feat_each_prf)))\n",
    "                    for mm in range(len(n_feat_each_prf)):\n",
    "                        features_each_prf_hl[:,0:n_feat_each_prf[mm],mm] = scores_each_prf[mm][image_inds,0:n_feat_each_prf[mm]]\n",
    "                     \n",
    "                else:                    \n",
    "                    features_each_prf_hl = self.features_each_prf_batch[:,0:self.n_hl_feats,:]\n",
    "                    self.n_hl_actual_batch = [self.n_hl_feats for bb in self.prf_batch_inds[batch_to_use]]\n",
    "                    \n",
    "                self.features_each_prf_batch = np.concatenate([features_each_prf_ll, features_each_prf_hl], axis=1)\n",
    "                \n",
    "            print('Size of features array for this batch is:')\n",
    "            print(self.features_each_prf_batch.shape)\n",
    "            \n",
    "        else:\n",
    "            assert(len(image_inds)==self.features_each_prf_batch.shape[0])\n",
    "            \n",
    "        index_into_batch = np.where(prf_model_index==self.prf_inds_loaded)[0][0]\n",
    "        print('Index into batch for prf %d: %d'%(prf_model_index, index_into_batch))\n",
    "        features_in_prf = self.features_each_prf_batch[:,:,index_into_batch]\n",
    "        values=None\n",
    "        print('Size of features array for this image set and prf is:')\n",
    "        print(features_in_prf.shape)\n",
    "        if self.any_pca==False:\n",
    "            feature_inds_defined = None # have to compute this later\n",
    "        else:\n",
    "            feature_inds_defined = np.zeros((self.max_features,), dtype=bool)\n",
    "            feature_inds_defined[0:self.n_ll_actual_batch[index_into_batch]] = 1\n",
    "            feature_inds_defined[self.n_ll_feats:self.n_ll_feats+self.n_hl_actual_batch[index_into_batch]] = 1\n",
    "            \n",
    "            \n",
    "        return features_in_prf, feature_inds_defined\n",
    "        \n",
    "    \n",
    "    def clear_big_features(self):\n",
    "        \n",
    "        if self.compute_features:\n",
    "            print('Clearing steerable pyramid features from memory.')\n",
    "            self.fmaps = None\n",
    "        else:\n",
    "            print('Clearing precomputed features from memory.')\n",
    "            self.features_each_prf_batch = None\n",
    "            self.n_ll_actual_batch = None\n",
    "            self.n_hl_actual_batch = None\n",
    "            self.prf_inds_loaded = []\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "    def forward(self, images, prf_params, prf_model_index, fitting_mode=True):\n",
    "        \n",
    "        if not self.compute_features:\n",
    "            \n",
    "            # Load from file the features for this set of images\n",
    "            # In this case, the item passed in through \"images\" must actually be the indices of the images to use, not images themselves.\n",
    "            # Check to make sure this is the case.\n",
    "            assert(len(images.shape)==1)\n",
    "            image_inds = images\n",
    "            features, feature_inds_defined = self.load_precomputed_features(image_inds, prf_model_index)            \n",
    "            assert(features.shape[0]==len(image_inds))\n",
    "            features = torch_utils._to_torch(features, self.device)\n",
    "            \n",
    "            # Choosing which of these columns to include in model (might be all)\n",
    "            feature_column_labels_all = np.squeeze(np.concatenate([fi*np.ones([1,self.feature_type_dims_all[fi]]) for fi in range(len(self.feature_type_dims_all))], axis=1).astype('int'))\n",
    "            all_feat = OrderedDict()\n",
    "            for fi, ff in enumerate(self.feature_types_all):\n",
    "                if ff in self.feature_types_include:\n",
    "                    all_feat[ff] = features[:,feature_column_labels_all==fi]\n",
    "                else:\n",
    "                    all_feat[ff] = None\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.fmaps is None:\n",
    "                self.get_maps(images)\n",
    "            else:\n",
    "                assert(images.shape[0]==self.fmaps[0][0].shape[0])\n",
    "\n",
    "            if isinstance(prf_params, torch.Tensor):\n",
    "                prf_params = torch_utils.get_value(prf_params)\n",
    "            assert(np.size(prf_params)==3)\n",
    "            prf_params = np.squeeze(prf_params)\n",
    "            if isinstance(images, torch.Tensor):\n",
    "                images = torch_utils.get_value(images)\n",
    "\n",
    "            print('Computing higher order correlations...')\n",
    "\n",
    "            t = time.time()\n",
    "            pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "                magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "                magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "                real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs =  \\\n",
    "                        get_higher_order_features(self.fmaps, images, prf_params, sample_batch_size=self.sample_batch_size, n_prf_sd_out=self.n_prf_sd_out, aperture=self.aperture, device=self.device)\n",
    "            if torch.any(torch.abs(pixel_stats)>10**6):\n",
    "                print('WARNING THERE ARE SOME VERY BIG VALUES (>10^6) IN PIXEL STATS')\n",
    "                print(torch.max(pixel_stats))\n",
    "                \n",
    "            elapsed =  time.time() - t\n",
    "            print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "            all_feat = OrderedDict({'pixel_stats':pixel_stats, 'mean_magnitudes':mean_magnitudes, 'mean_realparts':mean_realparts, \\\n",
    "                                    'marginal_stats_lowpass_recons':marginal_stats_lowpass_recons, 'variance_highpass_resid':variance_highpass_resid, \\\n",
    "                'magnitude_feature_autocorrs':magnitude_feature_autocorrs, 'lowpass_recon_autocorrs':lowpass_recon_autocorrs, 'highpass_resid_autocorrs':highpass_resid_autocorrs, \\\n",
    "                'magnitude_within_scale_crosscorrs':magnitude_within_scale_crosscorrs, 'real_within_scale_crosscorrs':real_within_scale_crosscorrs, \\\n",
    "                'magnitude_across_scale_crosscorrs':magnitude_across_scale_crosscorrs, 'real_imag_across_scale_crosscorrs':real_imag_across_scale_crosscorrs, \\\n",
    "                'real_spatshift_within_scale_crosscorrs':real_spatshift_within_scale_crosscorrs, 'real_spatshift_across_scale_crosscorrs':real_spatshift_across_scale_crosscorrs})\n",
    "\n",
    "        # Now concatenating everything to a big matrix\n",
    "        feature_names_full = list(all_feat.keys())\n",
    "        feature_names = [fname for fname in feature_names_full if fname in self.feature_types_include]\n",
    "        assert(feature_names==self.feature_types_include) # double check here that the order is correct\n",
    "\n",
    "        for ff, feature_name in enumerate(feature_names):   \n",
    "            assert(all_feat[feature_name] is not None)\n",
    "            if ff==0:\n",
    "                all_feat_concat = all_feat[feature_name]\n",
    "            else:               \n",
    "                all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "\n",
    "        assert(all_feat_concat.shape[1]==self.n_features_total)\n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        print('Feature types included are:')\n",
    "        print(feature_names)\n",
    "        \n",
    "        if torch.any(torch.isnan(all_feat_concat)):\n",
    "            print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "        if torch.any(torch.all(all_feat_concat==0, axis=0)):\n",
    "            print('\\nWARNING THERE ARE ZEROS IN FEATURES MATRIX\\n')\n",
    "            print('zeros for columns:')\n",
    "            print(np.where(torch_utils.get_value(torch.all(all_feat_concat==0, axis=0))))\n",
    "\n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        \n",
    "        if not self.any_pca:\n",
    "            feature_inds_defined = np.ones((self.n_features_total,), dtype=bool)\n",
    "        \n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        \n",
    "        return all_feat_concat, feature_inds_defined\n",
    "    \n",
    "    \n",
    "class steerable_pyramid_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module that utilizes steerable pyramid ( https://pyrtools.readthedocs.io/en/latest/) to extract features.\n",
    "    For a batch of input images, will return all the pyramid coefficients, as well as additional types of feature maps\n",
    "    (i.e. partially reconstructed lowpass images at several frequency levels, upsampled feature maps).\n",
    "    These are used by 'get_higher_order_features' to extract various textural features of the image.\n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pyr_height=4, n_ori=8):\n",
    "        \n",
    "        super(steerable_pyramid_extractor, self).__init__()       \n",
    "        self.n_ori = n_ori\n",
    "        self.pyr_height = pyr_height # how many spatial frequencies?\n",
    "        self.pyr = None\n",
    "        \n",
    "    def forward(self, image_batch, to_torch=False, device=None):\n",
    "        \n",
    "        batch_size = image_batch.shape[0]\n",
    "        t  = time.time()\n",
    "        for ii in range(batch_size):\n",
    "            \n",
    "            # Call the pyramid generation code here, get all features for this image.\n",
    "            pyr = pt.pyramids.SteerablePyramidFreq(image_batch[ii,0,:,:], is_complex=True, height = self.pyr_height, order = self.n_ori-1)\n",
    "            self.pyr = pyr # storing the most recently generated pyramid, in case we need its properties later\n",
    "            \n",
    "            if ii==0:  \n",
    "                \n",
    "                # Initialize all the feature maps we want to store...\n",
    "                fmaps_complex = []\n",
    "                fmaps_coarser_upsampled = []\n",
    "               \n",
    "                # Will generate several low-pass filtered representations of the image - will use these as additional feature maps for \n",
    "                # computing autocorrelations and marginal statistics.\n",
    "                fmaps_lowpass_recon = []\n",
    "                fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "               \n",
    "                # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "                sf_reverse  = self.pyr_height  # need to go backward because pyr comes out in the opposite order from what i want\n",
    "                for sf in range(self.pyr_height):\n",
    "                    sf_reverse -= 1\n",
    "                    fmaps_complex.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))\n",
    "                    \n",
    "                    # Initialize \"parent\" representations for this level (map from the next coarsest resolution, upsampled to the finer scale)\n",
    "                    # To be used for cross-scale comparisons.\n",
    "                    if sf==0:\n",
    "                        # this will be the lowpass residual (non-oriented).\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, 1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))                     \n",
    "                    else:\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))     \n",
    "                        \n",
    "                    fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                         pyr.pyr_coeffs[sf_reverse,0].shape[1])))\n",
    "\n",
    "                fmaps_resid = []    \n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_highpass'].shape[0],pyr.pyr_coeffs['residual_highpass'].shape[1])))\n",
    "\n",
    "            # First get lowpass filtered representation of the image\n",
    "            lowpass_recon = pyr.recon_pyr(levels='residual_lowpass', bands='all', twidth=1) \n",
    "            scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size['residual_lowpass'][0]\n",
    "            lowpass_recon = texture_utils.shrink(lowpass_recon, scale_by)*scale_by**2\n",
    "            fmaps_lowpass_recon[0][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "            # Get the \"parent\" for lowest SF level (upsample the residual lowpass)\n",
    "#             print(pyr.pyr_coeffs['residual_lowpass'].shape)\n",
    "            upsampled = texture_utils.expand(pyr.pyr_coeffs['residual_lowpass'], factor=2)/2**2\n",
    "#             print(upsampled.shape)\n",
    "            fmaps_coarser_upsampled[0][ii,0,:,:] = upsampled\n",
    "            \n",
    "            # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "            sf_reverse  = self.pyr_height # need to go backward because pyr comes out in the opposite order from what i want\n",
    "            for sf in range(self.pyr_height):\n",
    "                sf_reverse -= 1\n",
    "                for oo in range(self.n_ori):     \n",
    "\n",
    "                    # These are the main feature maps of the pyramid - one feature map per scale per orientation band.\n",
    "                    # Complex number, can take the magnitude or real/imaginary part to simulate complex or simple cell-type responses.\n",
    "                    fmaps_complex[sf][ii,oo,:,:] = pyr.pyr_coeffs[(sf_reverse,oo)]\n",
    "                    \n",
    "                    if sf<self.pyr_height-1:\n",
    "                        # Store this as a \"parent\" representation, will be used for the next most fine SF level (i.e. sf+1)\n",
    "                        upsampled = texture_utils.expand(pyr.pyr_coeffs[(sf_reverse,oo)], factor=2)/2**2\n",
    "                        # Double the phase (angle of the complex number); note this doesn't affect the magnitude. \n",
    "                        phase_doubled = texture_utils.double_phase(upsampled)\n",
    "                        fmaps_coarser_upsampled[sf+1][ii,oo,:,:] = phase_doubled\n",
    "                   \n",
    "                        \n",
    "                # Get the bandpass filtered representation for this scale\n",
    "                bandpass_image = np.real(pyr.recon_pyr(levels=sf_reverse, bands='all', twidth=1))\n",
    "                scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size[(sf_reverse,0)][0]\n",
    "                bandpass_image = texture_utils.shrink(bandpass_image, factor=scale_by)*scale_by**2\n",
    "                \n",
    "                # Add it onto the lowpass_recon (gets modified every loop iteration)\n",
    "                lowpass_recon = texture_utils.expand(lowpass_recon, factor=2)/2**2\n",
    "                lowpass_recon = lowpass_recon + bandpass_image\n",
    "                fmaps_lowpass_recon[sf+1][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "\n",
    "            # Grab residual feature maps, the lowest and highest levels of the pyramid\n",
    "            fmaps_resid[0][ii,0,:,:] = pyr.pyr_coeffs['residual_lowpass']\n",
    "            fmaps_resid[1][ii,0,:,:] = pyr.pyr_coeffs['residual_highpass']\n",
    "            \n",
    "            \n",
    "        elapsed = time.time() - t\n",
    "#         print('time elapsed: %.5f s'%elapsed)\n",
    "\n",
    "        if to_torch:            \n",
    "            fmaps_complex = [torch.from_numpy(fm).to(device) for fm in fmaps_complex]            \n",
    "            fmaps_resid = [torch_utils._to_torch(fm, device=device) for fm in fmaps_resid]\n",
    "            fmaps_lowpass_recon = [torch_utils._to_torch(fm, device=device) for fm in fmaps_lowpass_recon]                      \n",
    "            fmaps_coarser_upsampled = [torch.from_numpy(fm).to(device) for fm in fmaps_coarser_upsampled]\n",
    "\n",
    "        return fmaps_complex, fmaps_resid, fmaps_lowpass_recon, fmaps_coarser_upsampled\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "def get_higher_order_features(fmaps, images, prf_params, sample_batch_size=20, n_prf_sd_out=2, aperture=1.0, device=None, keep_orig_shape=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute higher order texture features for a batch of images.\n",
    "    Input the module that defines steerable pyramid (i.e. 'steerable_pyramid_extractor'), and desired prf parameters.\n",
    "    Returns arrays of each higher order feature.  \n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "\n",
    "    fmaps_complex_all, fmaps_resid_all, fmaps_lowpass_recon_all, fmaps_coarser_upsampled_all = fmaps\n",
    "   \n",
    "    n_trials = fmaps_complex_all[0].shape[0]\n",
    "    x,y,sigma = prf_params\n",
    "\n",
    "    n_sf = len(fmaps_complex_all)\n",
    "    n_ori = fmaps_complex_all[0].shape[1]\n",
    "        \n",
    "    # all pairs of different orientation channels.\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "\n",
    "    # mean, variance, skew, kurtosis, min, max\n",
    "    pixel_stats = torch.zeros((n_trials,6), device=device)\n",
    "\n",
    "    # Mean magnitude each scale/orientation, within the prf.\n",
    "    mean_magnitudes = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "    mean_realparts = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "\n",
    "    # Store the skew and kurtosis of the lowpass reconstructions at each scale\n",
    "    marginal_stats_lowpass_recons = torch.zeros((n_trials, n_sf+1, 2), device=device)\n",
    "\n",
    "    # Variance of the highpass residual\n",
    "    variance_highpass_resid = torch.zeros((n_trials, 1), device=device)\n",
    "\n",
    "    # how many unique autocorrelation values will we get out for each feature map? These will be pre-defined, same for every pRF.\n",
    "    # but different for different scales of feature maps.\n",
    "    # note also that for bigger prfs, there will potentially be more pixels that contribute to the autocorrelation computation - \n",
    "    # but a fixed portion of the matrix is returned.\n",
    "    autocorr_output_pix=np.array([3,3,5,7,7])\n",
    "    n_autocorr_vals = ((autocorr_output_pix**2+1)/2).astype('int')\n",
    "    max_autocorr_vals = np.max(n_autocorr_vals)\n",
    "    \n",
    "    # Spatial autocorrelation of the magnitude of spectral coefficients, within each scale and orientation.\n",
    "    magnitude_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, max_autocorr_vals], device=device) # this is ace in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the partially-reconstructed lowpass image representation at each scale\n",
    "    lowpass_recon_autocorrs = torch.zeros([n_trials, n_sf+1, max_autocorr_vals], device=device) # this is acr in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the highpass residual\n",
    "    highpass_resid_autocorrs = torch.zeros([n_trials, 1, max_autocorr_vals], device=device)\n",
    "\n",
    "    # Within scale correlations of feature maps: compare feature map magnitudes for different orientations.\n",
    "    magnitude_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is C0 in the matlab code\n",
    "    # Using the real parts.\n",
    "    real_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is Cr0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations of feature maps: always comparing each scale to an up-sampled version of the scale coarser than it.\n",
    "    magnitude_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_ori, n_ori], device=device) # this is Cx0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations, using the real and imaginary parts separately. The phase (angle) of the coarser map is doubled before computing these.\n",
    "    real_imag_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, 2, n_ori, n_ori], device=device) # this is Crx0 in the matlab code\n",
    "\n",
    "    # These are comparisons with spatially shifted versions of the lowpass residual. Not sure we need this...\n",
    "    n_spatshifts = 5;\n",
    "    real_spatshift_within_scale_crosscorrs = torch.zeros([n_trials, 1, n_spatshifts, n_spatshifts], device=device)# this is Cr0 in the matlab code\n",
    "    real_spatshift_across_scale_crosscorrs = torch.zeros([n_trials, 1, n_ori, n_spatshifts], device=device)  # this is Crx0 in the matlab code\n",
    "\n",
    "    # Looping over batches of trials to compute everything of interest.\n",
    "    bb=-1\n",
    "    for batch_inds, batch_size_actual in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "        bb=bb+1\n",
    "\n",
    "        fmaps_complex = [torch.from_numpy(fmaps_complex_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_complex_all))]\n",
    "        fmaps_resid = [torch.from_numpy(fmaps_resid_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_resid_all))]\n",
    "        fmaps_lowpass_recon = [torch.from_numpy(fmaps_lowpass_recon_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_lowpass_recon_all))]\n",
    "        fmaps_coarser_upsampled = [torch.from_numpy(fmaps_coarser_upsampled_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_coarser_upsampled_all))]\n",
    "\n",
    "        if bb==0:\n",
    "            npix_each_scale = [fmaps_complex_all[sc].shape[2] for sc in np.arange(n_sf-1,-1,-1)]\n",
    "            npix_each_scale.append(fmaps_resid_all[0].shape[2])\n",
    "            npix_each_scale.reverse()\n",
    "\n",
    "        # First working with the finest scale (original image)\n",
    "        n_pix = npix_each_scale[-1]      \n",
    "        g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[-1])\n",
    "\n",
    "        # Gather pixel-wise statistics here \n",
    "        wmean, wvar, wskew, wkurt = texture_utils.get_weighted_pixel_features(images[batch_inds], spatial_weights, device=device)\n",
    "        pixel_stats[batch_inds,0] = torch.squeeze(wmean)\n",
    "        pixel_stats[batch_inds,1] = torch.squeeze(wvar)\n",
    "        pixel_stats[batch_inds,2] = torch.squeeze(wskew)\n",
    "        pixel_stats[batch_inds,3] = torch.squeeze(wkurt)\n",
    "        pixel_stats[batch_inds,4] = torch_utils._to_torch(np.squeeze(np.min(np.min(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "        pixel_stats[batch_inds,5] = torch_utils._to_torch(np.squeeze(np.max(np.max(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "\n",
    "        # Autocorrs of the highpass residual\n",
    "        highpass_resid = fmaps_resid[1]\n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(highpass_resid, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[-1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        highpass_resid_autocorrs[batch_inds,0,0:n_autocorr_vals[-1]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[-1]])\n",
    "\n",
    "        # Variance of the highpass residual\n",
    "        m, wvar, s, k = texture_utils.get_weighted_pixel_features(highpass_resid, spatial_weights, device=device)\n",
    "        variance_highpass_resid[batch_inds,0] = torch.squeeze(wvar)\n",
    "\n",
    "        # Next work with the low-pass reconstruction (most coarse scale, smallest npix)\n",
    "        n_pix = npix_each_scale[0]       \n",
    "        g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[0])\n",
    "\n",
    "        lowpass_rec = fmaps_lowpass_recon[0]\n",
    "\n",
    "        # Marginal stats of low-pass reconstruction\n",
    "        m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_rec, spatial_weights, device=device)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,0] = torch.squeeze(wskew)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,1] = torch.squeeze(wkurt)\n",
    "\n",
    "        # Autocorrs of low-pass reconstruction \n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_rec, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[0], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        lowpass_recon_autocorrs[batch_inds,0,0:n_autocorr_vals[0]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[0]])\n",
    "\n",
    "        # Looping over spatial frequency/scale\n",
    "        # Loop goes low SF (smallest npix) to higher SF (largest npix)\n",
    "        for ff in range(n_sf):\n",
    "         \n",
    "            # Scale specific things - get the prf at this resolution of interest    \n",
    "            n_pix = npix_each_scale[ff+1]           \n",
    "            g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "            spatial_weights = g[2][0]\n",
    "            patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[1+ff])\n",
    "\n",
    "            # Get the low-pass reconstruction at this scale\n",
    "            lowpass_summed = fmaps_lowpass_recon[ff+1]  # this is summed over this scale band and those below it\n",
    "            m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_summed, spatial_weights, device=device)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,0] = torch.squeeze(wskew)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,1] = torch.squeeze(wkurt)\n",
    "\n",
    "            # Autocorrelations of low-pass reconstruction (at this scale)\n",
    "            auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_summed, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "            lowpass_recon_autocorrs[batch_inds,ff+1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "            # Loop over orientation channels\n",
    "            xx=-1\n",
    "            for oo1 in range(n_ori):       \n",
    "\n",
    "                # Magnitude of the complex coefficients; complex cell-like responses\n",
    "                mag1 = torch.abs(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                # The mean magnitudes here are basically second-order spectral statistics, within the specified spatial region defined by weights\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(mag1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_magnitudes[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "                \n",
    "                mag1 = mag1 - torch.tile(torch.mean(torch.mean(mag1, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                # Real parts of the complex coefficients; simple cell-like responses\n",
    "                real1 = torch.real(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()    \n",
    "                \n",
    "                # Average of the real parts within the specified spatial region\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(real1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_realparts[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "\n",
    "                # Complex cell autocorrelation (correlation w spatially shifted versions of itself)     \n",
    "                auto_corr = texture_utils.weighted_auto_corr_2d(mag1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "                magnitude_feature_autocorrs[batch_inds,ff,oo1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "                # Within-scale correlations - comparing resp at orient==oo1 to responses at all other orientations, same scale.\n",
    "                for oo2 in np.arange(oo1+1, n_ori):            \n",
    "                    xx = xx+1 \n",
    "                    assert(oo1==ori_pairs[xx,0] and oo2==ori_pairs[xx,1])\n",
    "\n",
    "                    # Magnitude at the other orientation (oo2)\n",
    "                    mag2 = torch.abs(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()      \n",
    "                    mag2 = mag2 - torch.tile(torch.mean(torch.mean(mag2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                    # Correlate the magnitude feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    magnitude_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                    # Real part at the other orientation (oo2)\n",
    "                    real2 = torch.real(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()                     \n",
    "\n",
    "                    # Correlate the real feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(real1, real2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    real_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                # Cross-scale correlations - for these we care about same ori to same ori, so looping over all orientations.\n",
    "                # Going to compare coefficients at the current scale to those at a coarser scale (ff-1)\n",
    "                # If we're at first scale (ff=0), then will use a different method.\n",
    "                if ff>0:\n",
    "\n",
    "                    for oo2 in range(n_ori):\n",
    "\n",
    "                        # Get magnitude of coefficients for neighboring (coarser) scale                        \n",
    "                        mag_coarser2 = torch.abs(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        mag_coarser2 = mag_coarser2 - torch.tile(torch.mean(torch.mean(mag_coarser2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                        # Correlate this with the finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)            \n",
    "                        magnitude_across_scale_crosscorrs[batch_inds,ff-1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        # Get the real and imaginary parts at coarser scale\n",
    "                        real_coarser2 = torch.real(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        imag_coarser2 = torch.imag(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                        # Correlate each of these with real part at finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,0,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, imag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # instead of different orientations for the \"parent\" level here, have spatially shifted versions.\n",
    "                    real_coarser = torch.real(fmaps_coarser_upsampled[ff][:,0,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                    shifts = [[0,0],[1,3],[-1,3],[1,2],[-1,2]]\n",
    "                    for si1, shift1 in enumerate(shifts):\n",
    "\n",
    "                        ss,dd = shift1\n",
    "                        real_coarser_shifted1 = torch.roll(real_coarser, shifts=ss, dims=dd)               \n",
    "                        # Real part at the finer scale compared to spatially shifted at the coarser scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser_shifted1, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_spatshift_across_scale_crosscorrs[batch_inds,ff,oo1,si1] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        for si2 in np.arange(si1+1, n_spatshifts):\n",
    "\n",
    "                            ss,dd = shifts[si2]\n",
    "                            real_coarser_shifted2 = torch.roll(real_coarser, shifts=ss, dims=dd) \n",
    "                            # Real parts at same scale, comparing spatially shifted.\n",
    "                            cross_corr = texture_utils.weighted_cross_corr_2d(real_coarser_shifted1, real_coarser_shifted2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                            real_spatshift_within_scale_crosscorrs[batch_inds,ff,si1,si2] = torch.squeeze(cross_corr)\n",
    "\n",
    "            \n",
    "    if not keep_orig_shape:\n",
    "        # Reshape everything to [ntrials x nfeatures]\n",
    "\n",
    "        mean_magnitudes = torch.reshape(mean_magnitudes, [n_trials, -1])\n",
    "        mean_realparts = torch.reshape(mean_realparts, [n_trials, -1])\n",
    "        marginal_stats_lowpass_recons = torch.reshape(marginal_stats_lowpass_recons, [n_trials, -1])\n",
    "        variance_highpass_resid =torch.reshape(variance_highpass_resid, [n_trials, -1])\n",
    "\n",
    "        magnitude_feature_autocorrs = torch.reshape(magnitude_feature_autocorrs, [n_trials, -1])\n",
    "        # take out the zero columns, which happen because of different size autocorr outputs.\n",
    "        magnitude_feature_autocorrs = magnitude_feature_autocorrs[:,torch.sum(magnitude_feature_autocorrs, axis=0)!=0]\n",
    "        assert(magnitude_feature_autocorrs.shape[1]==np.sum(n_autocorr_vals[1:]*n_ori))\n",
    "\n",
    "        lowpass_recon_autocorrs = torch.reshape(lowpass_recon_autocorrs, [n_trials, -1])\n",
    "        lowpass_recon_autocorrs = lowpass_recon_autocorrs[:,torch.sum(lowpass_recon_autocorrs, axis=0)!=0]\n",
    "        assert(lowpass_recon_autocorrs.shape[1]==np.sum(n_autocorr_vals))\n",
    "\n",
    "        highpass_resid_autocorrs = torch.reshape(highpass_resid_autocorrs, [n_trials, -1])\n",
    "\n",
    "        magnitude_within_scale_crosscorrs = torch.reshape(magnitude_within_scale_crosscorrs, [n_trials, -1])\n",
    "        real_within_scale_crosscorrs = torch.reshape(real_within_scale_crosscorrs, [n_trials, -1])\n",
    "        magnitude_across_scale_crosscorrs = torch.reshape(magnitude_across_scale_crosscorrs, [n_trials, -1])\n",
    "        real_imag_across_scale_crosscorrs = torch.reshape(real_imag_across_scale_crosscorrs, [n_trials, -1])\n",
    "        real_spatshift_within_scale_crosscorrs = torch.reshape(real_spatshift_within_scale_crosscorrs, [n_trials, -1])\n",
    "\n",
    "        real_spatshift_within_scale_crosscorrs = real_spatshift_within_scale_crosscorrs[:,torch.sum(real_spatshift_within_scale_crosscorrs, axis=0)!=0]\n",
    "        assert(real_spatshift_within_scale_crosscorrs.shape[1]==np.sum(np.arange(1,n_spatshifts)))\n",
    "\n",
    "        real_spatshift_across_scale_crosscorrs = torch.reshape(real_spatshift_across_scale_crosscorrs, [n_trials, -1])\n",
    "\n",
    "        \n",
    "    return pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "            magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "            magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "            real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c63773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
