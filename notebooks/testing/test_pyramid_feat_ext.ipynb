{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3957f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce GTX TITAN X\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import torch\n",
    "import time\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "from sklearn import decomposition\n",
    "\n",
    "#import custom modules\n",
    "code_dir = '/user_data/mmhender/imStat/code/'\n",
    "sys.path.append(code_dir)\n",
    "from utils import prf_utils, torch_utils, texture_utils, default_paths\n",
    "from model_fitting import initialize_fitting\n",
    "# from feature_extraction import texture_statistics_pyramid\n",
    "\n",
    "device = initialize_fitting.init_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862749f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nsd_utils\n",
    "batch_size=100\n",
    "subject=1\n",
    "n_sf = 4; \n",
    "n_ori = 4;\n",
    "debug=True\n",
    "use_node_storage = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8874b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_node_storage:\n",
    "    pyramid_texture_feat_path = default_paths.pyramid_texture_feat_path_localnode\n",
    "else:\n",
    "    pyramid_texture_feat_path = default_paths.pyramid_texture_feat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d628c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading images for subject 1\n",
      "\n",
      "image data size: (10000, 3, 227, 227) , dtype: uint8 , value range: 0 255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and prepare the image set to work with (all images for the current subject, 10,000 ims)\n",
    "stim_root = default_paths.stim_root\n",
    "image_data = nsd_utils.get_image_data(subject)  \n",
    "image_data = nsd_utils.image_uncolorize_fn(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2144167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n"
     ]
    }
   ],
   "source": [
    "# Need to have size a multiple of 8, for the pyramid to work right\n",
    "process_at_size=240\n",
    "resample_fn = torch.nn.Upsample((process_at_size, process_at_size), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "# Params for the spatial aspect of the model (possible pRFs)\n",
    "aperture = 1.0\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "# Set up the pyramid\n",
    "batch_size=2\n",
    "feature_types_exclude = []\n",
    "n_prf_sd_out = 2\n",
    "do_varpart=False # this doesn't do anything here\n",
    "group_all_hl_feats = False # this doesn't do anything here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e5348ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types to exclude from the model:\n",
      "[]\n",
      "Extracting features for images [0 - 1]\n",
      "Clearing steerable pyramid features from memory.\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.55, -0.55, 0.03999999910593033]\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "torch.Size([2, 1, 240, 240])\n",
      "time elapsed = 0.54063\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.30905\n",
      "Final size of features concatenated is [2 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "model 0, min/max of features in batch: [tensor(-3.9699, device='cuda:0'), tensor(56.7287, device='cuda:0')]\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.49210526315789477, -0.55, 0.03999999910593033]\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.27877\n",
      "Final size of features concatenated is [2 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "model 1, min/max of features in batch: [tensor(-61.3700, device='cuda:0'), tensor(245.0173, device='cuda:0')]\n",
      "Extracting features for images [2 - 3]\n",
      "Clearing steerable pyramid features from memory.\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.55, -0.55, 0.03999999910593033]\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "torch.Size([2, 1, 240, 240])\n",
      "time elapsed = 0.45544\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.22194\n",
      "Final size of features concatenated is [2 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "model 0, min/max of features in batch: [tensor(-12.6296, device='cuda:0'), tensor(57.8106, device='cuda:0')]\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.49210526315789477, -0.55, 0.03999999910593033]\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.22056\n",
      "Final size of features concatenated is [2 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "model 1, min/max of features in batch: [tensor(-29.6129, device='cuda:0'), tensor(62.9469, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "compute_features = True\n",
    "_fmaps_fn = steerable_pyramid_extractor(pyr_height = n_sf, n_ori = n_ori)\n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "_feature_extractor = texture_feature_extractor(_fmaps_fn,sample_batch_size=batch_size, feature_types_exclude=feature_types_exclude, \\\n",
    "                                               n_prf_sd_out=n_prf_sd_out, aperture=aperture, do_varpart = do_varpart, \\\n",
    "                              compute_features=compute_features, group_all_hl_feats = group_all_hl_feats, device=device)\n",
    "\n",
    "n_features = _feature_extractor.n_features_total\n",
    "n_images = image_data.shape[0]\n",
    "n_prfs = models.shape[0]\n",
    "n_batches = int(np.ceil(n_images/batch_size))\n",
    "\n",
    "features_each_prf = np.zeros((n_images, n_features, n_prfs))\n",
    "\n",
    "for bb in range(n_batches):\n",
    "\n",
    "    if debug and bb>1:\n",
    "        continue\n",
    "\n",
    "    batch_inds = np.arange(batch_size * bb, np.min([batch_size * (bb+1), n_images]))\n",
    "\n",
    "    print('Extracting features for images [%d - %d]'%(batch_inds[0], batch_inds[-1]))\n",
    "        \n",
    "    image_batch = torch_utils._to_torch(image_data[batch_inds,:,:,:], device)\n",
    "    \n",
    "    image_batch = resample_fn(image_batch)\n",
    "   \n",
    "    _feature_extractor.clear_big_features()\n",
    "    \n",
    "    for mm in range(n_prfs):\n",
    "\n",
    "        if debug and mm>1:\n",
    "            continue\n",
    "\n",
    "        x,y,sigma = models[mm,:]\n",
    "        print('Getting features for pRF [x,y,sigma]:')\n",
    "        print([x,y,sigma])\n",
    "\n",
    "        features_batch, _ = _feature_extractor(image_batch, models[mm],mm)\n",
    "\n",
    "        print('model %d, min/max of features in batch: [%s, %s]'%(mm, torch.min(features_batch), torch.max(features_batch))) \n",
    "\n",
    "        features_each_prf[batch_inds,:,mm] = torch_utils.get_value(features_batch)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6843b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prf features to /user_data/mmhender/features/pyramid_texture/S1_features_each_prf_4ori_4sf.h5py\n",
      "\n",
      "Took 43.46576 sec to write file\n"
     ]
    }
   ],
   "source": [
    "fn2save = os.path.join(pyramid_texture_feat_path, 'S%d_features_each_prf_%dori_%dsf.h5py'%(subject, n_ori, n_sf))\n",
    "\n",
    "print('Writing prf features to %s\\n'%fn2save)\n",
    "\n",
    "t = time.time()\n",
    "with h5py.File(fn2save, 'w') as data_set:\n",
    "    dset = data_set.create_dataset(\"features\", np.shape(features_each_prf), dtype=np.float64)\n",
    "    data_set['/features'][:,:,:] = features_each_prf\n",
    "    data_set.close()  \n",
    "elapsed = time.time() - t\n",
    "\n",
    "print('Took %.5f sec to write file'%elapsed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "428a3055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_each_prf.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types to exclude from the model:\n",
      "[]\n",
      "Loading pre-computed features from /user_data/mmhender/features/pyramid_texture/S1_features_each_prf_4ori_4sf.h5py\n"
     ]
    }
   ],
   "source": [
    "compute_features = False\n",
    "\n",
    "_fmaps_fn = steerable_pyramid_extractor(pyr_height = n_sf, n_ori = n_ori)\n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "_feature_extractor = texture_feature_extractor(_fmaps_fn,sample_batch_size=batch_size, feature_types_exclude=feature_types_exclude, \\\n",
    "                                               n_prf_sd_out=n_prf_sd_out, aperture=aperture, do_varpart = do_varpart, \\\n",
    "                              compute_features=compute_features, group_all_hl_feats = group_all_hl_feats, device=device)\n",
    "\n",
    "\n",
    "inds2use = np.arange(0,2)\n",
    "mm=0\n",
    "# image_batch = torch_utils._to_torch(image_data[inds2use,:,:,:], device)   \n",
    "# image_batch = resample_fn(image_batch)\n",
    "\n",
    "# feature_batch, _ = _feature_extractor(image_batch, models[mm], mm)\n",
    "feature_batch, _ = _feature_extractor(inds2use, models[mm], mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9371f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed features from /user_data/mmhender/features/pyramid_texture/S1_features_each_prf_4ori_4sf.h5py\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Unable to open file (file read failed: time = Fri Sep 17 18:31:54 2021\n, filename = '/user_data/mmhender/features/pyramid_texture/S1_features_each_prf_4ori_4sf.h5py', file descriptor = 69, errno = 5, error message = 'Input/output error', buf = 0x7ffffd092e48, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c42dca34094f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading pre-computed features from %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfn2save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn2save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'/features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    444\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 445\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Unable to open file (file read failed: time = Fri Sep 17 18:31:54 2021\n, filename = '/user_data/mmhender/features/pyramid_texture/S1_features_each_prf_4ori_4sf.h5py', file descriptor = 69, errno = 5, error message = 'Input/output error', buf = 0x7ffffd092e48, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "fn2save = os.path.join(pyramid_texture_feat_path, 'S%d_features_each_prf_%dori_%dsf.h5py'%(subject, n_ori, n_sf))\n",
    "\n",
    "print('Loading pre-computed features from %s'%fn2save)\n",
    "t = time.time()\n",
    "with h5py.File(fn2save, 'r') as data_set:\n",
    "    values = np.copy(data_set['/features'])\n",
    "    data_set.close() \n",
    "elapsed = time.time() - t\n",
    "print('Took %.5f seconds to load file'%elapsed)\n",
    "\n",
    "self.features_each_prf = values[inds2use,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2408fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pixel_stats',\n",
       " 'mean_magnitudes',\n",
       " 'mean_realparts',\n",
       " 'marginal_stats_lowpass_recons',\n",
       " 'variance_highpass_resid',\n",
       " 'magnitude_feature_autocorrs',\n",
       " 'lowpass_recon_autocorrs',\n",
       " 'highpass_resid_autocorrs',\n",
       " 'magnitude_within_scale_crosscorrs',\n",
       " 'real_within_scale_crosscorrs',\n",
       " 'magnitude_across_scale_crosscorrs',\n",
       " 'real_imag_across_scale_crosscorrs',\n",
       " 'real_spatshift_within_scale_crosscorrs',\n",
       " 'real_spatshift_across_scale_crosscorrs']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.feature_types_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "260c7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.random.normal(0,1,(2,641))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "523a5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type_dims_all = _feature_extractor.feature_type_dims_all\n",
    "feature_types_all = _feature_extractor.feature_types_all\n",
    "feature_types_include = _feature_extractor.feature_types_include\n",
    "# Choosing which of these columns to include in model (might be all)\n",
    "feature_column_labels_all = np.squeeze(np.concatenate([fi*np.ones([1,feature_type_dims_all[fi]]) for fi in range(len(feature_type_dims_all))], axis=1).astype('int'))\n",
    "all_feat = OrderedDict()\n",
    "for fi, ff in enumerate(feature_types_all):\n",
    "    if ff in feature_types_include:\n",
    "        all_feat[ff] = features[:,feature_column_labels_all==fi]\n",
    "    else:\n",
    "        all_feat[ff] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e0aa2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64864274, -0.05766403, -0.11315409,  0.39013013,  0.08890069,\n",
       "       -0.41363566,  0.36356258,  0.97814487,  0.02757046,  2.13463237,\n",
       "       -0.40927557, -1.04083409,  0.69508687,  0.78160623,  0.48580495,\n",
       "        0.24602694,  2.43595573, -0.30413834, -1.25075929, -0.48810806])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0,0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0262fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36356258,  0.97814487,  0.02757046,  2.13463237, -0.40927557,\n",
       "       -1.04083409,  0.69508687,  0.78160623,  0.48580495,  0.24602694,\n",
       "        2.43595573, -0.30413834, -1.25075929, -0.48810806])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0,0:20][feature_column_labels_all[0:20]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f52e49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64864274, -0.05766403, -0.11315409,  0.39013013,  0.08890069,\n",
       "        -0.41363566],\n",
       "       [-1.12960324, -0.19387951, -0.17234212, -0.55538908, -3.07988625,\n",
       "        -1.17531623]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat['pixel_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d6b09b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1d14682f7d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0mall_feat_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m        \u001b[0mall_feat_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feat_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feat_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0m_feature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    " # Now concatenating everything to a big matrix\n",
    "feature_names_full = list(all_feat.keys())\n",
    "feature_names = [fname for fname in feature_names_full if fname in feature_types_include]\n",
    "assert(feature_names==feature_types_include) # double check here that the order is correct\n",
    "\n",
    "for ff, feature_name in enumerate(feature_names):   \n",
    "    assert(all_feat[feature_name] is not None)\n",
    "    if ff==0:\n",
    "        all_feat_concat = all_feat[feature_name]\n",
    "    else:               \n",
    "        all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "\n",
    "assert(all_feat_concat.shape[1]==_feature_extractor.n_features_total)\n",
    "print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "print('Feature types included are:')\n",
    "print(feature_names)\n",
    "\n",
    "if torch.any(torch.isnan(all_feat_concat)):\n",
    "    print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "if torch.any(torch.sum(all_feat_concat, axis=0)==0):\n",
    "    print('\\nWARNING THERE ARE ZEROS IN FEATURES MATRIX\\n')\n",
    "    print('zeros for columns:')\n",
    "    print(np.where(torch.sum(all_feat_concat, axis=0)==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff80d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import pyrtools as pt\n",
    "from utils import numpy_utils, torch_utils, texture_utils, prf_utils\n",
    "pyramid_texture_feat_path = default_paths.pyramid_texture_feat_path\n",
    "\n",
    "class texture_feature_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module to compute higher-order texture statistics of input images (e.g. Portilla & Simoncelli 2000, IJCV)\n",
    "    Statistics are computed within a specified region of space (a voxel's pRF)\n",
    "    Can specify different subsets of features to include (i.e. pixel-level stats, simple/complex cells, cross-correlations, auto-correlations)\n",
    "    Inputs to the forward pass are images and pRF parameters of interest [x,y,sigma]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,_fmaps_fn, sample_batch_size=100, feature_types_exclude=None, n_prf_sd_out=2, \\\n",
    "                 aperture=1.0, do_varpart=False, group_all_hl_feats=False, compute_features=True, device=None):\n",
    "        \n",
    "        super(texture_feature_extractor, self).__init__()\n",
    "        \n",
    "        self.fmaps_fn = _fmaps_fn   \n",
    "        self.n_sf = _fmaps_fn.pyr_height\n",
    "        self.n_ori =  _fmaps_fn.n_ori\n",
    "       \n",
    "        self.sample_batch_size = sample_batch_size       \n",
    "        self.n_prf_sd_out = n_prf_sd_out\n",
    "        self.aperture = aperture\n",
    "        self.device = device       \n",
    "        \n",
    "        self.do_varpart = do_varpart\n",
    "        self.group_all_hl_feats = group_all_hl_feats\n",
    "   \n",
    "        self.update_feature_list(feature_types_exclude)\n",
    "        self.do_pca = False\n",
    "        \n",
    "        # if compute features is false, this means the features are already generated, so will be looking for a \n",
    "        # saved h5py file of pre-computed features. If true, will run the extraction step now.\n",
    "        self.compute_features = compute_features\n",
    "        \n",
    "        if not self.compute_features:\n",
    "            self.features_file = os.path.join(pyramid_texture_feat_path, 'S%d_features_each_prf_%dori_%dsf.h5py'%(subject, self.n_ori, self.n_sf))\n",
    "            if not os.path.exists(self.features_file):\n",
    "                raise RuntimeError('Looking at %s for precomputed features, not found.'%self.features_file)                \n",
    "            self.features_each_prf = None\n",
    "        else:\n",
    "            self.fmaps = None\n",
    "    \n",
    "    def init_for_fitting(self, image_size, models, dtype):\n",
    "\n",
    "        \"\"\"\n",
    "        Additional initialization operations.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.max_features = self.n_features_total            \n",
    "        self.clear_big_features()\n",
    "        \n",
    "    def update_feature_list(self, feature_types_exclude):\n",
    "        \n",
    "        # First defining all the possible features and their dimensionality (fixed)\n",
    "        feature_types_all = ['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', \\\n",
    "            'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', \\\n",
    "            'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', \\\n",
    "            'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
    "        self.feature_types_all = feature_types_all\n",
    "        feature_type_dims = [6,16,16,10,1,\\\n",
    "                        272,73,25,\\\n",
    "                        24,24,48,96,\\\n",
    "                       10,20]\n",
    "        self.feature_type_dims_all = feature_type_dims        \n",
    "         \n",
    "        # Decide which features to ignore, or use all features      \n",
    "        if feature_types_exclude is None:\n",
    "            feature_types_exclude = []\n",
    "        # a few shorthands for ignoring sets of features at a time\n",
    "        if 'crosscorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'crosscorrs' in ff])\n",
    "        if 'autocorrs' in feature_types_exclude:\n",
    "            feature_types_exclude.extend( [ff for ff in feature_types_all if 'autocorrs' in ff])\n",
    "        if 'pixel' in feature_types_exclude:\n",
    "            feature_types_exclude.extend(['pixel_stats'])\n",
    "        self.feature_types_exclude = feature_types_exclude\n",
    "        print('Feature types to exclude from the model:')\n",
    "        print(self.feature_types_exclude)    \n",
    "\n",
    "        # Now list all the features that we do want to use\n",
    "        self.feature_types_include  = [ff for ff in feature_types_all if not ff in self.feature_types_exclude]\n",
    "        if len(self.feature_types_include)==0:\n",
    "            raise ValueError('you have specified too many features to exclude, and now you have no features left! aborting.')\n",
    "            \n",
    "        feature_dims_include = [feature_type_dims[fi] for fi in range(len(feature_type_dims)) if not feature_types_all[fi] in self.feature_types_exclude]\n",
    "        # how many features will be needed, in total?\n",
    "        self.n_features_total = np.sum(feature_dims_include)\n",
    "        \n",
    "        # Numbers that define which feature types are in which columns of final output matrix\n",
    "        self.feature_column_labels = np.squeeze(np.concatenate([fi*np.ones([1,feature_dims_include[fi]]) for fi in range(len(feature_dims_include))], axis=1).astype('int'))\n",
    "        assert(np.size(self.feature_column_labels)==self.n_features_total)\n",
    "        \n",
    "        if self.group_all_hl_feats:\n",
    "            # In this case pretend there are just two groups of features - the 'mean_magnitudes' which are first-level gabor-like\n",
    "            # and all other features combined into a second group. Makes it simpler to do variance partition analysis.\n",
    "            # if do_varpart=False, this does nothing.\n",
    "            self.feature_column_labels[self.feature_column_labels != 1] = -1\n",
    "            self.feature_column_labels[self.feature_column_labels==1] = 0\n",
    "            self.feature_column_labels[self.feature_column_labels==-1] = 1\n",
    "            self.feature_group_names = ['mean_magnitudes', 'all_other_texture_feats']\n",
    "        else:\n",
    "            self.feature_group_names = self.feature_types_include\n",
    "            \n",
    "    def get_partial_versions(self):\n",
    "        \n",
    "        if not hasattr(self, 'max_features'):\n",
    "            raise RuntimeError('need to run init_for_fitting first')\n",
    "            \n",
    "        n_feature_types = len(self.feature_group_names)\n",
    "        partial_version_names = ['full_model'] \n",
    "        masks = np.ones([1,self.n_features_total])\n",
    "        \n",
    "        if self.do_varpart and n_feature_types>1:\n",
    "            \n",
    "            # \"Partial versions\" will be listed as: [full model, model w only first set of features, model w only second set, ...             \n",
    "            partial_version_names += ['just_%s'%ff for ff in self.feature_group_names]\n",
    "            masks2 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels==ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "            masks = np.concatenate((masks, masks2), axis=0)\n",
    "            \n",
    "            if n_feature_types > 2:\n",
    "                # if more than two types, also include models where we leave out first set of features, leave out second set of features...]\n",
    "                partial_version_names += ['leave_out_%s'%ff for ff in self.feature_group_names]           \n",
    "                masks3 = np.concatenate([np.expand_dims(np.array(self.feature_column_labels!=ff).astype('int'), axis=0) for ff in np.arange(0,n_feature_types)], axis=0)\n",
    "                masks = np.concatenate((masks, masks3), axis=0)           \n",
    "        \n",
    "        # masks always goes [n partial versions x n total features]\n",
    "        return masks, partial_version_names\n",
    "\n",
    "    \n",
    "    def get_maps(self, images):\n",
    "    \n",
    "        print('Running steerable pyramid feature extraction...')\n",
    "        print('Images array shape is:')\n",
    "        print(images.shape)\n",
    "        t = time.time()\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = torch_utils.get_value(images)\n",
    "        fmaps = self.fmaps_fn(images, to_torch=False, device=self.device)        \n",
    "        self.fmaps = fmaps\n",
    "        elapsed =  time.time() - t\n",
    "        print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "    def load_precomputed_features(self, image_inds):\n",
    "    \n",
    "        print('Loading pre-computed features from %s'%self.features_file)\n",
    "        t = time.time()\n",
    "        with h5py.File(self.features_file, 'r') as data_set:\n",
    "            values = np.copy(data_set['/features'])\n",
    "            data_set.close() \n",
    "        elapsed = time.time() - t\n",
    "        print('Took %.5f seconds to load file'%elapsed)\n",
    "        \n",
    "        self.features_each_prf = values[image_inds,:,:]\n",
    "        \n",
    "        print('Size of features array for this image set is:')\n",
    "        print(self.features_each_prf.shape)\n",
    "        \n",
    "    \n",
    "    def clear_big_features(self):\n",
    "        \n",
    "        if self.compute_features:\n",
    "            print('Clearing steerable pyramid features from memory.')\n",
    "            self.fmaps = None\n",
    "        else:\n",
    "            print('Clearing precomputed features from memory.')\n",
    "            self.features_each_prf = None\n",
    "        \n",
    "        \n",
    "    def forward(self, images, prf_params, prf_model_index, fitting_mode=True):\n",
    "        \n",
    "        if not self.compute_features:\n",
    "            \n",
    "            # Load from file the features for this set of images\n",
    "            # In this case, the item passed in through \"images\" must actually be the indices of the images to use, not images themselves.\n",
    "            # Check to make sure this is the case.\n",
    "            assert(len(images.shape)==1)\n",
    "            image_inds = images\n",
    "            if self.features_each_prf is None:\n",
    "                self.load_precomputed_features(image_inds)\n",
    "            else:\n",
    "                assert(self.features_each_prf.shape[0]==len(image_inds))\n",
    "            \n",
    "             # Taking the features for the desired prf model\n",
    "            features = self.features_each_prf[:,:,prf_model_index]\n",
    "            features = torch_utils._to_torch(features, self.device)\n",
    "            \n",
    "            # Choosing which of these columns to include in model (might be all)\n",
    "            feature_column_labels_all = np.squeeze(np.concatenate([fi*np.ones([1,self.feature_type_dims_all[fi]]) for fi in range(len(self.feature_type_dims_all))], axis=1).astype('int'))\n",
    "            all_feat = OrderedDict()\n",
    "            for fi, ff in enumerate(self.feature_types_all):\n",
    "                if ff in self.feature_types_include:\n",
    "                    all_feat[ff] = features[:,feature_column_labels_all==fi]\n",
    "                else:\n",
    "                    all_feat[ff] = None\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.fmaps is None:\n",
    "                self.get_maps(images)\n",
    "            else:\n",
    "                assert(images.shape[0]==self.fmaps[0][0].shape[0])\n",
    "\n",
    "            if isinstance(prf_params, torch.Tensor):\n",
    "                prf_params = torch_utils.get_value(prf_params)\n",
    "            assert(np.size(prf_params)==3)\n",
    "            prf_params = np.squeeze(prf_params)\n",
    "            if isinstance(images, torch.Tensor):\n",
    "                images = torch_utils.get_value(images)\n",
    "\n",
    "            print('Computing higher order correlations...')\n",
    "\n",
    "            t = time.time()\n",
    "            pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "                magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "                magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "                real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs =  \\\n",
    "                        get_higher_order_features(self.fmaps, images, prf_params, sample_batch_size=self.sample_batch_size, n_prf_sd_out=self.n_prf_sd_out, aperture=self.aperture, device=self.device)\n",
    "\n",
    "\n",
    "            elapsed =  time.time() - t\n",
    "            print('time elapsed = %.5f'%elapsed)\n",
    "\n",
    "            all_feat = OrderedDict({'pixel_stats':pixel_stats, 'mean_magnitudes':mean_magnitudes, 'mean_realparts':mean_realparts, \\\n",
    "                                    'marginal_stats_lowpass_recons':marginal_stats_lowpass_recons, 'variance_highpass_resid':variance_highpass_resid, \\\n",
    "                'magnitude_feature_autocorrs':magnitude_feature_autocorrs, 'lowpass_recon_autocorrs':lowpass_recon_autocorrs, 'highpass_resid_autocorrs':highpass_resid_autocorrs, \\\n",
    "                'magnitude_within_scale_crosscorrs':magnitude_within_scale_crosscorrs, 'real_within_scale_crosscorrs':real_within_scale_crosscorrs, \\\n",
    "                'magnitude_across_scale_crosscorrs':magnitude_across_scale_crosscorrs, 'real_imag_across_scale_crosscorrs':real_imag_across_scale_crosscorrs, \\\n",
    "                'real_spatshift_within_scale_crosscorrs':real_spatshift_within_scale_crosscorrs, 'real_spatshift_across_scale_crosscorrs':real_spatshift_across_scale_crosscorrs})\n",
    "\n",
    "        # Now concatenating everything to a big matrix\n",
    "        feature_names_full = list(all_feat.keys())\n",
    "        feature_names = [fname for fname in feature_names_full if fname in self.feature_types_include]\n",
    "        assert(feature_names==self.feature_types_include) # double check here that the order is correct\n",
    "\n",
    "        for ff, feature_name in enumerate(feature_names):   \n",
    "            assert(all_feat[feature_name] is not None)\n",
    "            if ff==0:\n",
    "                all_feat_concat = all_feat[feature_name]\n",
    "            else:               \n",
    "                all_feat_concat = torch.cat((all_feat_concat, all_feat[feature_name]), axis=1)\n",
    "\n",
    "        assert(all_feat_concat.shape[1]==self.n_features_total)\n",
    "        print('Final size of features concatenated is [%d x %d]'%(all_feat_concat.shape[0], all_feat_concat.shape[1]))\n",
    "        print('Feature types included are:')\n",
    "        print(feature_names)\n",
    "\n",
    "        if torch.any(torch.isnan(all_feat_concat)):\n",
    "            print('\\nWARNING THERE ARE NANS IN FEATURES MATRIX\\n')\n",
    "        if torch.any(torch.sum(all_feat_concat, axis=0)==0):\n",
    "            print('\\nWARNING THERE ARE ZEROS IN FEATURES MATRIX\\n')\n",
    "            print('zeros for columns:')\n",
    "            print(np.where(torch.sum(all_feat_concat, axis=0)==0))\n",
    "\n",
    "        feature_inds_defined = np.ones((self.n_features_total,), dtype=bool)\n",
    "        \n",
    "        return all_feat_concat, feature_inds_defined\n",
    "    \n",
    "\n",
    "class steerable_pyramid_extractor(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Module that utilizes steerable pyramid ( https://pyrtools.readthedocs.io/en/latest/) to extract features.\n",
    "    For a batch of input images, will return all the pyramid coefficients, as well as additional types of feature maps\n",
    "    (i.e. partially reconstructed lowpass images at several frequency levels, upsampled feature maps).\n",
    "    These are used by 'get_higher_order_features' to extract various textural features of the image.\n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pyr_height=4, n_ori=8):\n",
    "        \n",
    "        super(steerable_pyramid_extractor, self).__init__()       \n",
    "        self.n_ori = n_ori\n",
    "        self.pyr_height = pyr_height # how many spatial frequencies?\n",
    "        self.pyr = None\n",
    "        \n",
    "    def forward(self, image_batch, to_torch=False, device=None):\n",
    "        \n",
    "        batch_size = image_batch.shape[0]\n",
    "        t  = time.time()\n",
    "        for ii in range(batch_size):\n",
    "            \n",
    "            # Call the pyramid generation code here, get all features for this image.\n",
    "            pyr = pt.pyramids.SteerablePyramidFreq(image_batch[ii,0,:,:], is_complex=True, height = self.pyr_height, order = self.n_ori-1)\n",
    "            self.pyr = pyr # storing the most recently generated pyramid, in case we need its properties later\n",
    "            \n",
    "            if ii==0:  \n",
    "                \n",
    "                # Initialize all the feature maps we want to store...\n",
    "                fmaps_complex = []\n",
    "                fmaps_coarser_upsampled = []\n",
    "               \n",
    "                # Will generate several low-pass filtered representations of the image - will use these as additional feature maps for \n",
    "                # computing autocorrelations and marginal statistics.\n",
    "                fmaps_lowpass_recon = []\n",
    "                fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "               \n",
    "                # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "                sf_reverse  = self.pyr_height  # need to go backward because pyr comes out in the opposite order from what i want\n",
    "                for sf in range(self.pyr_height):\n",
    "                    sf_reverse -= 1\n",
    "                    fmaps_complex.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))\n",
    "                    \n",
    "                    # Initialize \"parent\" representations for this level (map from the next coarsest resolution, upsampled to the finer scale)\n",
    "                    # To be used for cross-scale comparisons.\n",
    "                    if sf==0:\n",
    "                        # this will be the lowpass residual (non-oriented).\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, 1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))                     \n",
    "                    else:\n",
    "                        fmaps_coarser_upsampled.append(np.zeros((batch_size, self.n_ori,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                   pyr.pyr_coeffs[sf_reverse,0].shape[1]), dtype=complex))     \n",
    "                        \n",
    "                    fmaps_lowpass_recon.append(np.zeros((batch_size,1,pyr.pyr_coeffs[sf_reverse,0].shape[0],\\\n",
    "                                                         pyr.pyr_coeffs[sf_reverse,0].shape[1])))\n",
    "\n",
    "                fmaps_resid = []    \n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_lowpass'].shape[0],pyr.pyr_coeffs['residual_lowpass'].shape[1])))\n",
    "                fmaps_resid.append(np.zeros((batch_size,1,pyr.pyr_coeffs['residual_highpass'].shape[0],pyr.pyr_coeffs['residual_highpass'].shape[1])))\n",
    "\n",
    "            # First get lowpass filtered representation of the image\n",
    "            lowpass_recon = pyr.recon_pyr(levels='residual_lowpass', bands='all', twidth=1) \n",
    "            scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size['residual_lowpass'][0]\n",
    "            lowpass_recon = texture_utils.shrink(lowpass_recon, scale_by)*scale_by**2\n",
    "            fmaps_lowpass_recon[0][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "            # Get the \"parent\" for lowest SF level (upsample the residual lowpass)\n",
    "#             print(pyr.pyr_coeffs['residual_lowpass'].shape)\n",
    "            upsampled = texture_utils.expand(pyr.pyr_coeffs['residual_lowpass'], factor=2)/2**2\n",
    "#             print(upsampled.shape)\n",
    "            fmaps_coarser_upsampled[0][ii,0,:,:] = upsampled\n",
    "            \n",
    "            # Feature maps will be listed from low SF to high SF. Each map stack is size [batch_size x orientations x height x width]\n",
    "            sf_reverse  = self.pyr_height # need to go backward because pyr comes out in the opposite order from what i want\n",
    "            for sf in range(self.pyr_height):\n",
    "                sf_reverse -= 1\n",
    "                for oo in range(self.n_ori):     \n",
    "\n",
    "                    # These are the main feature maps of the pyramid - one feature map per scale per orientation band.\n",
    "                    # Complex number, can take the magnitude or real/imaginary part to simulate complex or simple cell-type responses.\n",
    "                    fmaps_complex[sf][ii,oo,:,:] = pyr.pyr_coeffs[(sf_reverse,oo)]\n",
    "                    \n",
    "                    if sf<self.pyr_height-1:\n",
    "                        # Store this as a \"parent\" representation, will be used for the next most fine SF level (i.e. sf+1)\n",
    "                        upsampled = texture_utils.expand(pyr.pyr_coeffs[(sf_reverse,oo)], factor=2)/2**2\n",
    "                        # Double the phase (angle of the complex number); note this doesn't affect the magnitude. \n",
    "                        phase_doubled = texture_utils.double_phase(upsampled)\n",
    "                        fmaps_coarser_upsampled[sf+1][ii,oo,:,:] = phase_doubled\n",
    "                   \n",
    "                        \n",
    "                # Get the bandpass filtered representation for this scale\n",
    "                bandpass_image = np.real(pyr.recon_pyr(levels=sf_reverse, bands='all', twidth=1))\n",
    "                scale_by = pyr.pyr_size[(0,0)][0]/pyr.pyr_size[(sf_reverse,0)][0]\n",
    "                bandpass_image = texture_utils.shrink(bandpass_image, factor=scale_by)*scale_by**2\n",
    "                \n",
    "                # Add it onto the lowpass_recon (gets modified every loop iteration)\n",
    "                lowpass_recon = texture_utils.expand(lowpass_recon, factor=2)/2**2\n",
    "                lowpass_recon = lowpass_recon + bandpass_image\n",
    "                fmaps_lowpass_recon[sf+1][ii,0,:,:] = lowpass_recon\n",
    "            \n",
    "\n",
    "            # Grab residual feature maps, the lowest and highest levels of the pyramid\n",
    "            fmaps_resid[0][ii,0,:,:] = pyr.pyr_coeffs['residual_lowpass']\n",
    "            fmaps_resid[1][ii,0,:,:] = pyr.pyr_coeffs['residual_highpass']\n",
    "            \n",
    "            \n",
    "        elapsed = time.time() - t\n",
    "#         print('time elapsed: %.5f s'%elapsed)\n",
    "\n",
    "        if to_torch:            \n",
    "            fmaps_complex = [torch.from_numpy(fm).to(device) for fm in fmaps_complex]            \n",
    "            fmaps_resid = [torch_utils._to_torch(fm, device=device) for fm in fmaps_resid]\n",
    "            fmaps_lowpass_recon = [torch_utils._to_torch(fm, device=device) for fm in fmaps_lowpass_recon]                      \n",
    "            fmaps_coarser_upsampled = [torch.from_numpy(fm).to(device) for fm in fmaps_coarser_upsampled]\n",
    "\n",
    "        return fmaps_complex, fmaps_resid, fmaps_lowpass_recon, fmaps_coarser_upsampled\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "def get_higher_order_features(fmaps, images, prf_params, sample_batch_size=20, n_prf_sd_out=2, aperture=1.0, device=None, keep_orig_shape=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute higher order texture features for a batch of images.\n",
    "    Input the module that defines steerable pyramid (i.e. 'steerable_pyramid_extractor'), and desired prf parameters.\n",
    "    Returns arrays of each higher order feature.  \n",
    "    Adapted by MH from code in the library at:\n",
    "    https://github.com/freeman-lab/metamers\n",
    "    \"\"\"\n",
    "\n",
    "    fmaps_complex_all, fmaps_resid_all, fmaps_lowpass_recon_all, fmaps_coarser_upsampled_all = fmaps\n",
    "   \n",
    "    n_trials = fmaps_complex_all[0].shape[0]\n",
    "    x,y,sigma = prf_params\n",
    "\n",
    "    n_sf = len(fmaps_complex_all)\n",
    "    n_ori = fmaps_complex_all[0].shape[1]\n",
    "        \n",
    "    # all pairs of different orientation channels.\n",
    "    ori_pairs = np.vstack([[[oo1, oo2] for oo2 in np.arange(oo1+1, n_ori)] for oo1 in range(n_ori) if oo1<n_ori-1])\n",
    "    n_ori_pairs = np.shape(ori_pairs)[0]\n",
    "\n",
    "    # mean, variance, skew, kurtosis, min, max\n",
    "    pixel_stats = torch.zeros((n_trials,6), device=device)\n",
    "\n",
    "    # Mean magnitude each scale/orientation, within the prf.\n",
    "    mean_magnitudes = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "    mean_realparts = torch.zeros((n_trials, n_sf, n_ori), device=device)\n",
    "\n",
    "    # Store the skew and kurtosis of the lowpass reconstructions at each scale\n",
    "    marginal_stats_lowpass_recons = torch.zeros((n_trials, n_sf+1, 2), device=device)\n",
    "\n",
    "    # Variance of the highpass residual\n",
    "    variance_highpass_resid = torch.zeros((n_trials, 1), device=device)\n",
    "\n",
    "    # how many unique autocorrelation values will we get out for each feature map? These will be pre-defined, same for every pRF.\n",
    "    # but different for different scales of feature maps.\n",
    "    # note also that for bigger prfs, there will potentially be more pixels that contribute to the autocorrelation computation - \n",
    "    # but a fixed portion of the matrix is returned.\n",
    "    autocorr_output_pix=np.array([3,3,5,7,7])\n",
    "    n_autocorr_vals = ((autocorr_output_pix**2+1)/2).astype('int')\n",
    "    max_autocorr_vals = np.max(n_autocorr_vals)\n",
    "    \n",
    "    # Spatial autocorrelation of the magnitude of spectral coefficients, within each scale and orientation.\n",
    "    magnitude_feature_autocorrs = torch.zeros([n_trials, n_sf, n_ori, max_autocorr_vals], device=device) # this is ace in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the partially-reconstructed lowpass image representation at each scale\n",
    "    lowpass_recon_autocorrs = torch.zeros([n_trials, n_sf+1, max_autocorr_vals], device=device) # this is acr in the matlab code\n",
    "\n",
    "    # Spatial autocorrelation of the highpass residual\n",
    "    highpass_resid_autocorrs = torch.zeros([n_trials, 1, max_autocorr_vals], device=device)\n",
    "\n",
    "    # Within scale correlations of feature maps: compare feature map magnitudes for different orientations.\n",
    "    magnitude_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is C0 in the matlab code\n",
    "    # Using the real parts.\n",
    "    real_within_scale_crosscorrs = torch.zeros([n_trials, n_sf, n_ori_pairs], device=device) # this is Cr0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations of feature maps: always comparing each scale to an up-sampled version of the scale coarser than it.\n",
    "    magnitude_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, n_ori, n_ori], device=device) # this is Cx0 in the matlab code\n",
    "\n",
    "    # Cross-scale correlations, using the real and imaginary parts separately. The phase (angle) of the coarser map is doubled before computing these.\n",
    "    real_imag_across_scale_crosscorrs = torch.zeros([n_trials, n_sf-1, 2, n_ori, n_ori], device=device) # this is Crx0 in the matlab code\n",
    "\n",
    "    # These are comparisons with spatially shifted versions of the lowpass residual. Not sure we need this...\n",
    "    n_spatshifts = 5;\n",
    "    real_spatshift_within_scale_crosscorrs = torch.zeros([n_trials, 1, n_spatshifts, n_spatshifts], device=device)# this is Cr0 in the matlab code\n",
    "    real_spatshift_across_scale_crosscorrs = torch.zeros([n_trials, 1, n_ori, n_spatshifts], device=device)  # this is Crx0 in the matlab code\n",
    "\n",
    "    # Looping over batches of trials to compute everything of interest.\n",
    "    bb=-1\n",
    "    for batch_inds, batch_size_actual in numpy_utils.iterate_range(0, n_trials, sample_batch_size):\n",
    "        bb=bb+1\n",
    "\n",
    "        fmaps_complex = [torch.from_numpy(fmaps_complex_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_complex_all))]\n",
    "        fmaps_resid = [torch.from_numpy(fmaps_resid_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_resid_all))]\n",
    "        fmaps_lowpass_recon = [torch.from_numpy(fmaps_lowpass_recon_all[ii][batch_inds,:,:,:]).float().to(device) for ii in range(len(fmaps_lowpass_recon_all))]\n",
    "        fmaps_coarser_upsampled = [torch.from_numpy(fmaps_coarser_upsampled_all[ii][batch_inds,:,:,:]).to(device) for ii in range(len(fmaps_coarser_upsampled_all))]\n",
    "\n",
    "        if bb==0:\n",
    "            npix_each_scale = [fmaps_complex_all[sc].shape[2] for sc in np.arange(n_sf-1,-1,-1)]\n",
    "            npix_each_scale.append(fmaps_resid_all[0].shape[2])\n",
    "            npix_each_scale.reverse()\n",
    "\n",
    "        # First working with the finest scale (original image)\n",
    "        n_pix = npix_each_scale[-1]      \n",
    "        g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[-1])\n",
    "\n",
    "        # Gather pixel-wise statistics here \n",
    "        wmean, wvar, wskew, wkurt = texture_utils.get_weighted_pixel_features(images[batch_inds], spatial_weights, device=device)\n",
    "        pixel_stats[batch_inds,0] = torch.squeeze(wmean)\n",
    "        pixel_stats[batch_inds,1] = torch.squeeze(wvar)\n",
    "        pixel_stats[batch_inds,2] = torch.squeeze(wskew)\n",
    "        pixel_stats[batch_inds,3] = torch.squeeze(wkurt)\n",
    "        pixel_stats[batch_inds,4] = torch_utils._to_torch(np.squeeze(np.min(np.min(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "        pixel_stats[batch_inds,5] = torch_utils._to_torch(np.squeeze(np.max(np.max(images[batch_inds], axis=3), axis=2)), device=device)\n",
    "\n",
    "        # Autocorrs of the highpass residual\n",
    "        highpass_resid = fmaps_resid[1]\n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(highpass_resid, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[-1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        highpass_resid_autocorrs[batch_inds,0,0:n_autocorr_vals[-1]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[-1]])\n",
    "\n",
    "        # Variance of the highpass residual\n",
    "        m, wvar, s, k = texture_utils.get_weighted_pixel_features(highpass_resid, spatial_weights, device=device)\n",
    "        variance_highpass_resid[batch_inds,0] = torch.squeeze(wvar)\n",
    "\n",
    "        # Next work with the low-pass reconstruction (most coarse scale, smallest npix)\n",
    "        n_pix = npix_each_scale[0]       \n",
    "        g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "        spatial_weights = g[2][0]\n",
    "        patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[0])\n",
    "\n",
    "        lowpass_rec = fmaps_lowpass_recon[0]\n",
    "\n",
    "        # Marginal stats of low-pass reconstruction\n",
    "        m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_rec, spatial_weights, device=device)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,0] = torch.squeeze(wskew)\n",
    "        marginal_stats_lowpass_recons[batch_inds,0,1] = torch.squeeze(wkurt)\n",
    "\n",
    "        # Autocorrs of low-pass reconstruction \n",
    "        auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_rec, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[0], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "        lowpass_recon_autocorrs[batch_inds,0,0:n_autocorr_vals[0]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[0]])\n",
    "\n",
    "        # Looping over spatial frequency/scale\n",
    "        # Loop goes low SF (smallest npix) to higher SF (largest npix)\n",
    "        for ff in range(n_sf):\n",
    "         \n",
    "            # Scale specific things - get the prf at this resolution of interest    \n",
    "            n_pix = npix_each_scale[ff+1]           \n",
    "            g = prf_utils.make_gaussian_mass_stack([x], [y], [sigma], n_pix=n_pix, size=aperture, dtype=np.float32)\n",
    "            spatial_weights = g[2][0]\n",
    "            patch_bbox_square = texture_utils.get_bbox_from_prf(prf_params, spatial_weights.shape, n_prf_sd_out, force_square=True, min_pix=autocorr_output_pix[1+ff])\n",
    "\n",
    "            # Get the low-pass reconstruction at this scale\n",
    "            lowpass_summed = fmaps_lowpass_recon[ff+1]  # this is summed over this scale band and those below it\n",
    "            m, v, wskew, wkurt = texture_utils.get_weighted_pixel_features(lowpass_summed, spatial_weights, device=device)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,0] = torch.squeeze(wskew)\n",
    "            marginal_stats_lowpass_recons[batch_inds,ff+1,1] = torch.squeeze(wkurt)\n",
    "\n",
    "            # Autocorrelations of low-pass reconstruction (at this scale)\n",
    "            auto_corr = texture_utils.weighted_auto_corr_2d(lowpass_summed, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "            lowpass_recon_autocorrs[batch_inds,ff+1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "            # Loop over orientation channels\n",
    "            xx=-1\n",
    "            for oo1 in range(n_ori):       \n",
    "\n",
    "                # Magnitude of the complex coefficients; complex cell-like responses\n",
    "                mag1 = torch.abs(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                # The mean magnitudes here are basically second-order spectral statistics, within the specified spatial region defined by weights\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(mag1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_magnitudes[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "                \n",
    "                mag1 = mag1 - torch.tile(torch.mean(torch.mean(mag1, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                # Real parts of the complex coefficients; simple cell-like responses\n",
    "                real1 = torch.real(fmaps_complex[ff][:,oo1,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()    \n",
    "                \n",
    "                # Average of the real parts within the specified spatial region\n",
    "                wmean, v, s, k = texture_utils.get_weighted_pixel_features(real1, spatial_weights/np.sum(spatial_weights), device=device)\n",
    "                mean_realparts[batch_inds, ff, oo1] = torch.squeeze(wmean)\n",
    "\n",
    "                # Complex cell autocorrelation (correlation w spatially shifted versions of itself)     \n",
    "                auto_corr = texture_utils.weighted_auto_corr_2d(mag1, spatial_weights, patch_bbox=patch_bbox_square, output_pix = autocorr_output_pix[ff+1], subtract_patch_mean = True, enforce_size=True, device=device)       \n",
    "                magnitude_feature_autocorrs[batch_inds,ff,oo1,0:n_autocorr_vals[1+ff]] = torch.reshape(texture_utils.unique_autocorrs(auto_corr), [batch_size_actual, n_autocorr_vals[1+ff]])\n",
    "\n",
    "                # Within-scale correlations - comparing resp at orient==oo1 to responses at all other orientations, same scale.\n",
    "                for oo2 in np.arange(oo1+1, n_ori):            \n",
    "                    xx = xx+1 \n",
    "                    assert(oo1==ori_pairs[xx,0] and oo2==ori_pairs[xx,1])\n",
    "\n",
    "                    # Magnitude at the other orientation (oo2)\n",
    "                    mag2 = torch.abs(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()      \n",
    "                    mag2 = mag2 - torch.tile(torch.mean(torch.mean(mag2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                    # Correlate the magnitude feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    magnitude_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                    # Real part at the other orientation (oo2)\n",
    "                    real2 = torch.real(fmaps_complex[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()                     \n",
    "\n",
    "                    # Correlate the real feature maps for the two orientations, within scale\n",
    "                    cross_corr = texture_utils.weighted_cross_corr_2d(real1, real2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)\n",
    "                    real_within_scale_crosscorrs[batch_inds,ff,xx] = torch.squeeze(cross_corr);\n",
    "\n",
    "                # Cross-scale correlations - for these we care about same ori to same ori, so looping over all orientations.\n",
    "                # Going to compare coefficients at the current scale to those at a coarser scale (ff-1)\n",
    "                # If we're at first scale (ff=0), then will use a different method.\n",
    "                if ff>0:\n",
    "\n",
    "                    for oo2 in range(n_ori):\n",
    "\n",
    "                        # Get magnitude of coefficients for neighboring (coarser) scale                        \n",
    "                        mag_coarser2 = torch.abs(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        mag_coarser2 = mag_coarser2 - torch.tile(torch.mean(torch.mean(mag_coarser2, axis=3, keepdim=True), axis=2, keepdim=True), [1,1,n_pix, n_pix])\n",
    "\n",
    "                        # Correlate this with the finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(mag1, mag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix)            \n",
    "                        magnitude_across_scale_crosscorrs[batch_inds,ff-1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        # Get the real and imaginary parts at coarser scale\n",
    "                        real_coarser2 = torch.real(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "                        imag_coarser2 = torch.imag(fmaps_coarser_upsampled[ff][:,oo2,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                        # Correlate each of these with real part at finer scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,0,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, imag_coarser2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_imag_across_scale_crosscorrs[batch_inds,ff-1,1,oo1,oo2] = torch.squeeze(cross_corr)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # instead of different orientations for the \"parent\" level here, have spatially shifted versions.\n",
    "                    real_coarser = torch.real(fmaps_coarser_upsampled[ff][:,0,:,:].view([batch_size_actual,1,n_pix,n_pix])).float()\n",
    "\n",
    "                    shifts = [[0,0],[1,3],[-1,3],[1,2],[-1,2]]\n",
    "                    for si1, shift1 in enumerate(shifts):\n",
    "\n",
    "                        ss,dd = shift1\n",
    "                        real_coarser_shifted1 = torch.roll(real_coarser, shifts=ss, dims=dd)               \n",
    "                        # Real part at the finer scale compared to spatially shifted at the coarser scale\n",
    "                        cross_corr = texture_utils.weighted_cross_corr_2d(real1, real_coarser_shifted1, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                        real_spatshift_across_scale_crosscorrs[batch_inds,ff,oo1,si1] = torch.squeeze(cross_corr)\n",
    "\n",
    "                        for si2 in np.arange(si1+1, n_spatshifts):\n",
    "\n",
    "                            ss,dd = shifts[si2]\n",
    "                            real_coarser_shifted2 = torch.roll(real_coarser, shifts=ss, dims=dd) \n",
    "                            # Real parts at same scale, comparing spatially shifted.\n",
    "                            cross_corr = texture_utils.weighted_cross_corr_2d(real_coarser_shifted1, real_coarser_shifted2, spatial_weights, patch_bbox=None, subtract_patch_mean = True, device=device)/(n_pix*n_pix) \n",
    "                            real_spatshift_within_scale_crosscorrs[batch_inds,ff,si1,si2] = torch.squeeze(cross_corr)\n",
    "\n",
    "            \n",
    "    if not keep_orig_shape:\n",
    "        # Reshape everything to [ntrials x nfeatures]\n",
    "\n",
    "        mean_magnitudes = torch.reshape(mean_magnitudes, [n_trials, -1])\n",
    "        mean_realparts = torch.reshape(mean_realparts, [n_trials, -1])\n",
    "        marginal_stats_lowpass_recons = torch.reshape(marginal_stats_lowpass_recons, [n_trials, -1])\n",
    "        variance_highpass_resid =torch.reshape(variance_highpass_resid, [n_trials, -1])\n",
    "\n",
    "        magnitude_feature_autocorrs = torch.reshape(magnitude_feature_autocorrs, [n_trials, -1])\n",
    "        # take out the zero columns, which happen because of different size autocorr outputs.\n",
    "        magnitude_feature_autocorrs = magnitude_feature_autocorrs[:,torch.sum(magnitude_feature_autocorrs, axis=0)!=0]\n",
    "        assert(magnitude_feature_autocorrs.shape[1]==np.sum(n_autocorr_vals[1:]*n_ori))\n",
    "\n",
    "        lowpass_recon_autocorrs = torch.reshape(lowpass_recon_autocorrs, [n_trials, -1])\n",
    "        lowpass_recon_autocorrs = lowpass_recon_autocorrs[:,torch.sum(lowpass_recon_autocorrs, axis=0)!=0]\n",
    "        assert(lowpass_recon_autocorrs.shape[1]==np.sum(n_autocorr_vals))\n",
    "\n",
    "        highpass_resid_autocorrs = torch.reshape(highpass_resid_autocorrs, [n_trials, -1])\n",
    "\n",
    "        magnitude_within_scale_crosscorrs = torch.reshape(magnitude_within_scale_crosscorrs, [n_trials, -1])\n",
    "        real_within_scale_crosscorrs = torch.reshape(real_within_scale_crosscorrs, [n_trials, -1])\n",
    "        magnitude_across_scale_crosscorrs = torch.reshape(magnitude_across_scale_crosscorrs, [n_trials, -1])\n",
    "        real_imag_across_scale_crosscorrs = torch.reshape(real_imag_across_scale_crosscorrs, [n_trials, -1])\n",
    "        real_spatshift_within_scale_crosscorrs = torch.reshape(real_spatshift_within_scale_crosscorrs, [n_trials, -1])\n",
    "\n",
    "        real_spatshift_within_scale_crosscorrs = real_spatshift_within_scale_crosscorrs[:,torch.sum(real_spatshift_within_scale_crosscorrs, axis=0)!=0]\n",
    "        assert(real_spatshift_within_scale_crosscorrs.shape[1]==np.sum(np.arange(1,n_spatshifts)))\n",
    "\n",
    "        real_spatshift_across_scale_crosscorrs = torch.reshape(real_spatshift_across_scale_crosscorrs, [n_trials, -1])\n",
    "\n",
    "        \n",
    "    return pixel_stats, mean_magnitudes, mean_realparts, marginal_stats_lowpass_recons, variance_highpass_resid, \\\n",
    "            magnitude_feature_autocorrs, lowpass_recon_autocorrs, highpass_resid_autocorrs, \\\n",
    "            magnitude_within_scale_crosscorrs, real_within_scale_crosscorrs, magnitude_across_scale_crosscorrs, real_imag_across_scale_crosscorrs, \\\n",
    "            real_spatshift_within_scale_crosscorrs, real_spatshift_across_scale_crosscorrs\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
