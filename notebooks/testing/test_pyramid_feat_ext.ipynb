{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c4cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: GeForce GTX TITAN X\n",
      "\n",
      "torch: 1.8.1+cu111\n",
      "cuda:  11.1\n",
      "cudnn: 8005\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import torch\n",
    "import time\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "from sklearn import decomposition\n",
    "\n",
    "#import custom modules\n",
    "code_dir = '/user_data/mmhender/imStat/code/'\n",
    "sys.path.append(code_dir)\n",
    "from utils import prf_utils, torch_utils, texture_utils, default_paths\n",
    "from model_fitting import initialize_fitting\n",
    "from feature_extraction import texture_statistics_pyramid2 as texture_statistics_pyramid\n",
    "\n",
    "device = initialize_fitting.init_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6523fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nsd_utils\n",
    "batch_size=2\n",
    "subject=1\n",
    "n_sf = 4; \n",
    "n_ori = 4;\n",
    "debug=True\n",
    "use_node_storage = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1020d346",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyramid_texture_feat_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ddcf93912ed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfn2save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyramid_texture_feat_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'S%d_features_each_prf_%dori_%dsf.h5py'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ori\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyramid_texture_feat_path' is not defined"
     ]
    }
   ],
   "source": [
    "fn2save = os.path.join(pyramid_texture_feat_path, 'S%d_features_each_prf_%dori_%dsf.h5py'%(subject, n_ori, n_sf))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c861de",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inds = np.arange(0,10)\n",
    "print('Loading pre-computed features from %s'%fn2save)\n",
    "t = time.time()\n",
    "with h5py.File(fn2save, 'r') as data_set:\n",
    "    values = np.copy(data_set['/features'][image_inds,:,:])\n",
    "    data_set.close() \n",
    "elapsed = time.time() - t\n",
    "print('Took %.5f seconds to load file'%elapsed)\n",
    "\n",
    "# self.features_each_prf = values[image_inds,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46c5db42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.54796582e-01,  4.46013734e-03, -2.52191448e+00,  1.96903534e+01,\n",
       "        3.85581143e-03,  9.86981571e-01,  1.44901156e+00,  3.12045217e+00,\n",
       "        5.94274855e+00,  6.62372065e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[1,0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e4968cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.54796582e-01,  4.46013734e-03, -2.52191448e+00,  1.96903534e+01,\n",
       "        3.85581143e-03,  9.86981571e-01,  1.44901156e+00,  3.12045217e+00,\n",
       "        5.94274855e+00,  6.62372065e+00])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_each_prf[1,0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee95386",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_node_storage:\n",
    "    pyramid_texture_feat_path = default_paths.pyramid_texture_feat_path_localnode\n",
    "else:\n",
    "    pyramid_texture_feat_path = default_paths.pyramid_texture_feat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0378707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading images for subject 1\n",
      "\n",
      "image data size: (10000, 3, 227, 227) , dtype: uint8 , value range: 0 255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and prepare the image set to work with (all images for the current subject, 10,000 ims)\n",
    "stim_root = default_paths.stim_root\n",
    "image_data = nsd_utils.get_image_data(subject)  \n",
    "image_data = nsd_utils.image_uncolorize_fn(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb0315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most extreme RF positions:\n",
      "[-0.55 -0.55  0.04]\n",
      "[0.55       0.55       0.40000001]\n"
     ]
    }
   ],
   "source": [
    "# Need to have size a multiple of 8, for the pyramid to work right\n",
    "process_at_size=240\n",
    "resample_fn = torch.nn.Upsample((process_at_size, process_at_size), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "# Params for the spatial aspect of the model (possible pRFs)\n",
    "aperture = 1.0\n",
    "aperture_rf_range = 1.1\n",
    "aperture, models = initialize_fitting.get_prf_models(aperture_rf_range=aperture_rf_range)    \n",
    "\n",
    "# Set up the pyramid\n",
    "batch_size=2\n",
    "feature_types_exclude = []\n",
    "n_prf_sd_out = 2\n",
    "do_varpart=False # this doesn't do anything here\n",
    "group_all_hl_feats = False # this doesn't do anything here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2049c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,  22,  38,  48,  49, 321, 394, 419, 443, 467, 515, 611, 621,\n",
       "       641])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(_feature_extractor.feature_type_dims_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34b4282e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.feature_column_labels[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817cbd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types to exclude from the model:\n",
      "[]\n",
      "Extracting features for images [0 - 1]\n",
      "Clearing steerable pyramid features from memory.\n",
      "Getting features for pRF [x,y,sigma]:\n",
      "[-0.31842105263157894, 0.02894736842105261, 0.03999999910593033]\n",
      "Running steerable pyramid feature extraction...\n",
      "Images array shape is:\n",
      "torch.Size([2, 1, 240, 240])\n",
      "time elapsed = 0.39375\n",
      "Computing higher order correlations...\n",
      "time elapsed = 0.23951\n",
      "Final size of features concatenated is [2 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n",
      "model 204, min/max of features in batch: [tensor(-281.7485, device='cuda:0'), tensor(1216.4402, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "compute_features = True\n",
    "_fmaps_fn = texture_statistics_pyramid.steerable_pyramid_extractor(pyr_height = n_sf, n_ori = n_ori)\n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "_feature_extractor = texture_statistics_pyramid.texture_feature_extractor(_fmaps_fn,sample_batch_size=batch_size, feature_types_exclude=feature_types_exclude, \\\n",
    "                                               n_prf_sd_out=n_prf_sd_out, aperture=aperture, do_varpart = do_varpart, \\\n",
    "                              compute_features=compute_features, group_all_hl_feats = group_all_hl_feats, device=device)\n",
    "\n",
    "n_features = _feature_extractor.n_features_total\n",
    "n_images = image_data.shape[0]\n",
    "n_prfs = models.shape[0]\n",
    "n_batches = int(np.ceil(n_images/batch_size))\n",
    "\n",
    "features_each_prf = np.zeros((n_images, n_features, n_prfs))\n",
    "\n",
    "bb=0\n",
    "\n",
    "# for bb in range(n_batches):\n",
    "\n",
    "#     if debug and bb>1:\n",
    "#         continue\n",
    "\n",
    "batch_inds = np.arange(batch_size * bb, np.min([batch_size * (bb+1), n_images]))\n",
    "\n",
    "print('Extracting features for images [%d - %d]'%(batch_inds[0], batch_inds[-1]))\n",
    "\n",
    "image_batch = torch_utils._to_torch(image_data[batch_inds,:,:,:], device)\n",
    "\n",
    "image_batch = resample_fn(image_batch)\n",
    "\n",
    "_feature_extractor.clear_big_features()\n",
    "\n",
    "mm=204\n",
    "\n",
    "#     for mm in range(n_prfs):\n",
    "\n",
    "#         if debug and mm>1:\n",
    "#             continue\n",
    "\n",
    "x,y,sigma = models[mm,:]\n",
    "print('Getting features for pRF [x,y,sigma]:')\n",
    "print([x,y,sigma])\n",
    "\n",
    "features_batch, _ = _feature_extractor(image_batch, models[mm],mm)\n",
    "\n",
    "print('model %d, min/max of features in batch: [%s, %s]'%(mm, torch.min(features_batch), torch.max(features_batch))) \n",
    "\n",
    "features_each_prf[batch_inds,:,mm] = torch_utils.get_value(features_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f121442c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4330e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_each_prf.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3554f990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(torch_utils.get_value(torch.sum(features_batch, axis=0)==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbf7dd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(features_batch[:,30:38]==0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5eb98ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = torch.tensor(np.array([[1,2,3],[0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c39b0722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(thing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edf387e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6532e-06, -9.0402e-07, -1.5367e-07,  1.0408e-06, -1.0979e-08,\n",
       "         -4.1182e-09, -5.8208e-11,  3.9290e-09],\n",
       "        [ 2.7426e-06, -9.9302e-07, -1.0435e-06, -4.4724e-06, -1.7801e-08,\n",
       "         -5.5879e-09,  5.8208e-11,  6.6939e-09]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_batch[:,30:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c917d5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6532e-06, -9.0402e-07, -1.5367e-07,  1.0408e-06, -1.0979e-08,\n",
       "         -4.1182e-09, -5.8208e-11,  3.9290e-09],\n",
       "        [ 2.7426e-06, -9.9302e-07, -1.0435e-06, -4.4724e-06, -1.7801e-08,\n",
       "         -5.5879e-09,  5.8208e-11,  6.6939e-09]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_batch[:,30:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d10dc91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types to exclude from the model:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "compute_features = False\n",
    "feature_types_exclude = []\n",
    "do_varpart=True\n",
    "group_all_hl_feats=True\n",
    "_fmaps_fn = steerable_pyramid_extractor(pyr_height = n_sf, n_ori = n_ori)\n",
    "# Initialize the \"texture\" model which builds on first level feature maps\n",
    "_feature_extractor = texture_feature_extractor(_fmaps_fn,sample_batch_size=batch_size, feature_types_exclude=feature_types_exclude, \\\n",
    "                                               n_prf_sd_out=n_prf_sd_out, aperture=aperture, do_varpart = do_varpart, \\\n",
    "                              compute_features=compute_features, group_all_hl_feats = group_all_hl_feats, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bdebfe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_feature_extractor.feature_column_labels==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b466452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pixel_stats',\n",
       " 'mean_magnitudes',\n",
       " 'mean_realparts',\n",
       " 'marginal_stats_lowpass_recons',\n",
       " 'variance_highpass_resid',\n",
       " 'magnitude_feature_autocorrs',\n",
       " 'lowpass_recon_autocorrs',\n",
       " 'highpass_resid_autocorrs',\n",
       " 'magnitude_within_scale_crosscorrs',\n",
       " 'real_within_scale_crosscorrs',\n",
       " 'magnitude_across_scale_crosscorrs',\n",
       " 'real_imag_across_scale_crosscorrs',\n",
       " 'real_spatshift_within_scale_crosscorrs',\n",
       " 'real_spatshift_across_scale_crosscorrs']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.feature_types_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f7738b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing precomputed features from memory.\n"
     ]
    }
   ],
   "source": [
    "_feature_extractor.init_for_fitting([240,240], models, device)\n",
    "masks, names = _feature_extractor.get_partial_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "57cfc966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 641)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4df8c289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0:4,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "93af8138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[:,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "594deb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(_feature_extractor.feature_column_labels==1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "20c80d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(_feature_extractor.feature_column_labels==1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "beb84a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feature_extractor.group_all_hl_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ded7b41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full_model', 'just_lower-level', 'just_higher-level']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9dc5b8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-77-41fc8936abb2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-77-41fc8936abb2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    _feature_extractor.\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "_feature_extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f98908b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed features from /user_data/mmhender/features/pyramid_texture/S1_features_each_prf_4ori_4sf.h5py\n",
      "Took 0.71024 seconds to load file\n",
      "Size of features array for this image set is:\n",
      "(4, 641, 875)\n",
      "pixel_stats, size is:\n",
      "torch.Size([4, 6])\n",
      "mean_magnitudes, size is:\n",
      "torch.Size([4, 16])\n",
      "mean_realparts, size is:\n",
      "torch.Size([4, 16])\n",
      "marginal_stats_lowpass_recons, size is:\n",
      "torch.Size([4, 10])\n",
      "variance_highpass_resid, size is:\n",
      "torch.Size([4, 1])\n",
      "magnitude_feature_autocorrs, size is:\n",
      "torch.Size([4, 272])\n",
      "lowpass_recon_autocorrs, size is:\n",
      "torch.Size([4, 73])\n",
      "highpass_resid_autocorrs, size is:\n",
      "torch.Size([4, 25])\n",
      "magnitude_within_scale_crosscorrs, size is:\n",
      "torch.Size([4, 24])\n",
      "real_within_scale_crosscorrs, size is:\n",
      "torch.Size([4, 24])\n",
      "magnitude_across_scale_crosscorrs, size is:\n",
      "torch.Size([4, 48])\n",
      "real_imag_across_scale_crosscorrs, size is:\n",
      "torch.Size([4, 96])\n",
      "real_spatshift_within_scale_crosscorrs, size is:\n",
      "torch.Size([4, 10])\n",
      "real_spatshift_across_scale_crosscorrs, size is:\n",
      "torch.Size([4, 20])\n",
      "Final size of features concatenated is [4 x 641]\n",
      "Feature types included are:\n",
      "['pixel_stats', 'mean_magnitudes', 'mean_realparts', 'marginal_stats_lowpass_recons', 'variance_highpass_resid', 'magnitude_feature_autocorrs', 'lowpass_recon_autocorrs', 'highpass_resid_autocorrs', 'magnitude_within_scale_crosscorrs', 'real_within_scale_crosscorrs', 'magnitude_across_scale_crosscorrs', 'real_imag_across_scale_crosscorrs', 'real_spatshift_within_scale_crosscorrs', 'real_spatshift_across_scale_crosscorrs']\n"
     ]
    }
   ],
   "source": [
    "image_inds = np.array([50,1,90,15])\n",
    "mm=1\n",
    "feats, _ = _feature_extractor(image_inds, models[mm], mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "db8275f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 592])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[:, masks[2,:]==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d51e4629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[1,0:10].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24572ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_each_prf[1,0:10,1].dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
